[
{"site": "docs", "section": "Docs", "title": "Introduction", "url": "/", "documentId": "9d510cd93abf123fb2fa61e035d88c6dece69201", "text": "\n                        \n                            \n                                \n                                \n                                Introduction\nPlatform.sh is a second-generation Platform-as-a-Service built especially for continuous deployment. It allows you to host web applications on the cloud while making your development and testing workflows more productive.\nIf you're new to Platform.sh, we recommend starting with the Big Picture, in particular  Structure, and Build &amp; Deploy will get you started on the right track to best leverage Platform.sh.\nThe main requirement of Platform.sh is that you use Git to manage your application code. Your project's configuration is driven almost entirely by a small number of YAML files in your Git repository.  The Configuration section covers those in more detail and can serve as both tutorial and quick-reference.\nPlatform.sh supports a number of different programming Languages and environments, and it features recommended optimizations for a number of Featured Frameworks.\nFinally, you can also get tips for setting up your own Development workflow and Administering your Platform.sh account.\nGit Driven Infrastructure\nAs a Platform as a Service, or PaaS, Platform.sh automatically manages everything your application needs in order to run.  That means you can, and should, view your infrastructure needs as part of your application, and version-control it as part of your application.\nInfrastructure as code\nPlatform.sh covers not only all of your hosting needs but also most of your DevOps needs. It is a simple, single tool that covers the application life-cycle from development to production and scaling.\nYou only need to write your code, including a few YAML files that specify your desired infrastructure, commit it to Git, and push.  You don't need to setup anything manually. The web server is already setup and configured, as is any database, search engine, or cache that you specify.\nEvery branch you push is a fully independent environment\u2014complete with your application code, a copy of your database, a copy of your search index, a copy of your user files, everything\u2014and its automatically generated URL can be sent to stakeholders or to automated CI systems.  It really is \"what would my site look like if I merged this to production?\"  Every time.\nYou can use these concepts to replicate a traditional development/staging/production workflow or even to give every feature its own effective staging environment before merging to production (empowering you to use git-flow like methodologies even better). You could also have an intermediary integration branch for several other branches.\nPlatform.sh respects the structure of branches. It's entirely up to you.\nFull stack management\nManaging your full stack internally gives Platform.sh some unique features:\n\nUnified Environment: All of your services (MySQL, ElasticSearch, MongoDB, etc...) are managed inside the cluster and included in the price, with no external single-points-of-failure. When you back up an environment, you get a fully consistent snapshot of your whole application.\nMulti-Services &amp; Multi-App: You can deploy multiple applications (for example, in a microservice-based architecture), using multiple data backends (MySQL, Postgres, Redis etc..) written in multiple frameworks (Drupal + NodeJS + Flask, for example) in multiple languages, all in the same cluster.\nFull Cluster Cloning Technology: The full production cluster can be cloned in under a minute\u2014including all of its data\u2014to create on-the-fly, ephemeral development environments that are a byte-level copy of production.\nFail-Proof Deployments: Every time you test a new feature, you also test the deployment process.\nContinuous Deployment from the Start: Everything is build-oriented, with a consistent, repeatable build process, simplifying the process of keeping your application updated and secure.\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Overview", "title": "Structure", "url": "/overview/structure.html", "documentId": "2c704f917b601ea53924c1aa3bd03600700c0976", "text": "\n                        \n                            \n                                \n                                \n                                Structure\nEvery application you deploy on Platform.sh is built as a virtual cluster, containing a set of containers.  The master branch of your Git repository is always deployed as the production cluster.  Any other branch can be deployed as a development cluster.  By default, you can have up to three live development clusters at once, but you can buy more on a per-project basis.\nThere are three types of containers within your cluster:\n\none Router\none or more Application containers\nzero or more Service containers\n\nAll of those containers are managed by three special files in your Git repository:\n\n.platform/routes.yaml\n.platform/services.yaml\n.platform.app.yaml\n\nIn most cases, that means your repository will look like this:\nyourproject/\n  .git/\n  .platform/\n    services.yaml\n    routes.yaml\n  .platform.app.yaml\n  &lt;your application files&gt;\nRouter\nThere is always exactly one Router per cluster.\nThe Router of a cluster is a single nginx process.  It is configured by the routes.yaml file.  It maps incoming requests to the appropriate Application container and provides basic caching of responses, if so configured. It has no persistent storage.\nService\nService containers are configured by the services.yaml file.\nThere may be zero or more Service containers in a cluster, depending on the services.yaml file.  The code for a Service is provided by Platform.sh in a pre-built container image, along with a default configuration.  Depending on the service it may also include user-provided configuration in the services.yaml file.  Examples of services include MySQL/MariaDB, Elasticsearch, Redis, and RabbitMQ.\nApplication\nThere always must be one Application container in a cluster, but there may be more.\nEach Application container corresponds to a .platform.app.yaml file in the repository.  If there are 3 .platform.app.yaml files, there will be three Application containers.  Application containers hold the code you provide via your Git repository.  Application containers are always built off of one of the Platform.sh-provided language-specific images, such as \u201cPHP 5.6\u201d, \u201cPHP 7.2\u201d, or \u201cPython 3.7\u201d. It is also possible to have multiple Application containers running different languages or versions.\nFor typical applications, there is only one .platform.app.yaml file, which is generally placed at the repository root.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Next steps", "url": "/gettingstarted/local-development/next-steps.html", "documentId": "e466b3311d77778af381b9c82aa414f218b1ae24", "text": "\n                        \n                            \n                                \n                                \n                                Local development\nNext steps\nIn this guide you opened an SSH tunnel to your Platform.sh project and built your application locally.\nDon't stop now! There are far more features that make Platform.sh profoundly helpful to developers that you have left to explore.\nDeveloping on Platform.sh\nConsult these additional resources to help improve your development life cycle.\n\n  Development environmentsActivate development branches and test new features before merging into production.\n\n\n\nAdditional Resources\n\n  Next stepsSet up domains to take your application live, configure external integrations to GitHub, and more!\n\n\n\n  Platform.sh CommunityCheck out how-tos, tutorials, and get help for your questions about Platform.sh.\n\n\n\n  Platform.sh BlogRead news and how-to posts all about working with Platform.sh.\n\n\n\n  Back\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Build site locally", "url": "/gettingstarted/local-development/build-locally.html", "documentId": "2696c65484c1515c1fdac670444ffc5697a27070", "text": "\n                        \n                            \n                                \n                                \n                                Local development\nBuild site locally\nNow that you've opened tunnels into your services, you'll have access to all of your data in your environment. All that's left now is to actually build the site.\n&lt;/asciinema-player&gt;\n\nBuild the site\n From the repository root, run the command\n platform build\n\n The Platform CLI will first ask you for the source directory and the build destination, then it will use your .platform.app.yaml file to execute the build process locally. This will create a _www directory in the project root that is a symlink to the currently active build, which is now located in .platform/local/builds.\n\nVerify\n Move to the build destination (i.e. cd _www) and then run a local web server to verify the build.\n PHPPythonRubyphp -d variables_order=EGPCS -S localhost:8001python3 -m http.server 8000ruby -run -e httpd . -p 8000\n Applications written in Node.js, Go and Java can be configured to listen on a port locally, so it will only be necessary to execute the program directly.\n\nCleanup\n That's it! Now you can easily spin up a local build of your application and test new features with full access to all of the data in your services. When you are finished, shut down the web server and then close the tunnel to your services:\n platform tunnel:close\n\n\n\nNow you know how to connect to your services on Platform.sh and perform a local build during development.\n\n  Back\n  I have built my application locally\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Connect to services", "url": "/gettingstarted/local-development/connect-services.html", "documentId": "c0b0e4adee602b92a0f5ae4fadbfd3b2f9f263d1", "text": "\n                        \n                            \n                                \n                                \n                                Local development\nConnect to services\n\nNote: If your application does not contain any services, you do not need to open a tunnel and can proceed to the next step.\n\nNow that you have a local copy of your application code, you can make changes to the project without pushing to Platform.sh each time to test them. Instead you can locally build your application using the CLI, even when its functionality depends on a number of services.\n&lt;/asciinema-player&gt;\n\nOpen an SSH tunnel to connect to your services\n Open local SSH tunnels to your environment's services.\n platform tunnel:open\n\n\nExport environment variables\n Platform.sh utilizes environment variables called Relationships within the application container. These store the credentials needed to connect to individual services. In order to connect with them remotely using the SSH tunnel you will need to mimic the same environment variables locally.\n export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\"\n\n In order to use these credentials to connect to your services, it may also be necessary to install the clients for those services are locally.\n Additionally, if your application also needs access to the PORT environment variable, you can mock the variable used in a Platform.sh environment with\n export PORT=8888\n\n If you are using a Config Reader library with the application, it will also be necessary to mock two additional variables\n export PLATFORM_APPLICATION_NAME=&lt;.platform.app.yaml name, i.e. app&gt;\n export PLATFORM_BRANCH=&lt;branch being built, i.e. dev&gt;\n\n\nVerify\n You can visualize the open tunnels for your application with the command\n platform tunnel:list\n\n The tunnel will close itself after a timeout period of inactivity, but you can also do so with the command\n platform tunnel:close\n\n\n\nNow that you have created an SSH tunnel to your services, build your application locally.\n\n  Back\n  I have opened an SSH tunnel into my services\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Create environment", "url": "/gettingstarted/dev-environments/create-environment.html", "documentId": "466f6fa62466995756c613c12569d9b59535d0c4", "text": "\n                        \n                            \n                                \n                                \n                                Development Environments\nCreate environment\nPlatform.sh supports the deployment of isolated development environments from your branches, complete with exact copies of all of your data. This is useful for testing changes in isolation before merging them.\n&lt;/asciinema-player&gt;\n\nCreate branch, make changes, push to Platform.sh\n Create and checkout a branch for your new feature.\n platform branch dev\n This command will create a new branch dev from master, as well as a local dev branch for you to work on. dev will be a clone of master, including an exact-copy of all of its data and files.\n Make changes to your code, commit them, and push to the Platform.sh remote.\n git add .\n git commit -m \"Commit message.\"\n git push platform dev\n\n\nNote: If you create a new branch with Git (i.e. git checkout -b new-feature), when you push its commits to Platform.sh that branch will not automatically build. new-feature is an <a href=\"../../GLOSSARY.html#inactive-environment\" class=\"glossary-term\" title=\"An environment which is not deployed. You can activate an inactive environment from the\nenvironment configuration page on the Platform.sh management console.\">inactive environment, because Platform.sh does not assume that every branch should be associated with an active environment, since your plan will limit the number of active environments you are allowed.\nIf you want to activate the new-feature environment after it has been pushed, you can do so with the command\nplatform environment:activate new-feature\n\n\nVerify\nAfter Platform.sh has built and deployed the environment, verify that it has been activated by visiting its new url:\nplatform url\n\nThe command will provide a list of the generated routes for the application according to how the routes were configured. The structure will be:\nEnter a number to open a URL\n [0] https://&lt;branch name&gt;-&lt;hash of branch name&gt;-&lt;project ID&gt;.&lt;region&gt;.platformsh.site/\n [1] https://www.&lt;branch name&gt;-&lt;hash of branch name&gt;-&lt;project ID&gt;.&lt;region&gt;.platformsh.site/\nThe above URLs represent the upstream (0) and redirect (1) routes for the most common routes.yaml configuration.\n\n\n\n  Back\n  I have created and activated a development environment\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Next Steps", "url": "/gettingstarted/next-steps.html", "documentId": "f10ab46ca0e081296412f7f55be1dcd6c069a160", "text": "\n                        \n                            \n                                \n                                \n                                Next Steps\nNow that your application is up and running, here are some additional pieces of information that will help you leverage every bit of technology Platform.sh has to offer.\n\n  External IntegrationsConfigure Platform.sh to mirror every push and pull request on GitHub, Gitlab, and Bitbucket.\n\n  Going LiveSet up your site for production, configure domains, and go live!\n\n\nAdditional Resources\n\n  Platform.sh Community PortalCheck out how-tos, tutorials, and get help for your questions about Platform.sh.\n\n  Platform.sh BlogRead news and how-to posts all about working with Platform.sh.\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "External integrations", "url": "/gettingstarted/integrations.html", "documentId": "cdd54929da4b5ee0f67d04ef7c8d86ddb7e330d3", "text": "\n                        \n                            \n                                \n                                \n                                External Integrations\nWhile you can host your application repository entirely on Platform.sh, it's likely that you will want to integrate your deployments with your pre-existing service. Platform.sh can be easily integrated with external services such as GitHub, Gitlab, or Bitbucket.\nChoose your current service, and this guide will take you through the steps to mirror your repository on Platform.sh and have environments created automatically for your pull requests and branches.\n\n  GitHub\n  Bitbucket\n  GitLab\n\n\nThese steps assume that you have already:\n\nSigned up for a free trial account with Platform.sh.\n\nIf you have not completed these steps by now, click the links and do so before you begin.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Download the code", "url": "/gettingstarted/local-development/download-code.html", "documentId": "a5c16e5b9e201d9f17157e3acc7c989967418785", "text": "\n                        \n                            \n                                \n                                \n                                Local development\nDownload the code\nIf you have already pushed your code to Platform.sh, then you should already have a local repository that you can build from.\nOtherwise, it will be necessary to download a local copy of your project first.\n&lt;/asciinema-player&gt;\n\nGet project ID\n You will need the your project ID. You can retrieve this ID at any time using the CLI command platform.\n\nGet a copy of the repository locally\n Next, use the CLI to download the code in your Platform.sh project using the command\n platform get &lt;project id&gt;\n\n\n\nNext you can now connect to its services and build it on your machine.\n\n  Back\n  I have a local copy of my code\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Next steps", "url": "/gettingstarted/dev-environments/next-steps.html", "documentId": "4a332d873f8ed0564428c7997f9f1657485bcff9", "text": "\n                        \n                            \n                                \n                                \n                                Development Environments\nNext steps\nIn this guide you learned how to create and activate live feature environments, test them, and merge them into production safely using backups.\nDon't stop now! There are many more features that make Platform.sh profoundly helpful to developers.\nDeveloping on Platform.sh\nThe next guide shows how to set up your development workflow to benefit from Platform.sh.\n\n  Local DevelopmentRemotely connect to services and build your application locally during development.\n\n\n\nAdditional Resources\n\n  Next stepsSet up domains to take your application live, configure external integrations to GitHub, and more!\n\n\n\n  Platform.sh CommunityCheck out how-tos, tutorials, and get help for your questions about Platform.sh.\n\n\n\n  Platform.sh BlogRead news and how-to posts all about working with Platform.sh.\n\n\n\n  Back\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Development environments", "url": "/gettingstarted/dev-environments.html", "documentId": "68e0d50d80e50ef5fd4a69f2f66dd065265b464f", "text": "\n                        \n                            \n                                \n                                \n                                Development Environments\nWhile you're developing your application, you will at some point create some new features for it. Typically you're going to develop that feature on a separate branch in your Git repository, run some tests, and then merge that feature into your production application.\nThis is where the stress comes in and where breaking your live site becomes a real worry.\nPlatform.sh removes this stress considerably by providing live development environments for the features you're working on.\nThis guide assumes that you have already:\n\nSigned up for a free trial account with Platform.sh.\nStarted either a template project or pushed your own code to Platform.sh.\n\nIf you have not completed these steps by now, click the links and do so before you begin.\n\n  Get started!\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Download the code", "url": "/gettingstarted/dev-environments/download-code.html", "documentId": "6682c8d23a4c5abd88509a7b04878c759dd3ee91", "text": "\n                        \n                            \n                                \n                                \n                                Development Environments\nDownload the code\nIf you have already pushed your code to Platform.sh, then you should already have a local repository that you can build from.\nOtherwise, it will be necessary to download a local copy of your project first.\n&lt;/asciinema-player&gt;\n\nGet project ID\n You will need the your project ID. You can retrieve this ID at any time using the CLI commands platform or platform project:list.\n\nGet a copy of the repository locally\n Next, use the CLI to download the code in your Platform.sh project using the command\n platform get &lt;project id&gt;\n\n\n\nNow that you have a local copy of your application that is configured to the Platform.sh remote repository, you can create a new .\n\n  Back\n  I have a local copy of my code\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Merge into production", "url": "/gettingstarted/dev-environments/merge.html", "documentId": "35a0c40baba3abf81956127af0cbf336d7fe7390", "text": "\n                        \n                            \n                                \n                                \n                                Development Environments\nMerge into production\nNow that you've had the chance to verify that your application built and deployed correctly on your development environment, you're ready to merge it into your production site. Platform.sh provides backup features that protect against any unforeseen consequences of your merges, keeping a historical copy of all of your code and data.\n&lt;/asciinema-player&gt;\n\nNote: The --project flag is not needed if you are running the platform command from within your local repository.\n\n\nCreate a backup\n Before you merge the dev feature into master, create a backup of the master environment. The backup will preserve both the code and all of its data.\n platform backup --project &lt;project id&gt;\n\n Select master as the environment you want to back up.\n\nMerge feature into production\n git checkout master\n git merge dev\n git push\n\n When the build process completes, verify that your changes have been merged.\n platform url\n\n\nRestore a backup\n If you would like to restore the code and data to the time of your backup, use the command\n platform backup:restore --project &lt;project id&gt;\n\n\n\n\n  Back\n  I have merged the new feature\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Developing on Platform.sh", "url": "/gettingstarted/developing.html", "documentId": "d73e595cd69651b720290326641fd81f115089b0", "text": "\n                        \n                            \n                                \n                                \n                                Developing on Platform.sh\nOnce an application has been migrated to Platform.sh, there's plenty more features that will help improve your development life cycle. You can build your site locally, remotely connect to your services, and test new features on a live site all with Platform.sh.\n\n  Local DevelopmentRemotely connect to services and build your application locally during development.\n\n  Development environmentsActivate development branches and test new features before merging into production.\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Build, Deploy, Done!", "url": "/gettingstarted/own-code/push-project.html", "documentId": "512478247caba52b2c0c2894ea0d41d335d72a6d", "text": "\n                        \n                            \n                                \n                                \n                                Import your own code\nBuild, Deploy, Done!\nWith your configuration files complete, all that's left is to commit the changes and push to Platform.sh.\n&lt;/asciinema-player&gt;\n\nCommit and push\n Run the commands\n git add .\n git commit -m \"Add config files.\"\n git push -u platform master\n\n Platform.sh will detect the presence of your configuration files and use them to build the application.\n\nVerify\n When the build is completed, you can verify the deployment by typing the command\n platform url\n\n This will return a list of your routes. Pick the primary route 0 and click Enter, which will open your application in a browser window.\n Alternatively, you can also log back into the management console in your new project. Select the Master environment in the Environments list and click the link below the Overview box on the left side of the page.\n \n   \n \n\n\n\nThat's it! Using the Platform.sh CLI and a few properly configured files, pushing your application to run on Platform.sh takes only a few minutes.\nNow that your code is on Platform.sh, check out some of the Next Steps to get started developing.\n\n  Back\n  I have deployed my application\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Local development", "url": "/gettingstarted/local-development.html", "documentId": "928de7ce83158a75138f225abfccc8477d8f8def", "text": "\n                        \n                            \n                                \n                                \n                                Local development\nNow that you have a project on Platform.sh, it would be helpful to run the same build process on your local machine so that you can develop and test new features before pushing them. This guide will take you through the steps of connecting remotely to your services and building your application locally.\nThese steps assume that you have already:\n\nSigned up for a free trial account with Platform.sh.\nStarted either a template project or pushed your own code to Platform.sh.\n\nIf you have not completed these steps by now, click the links and do so before you begin.\n\n  Get started!\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Next steps", "url": "/gettingstarted/own-code/next-steps.html", "documentId": "b9fdfc2125444c70b9286ba357922ed9caf4568c", "text": "\n                        \n                            \n                                \n                                \n                                Import your own code\nNext steps\nIn this guide you created a project using the CLI and configured your project to run on Platform.sh using a few simple configuration files.\nDon't stop now! There are far more features that make Platform.sh profoundly helpful to developers that you have left to explore.\nDeveloping on Platform.sh\nOnce an application has been migrated to Platform.sh, there's plenty more features that will help improve your development life cycle.\n\n  Local DevelopmentRemotely connect to services and build your application locally during development.\n\n  Development environmentsActivate development branches and test new features before merging into production.\n\n\nAdditional Resources\n\n  Next stepsSet up domains to take your application live, configure external integrations to GitHub, and more!\n\n\n\n  Platform.sh CommunityCheck out how-tos, tutorials, and get help for your questions about Platform.sh.\n\n\n\n  Platform.sh BlogRead news and how-to posts all about working with Platform.sh.\n\n\n\n  Back\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Docs", "title": "Introduction", "url": "/", "documentId": "9d510cd93abf123fb2fa61e035d88c6dece69201", "text": "\n                        \n                            \n                                \n                                \n                                Introduction\nPlatform.sh is a second-generation Platform-as-a-Service built especially for continuous deployment. It allows you to host web applications on the cloud while making your development and testing workflows more productive.\nIf you're new to Platform.sh, we recommend starting with the Big Picture, in particular  Structure, and Build &amp; Deploy will get you started on the right track to best leverage Platform.sh.\nThe main requirement of Platform.sh is that you use Git to manage your application code. Your project's configuration is driven almost entirely by a small number of YAML files in your Git repository.  The Configuration section covers those in more detail and can serve as both tutorial and quick-reference.\nPlatform.sh supports a number of different programming Languages and environments, and it features recommended optimizations for a number of Featured Frameworks.\nFinally, you can also get tips for setting up your own Development workflow and Administering your Platform.sh account.\nGit Driven Infrastructure\nAs a Platform as a Service, or PaaS, Platform.sh automatically manages everything your application needs in order to run.  That means you can, and should, view your infrastructure needs as part of your application, and version-control it as part of your application.\nInfrastructure as code\nPlatform.sh covers not only all of your hosting needs but also most of your DevOps needs. It is a simple, single tool that covers the application life-cycle from development to production and scaling.\nYou only need to write your code, including a few YAML files that specify your desired infrastructure, commit it to Git, and push.  You don't need to setup anything manually. The web server is already setup and configured, as is any database, search engine, or cache that you specify.\nEvery branch you push is a fully independent environment\u2014complete with your application code, a copy of your database, a copy of your search index, a copy of your user files, everything\u2014and its automatically generated URL can be sent to stakeholders or to automated CI systems.  It really is \"what would my site look like if I merged this to production?\"  Every time.\nYou can use these concepts to replicate a traditional development/staging/production workflow or even to give every feature its own effective staging environment before merging to production (empowering you to use git-flow like methodologies even better). You could also have an intermediary integration branch for several other branches.\nPlatform.sh respects the structure of branches. It's entirely up to you.\nFull stack management\nManaging your full stack internally gives Platform.sh some unique features:\n\nUnified Environment: All of your services (MySQL, ElasticSearch, MongoDB, etc...) are managed inside the cluster and included in the price, with no external single-points-of-failure. When you back up an environment, you get a fully consistent snapshot of your whole application.\nMulti-Services &amp; Multi-App: You can deploy multiple applications (for example, in a microservice-based architecture), using multiple data backends (MySQL, Postgres, Redis etc..) written in multiple frameworks (Drupal + NodeJS + Flask, for example) in multiple languages, all in the same cluster.\nFull Cluster Cloning Technology: The full production cluster can be cloned in under a minute\u2014including all of its data\u2014to create on-the-fly, ephemeral development environments that are a byte-level copy of production.\nFail-Proof Deployments: Every time you test a new feature, you also test the deployment process.\nContinuous Deployment from the Start: Everything is build-oriented, with a consistent, repeatable build process, simplifying the process of keeping your application updated and secure.\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Dedicated", "title": "Options", "url": "/dedicated/architecture/options.html", "documentId": "a9c50fd3567e2975a4bc7cb4433c16fd60ec76e7", "text": "\n                        \n                            \n                                \n                                \n                                Options\nStaging environments\nBy default, the staging instance and production instance run on the same trio of virtual machines.  That ensures identical configuration between them but can incur a performance penalty for production if the load generated during QA and UAT in staging is of any appreciable size.\nA dedicated single-node staging machine can be provisioned for your application with an identical software configuration to your production hardware, but reduced hardware specs.  This gives the advantages of isolating the staging load from the production hardware as well as having an identical software configuration to perform UAT, but this option does not provide a bed for performance testing as the physical hardware configuration is not the same as production.\nMultiple applications\nEach application deployed to the Dedicated Cluster corresponds to a single Git repository in the Development Environment.  Multiple .platform.app.yaml files are not supported.  While it is possible to host multiple application code bases in separate subdirectories/subpaths of the application (such as /drupal, /api, /symfony, etc.) controlled by a single .platform.app.yaml, it is not recommended and requires additional configuration.  One or more domains may be mapped to the application.\nOur experience has shown that hosting multiple applications on a common resource pool is often bad for all applications on the cluster.  We therefore limit the number of applications that may be hosted on a single Dedicated Cluster.  On a D6 instance, only one application is supported.  On D12 and larger Dedicated plans multiple applications are supported at an extra cost.  Each application would correspond to a different Development Environment and Git repository and cannot share data or files with other applications.  This configuration is discouraged.\nMultiple-AZ\nThe default configuration for Platform.sh Dedicated clusters is to launch into a single Availability Zone (AZ).  This is for a few reasons:\n\nBecause the members of your cluster communicate with each other via TCP to perform DB replication, cache lookup, and other associated tasks, the latency between data centers/AZs can become a significant performance liability.  Having your entire cluster within one AZ ensures that the latency between cluster members is minimal, having a direct effect on perceived end-user performance.\nNetwork traffic between AZs is billed, whereas intra-AZ traffic is not.  That leads to higher costs for this decreased performance.\n\nSome clients prefer the peace of mind of hosting across multiple AZs, but it should be noted that multiple-AZ configurations do not improve the contractual 99.99% uptime SLA, nor does our standard, single-AZ configuration decrease the 99.99% uptime SLA.  We are responsible for meeting the 99.99% uptime SLA no matter what, so multiple-AZ deployments should only be considered in cases where it is truly appropriate.\nMulti-AZ deployments are available only on select AWS regions.\nAdditional application servers\nFor especially high-traffic sites we can also add additional application-only servers.  These servers will contain just the application code; data storage services (SQL, Solr, Redis, etc.) are limited to the standard three.  The cluster begins to look more like a standard N-Tier architecture at this point, with a horizontal line of web application servers in front of a 3 node (N+1) cluster of Galera database servers.\nSpeak to your sales representative about the costs associated with adding additional application servers.  This configuration requires a separate setup from the default so advanced planning is required.\nSFTP accounts\nIn addition to SSH accounts, SFTP accounts can be created with a custom user/password that are restricted to certain directories. These directories must be one of the writeable mounts (or rather, there's no point assigning them to the read-only code directory).  There is no cost for this configuration, and it can be requested at any time via a support ticket. SSH public key based authentication is also supported on the SFTP account.\nError handling\nOn Platform.sh Professional, incoming requests are held at the edge router temporarily during a deploy.  That allows a site to simply \"respond slowly\" rather than be offline during a deploy, provided the deploy time is short (a few seconds).\nOn Platform.sh Dedicated, incoming requests are not held during deploy and receive a 503 error.  As the Dedicated Cluster is almost always fronted by a CDN, the CDN will continue to serve cached pages during the few seconds of deploy, so for the vast majority of users there is no downtime or even slowdown.  If a request does pass the CDN during a deploy that is not counted as downtime covered by our Service Level Agreement.\nBy default, Platform.sh will serve generic Platform.sh-branded error pages for errors generated before a request reaches the application.  (500 errors, some 400 errors, etc.)  Alternatively you may provide a static error page for each desired error code via a ticket for us to configure with the CDN.  This file may be any static HTML file but is limited to 64 KB in size.\nIP restrictions\nPlatform.sh supports project-level IP restrictions (whitelisting) and HTTP Basic authentication.  These may be configured through the Development Environment and will be automatically replicated from the production and staging branches to the production and staging environments, respectively.\n\nnote\nChanging access control will trigger a new deploy of the current environment. However, the changes will not propagate to child environments until they are manually redeployed.\n\nRemote logging\nPlatform.sh Dedicated supports sending logs to a remote logging service such as Loggly, Papertrail, or Logz.io using the rsyslog service.  This is an optional feature and you can request that it be enabled via a support ticket.  Once enabled and configured your application can direct log output to the system syslog facility and it will be replicated to the remote service you have configured.\nWhen contacting support to enable rsyslog, you will need:\n\nThe name of the remote logging service you will be using.\nThe message template format used by your logging service.\nThe specific log files you want forwarded to your logging service.\n\nThere is no cost for this functionality.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Docs", "title": "", "url": "/GLOSSARY.html", "documentId": "13ff3f51132d6bc5708892fc9a33b10ec717f407", "text": "\n                        \n                            \n                                \n                                \n                                Active Environment\nAn environment which is deployed. You can deactivate an active environment from the environment configuration page on the Platform.sh management console.\nCluster\nEvery active environment is deployed as a cluster, that is, a collection of independent containers representing different services that make up your web application.  That may include a database container, an Elasticsearch container, a container for your application, etc.  They are always deployed together as a single unit.\nDrush\nDrush is a command-line shell and scripting interface for Drupal.\nDrush aliases\nDrush site aliases allow you to define short names that let you run Drush commands on specific local or remote Drupal installations. The Platform.sh CLI configures Drush aliases for you on your local environment (via platform get or platform drush-aliases). You can also configure them manually.\nInactive environment\nAn environment which is not deployed. You can activate an <a href=\"GLOSSARY.html#inactive-environment\" class=\"glossary-term\" title=\"An environment which is not deployed. You can activate an inactive environment from the\nenvironment configuration page on the Platform.sh management console.\">inactive environment from the\nenvironment configuration page on the Platform.sh management console.\nLive Environment\nAn environment which is deployed from the master branch under a production plan.\nPaaS\nA Platform as a Service is an end-to-end hosting solution that includes workflow tools, APIs, and other functionality above and beyond basic hosting.  The best example is Platform.sh (although we are a little biased).\nProduction plan\nA subscription level which allows you to host your production website by adding a domain and a custom SSL certificate.\nTLS\nTransport Layer Security is the successor of SSL (Secure Socket Layer).  It provides the cryptographic \"S\" in HTTPS.  It's often still referred to as SSL even though it has largely replaced SSL for online encrypted connections.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Dedicated", "title": "Scalability", "url": "/dedicated/architecture/scalability.html", "documentId": "789139819e6436ab664ddefd026a1ea7f2a3690b", "text": "\n                        \n                            \n                                \n                                \n                                Scalability\nPart of the original design goal of Platform.sh\u2019s Triple Redundant Architecture was to ensure scalability in times of load spikes outside of the bounds of the original traffic specs.  Because the cluster is configured as an N+1 architecture, we can respond to legitimate traffic events by removing a node from the cluster, upsizing it, returning it into rotation, and then repeating the process on the next node in turn.\nScaling Process and Procedure\nThe scaling process is not automatic and requires manual effort.  It may be initiated in two ways.\n\nOn customer request via a ticket.  We strongly recommend notifying us ahead of time if you know a large traffic event is coming (a major product launch, Black Friday, etc.)  We cannot guarantee a turnaround time on a resizing unless given prior notice.\nHigh load incidents detected by our monitoring system.\n\nIf the load is diagnosed to be due to a bot or crawler that we are able to block, we will attempt to block it.  This prevents unnecessary scaling, which prevents unnecessary costs to you.  If it is not a bot or is not blockable, then we will begin the upscaling process detailed above.  Be advised that this process may take up to 60-90 minutes depending on the diagnostic steps needed.\nWe will open a support ticket to notify you of such changes, but we will not wait for your response before upscaling your cluster.  The uptime of your application is our top priority and reactive scaling events are part of how we ensure that we meet the obligations of our Service Level Agreement.\nYou may opt-out of the upsizing service if you wish, but outages caused by high-traffic will not be considered to violate the Service Level Agreement.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Dedicated", "title": "Dev Environments", "url": "/dedicated/architecture/development.html", "documentId": "6568d98f2b7de80f97c1b76cd3b24de90896a3df", "text": "\n                        \n                            \n                                \n                                \n                                Platform.sh Development environments\nPlatform.sh Dedicated customers will have a development environment for their project that consists of a Platform.sh Grid project, typically provisioned by the Platform.sh team to reflect the amount of storage in your contract.  This environment will provide you with all the DevOps, Continuous Integration, Continuous Deployment, and other workflow tooling of the professional product, but will segregate the performance impacts from your production hardware.\nArchitecture (Development Environments)\n\nDefault limits\nThe Development Environment for a Dedicated project provides a production and staging branch linked to the Dedicated Cluster, a master branch, and ten (10) additional active environments.  This number can be increased if needed for an additional fee.\nThe default storage for Dedicated contracts is 50GB per environment (production, staging, and each development environment) - this comprises total storage for your project and is inclusive of any databases, uploaded files, writable application logging directories, search index cores, and so on.  The storage amount for your development environment will reflect the amount in your Enterprise contract.\nA project may have up to six (6) users associated with it at no additional charge.  Additional users may be added for an additional fee.  These users will have access to both the Development Environment and the Dedicated Cluster.\nLarger developments environments\nBy default, all containers in development environments are \"Small\" sized, as they have limited traffic needs.  For more resource-intensive applications this size can be increased for an additional fee.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Dedicated", "title": "Deploying", "url": "/dedicated/architecture/deploying.html", "documentId": "4131037e35d3b202e81097986dcf7357faf398f2", "text": "\n                        \n                            \n                                \n                                \n                                Deployment\nDeploying to Production and Staging\nThe production branch of your Git repository is designated for production, and a staging branch is designated for staging.  Any code merged to those branches will automatically trigger a rebuild of the production and staging environments, respectively, in the Dedicated Cluster.  Any defined users or environment variables will also be propagated to the Dedicated Cluster as well.\nNote that there is no automatic cloning of data from the Dedicated Cluster to the Development Environment the way there is between branches in the Development Environment.  Production data may still be replicated to the Development Environment manually.\nThe master branch is still available but will have no impact on either the production or staging environments.  Deploys of the master branch will not trigger a rebuild of the Dedicated Cluster environments.  A common model is to use the master branch as a pre-integration branch before merging code to staging, such as at the end of a sprint.\nDeployment process\nWhen deploying to the Dedicated Cluster the process is slightly different than when working with Platform.sh on the Grid.\n\nThe new application image is built in the exact same fashion as for Platform.sh Professional.\nAny active background tasks on the cluster, including cron tasks, are terminated.\nThe cluster (production or staging) is closed, meaning it does not accept new requests.  Incoming requests will receive an HTTP 500 error.\nThe application image on all three servers is replaced with the new image.\nThe deploy hook is run on one, and only one, of the three servers.\nThe cluster is opened to allow new requests.\n\nThe deploy usually takes approximately 30-90 seconds, although that is highly dependent on how long the deploy hook takes to run.\nDuring the deploy process the cluster is unavailable.  However, nearly all Platform.sh Dedicated instances are fronted by a Content Delivery Network (CDN).  Most CDNs can be configured to allow a \"grace period\", that is, requests to the origin that fail will be served from the existing cache, even if that cache item is stale.  We strongly recommend configuring the CDN with a grace period longer than a typical deployment.  That means anonymous users should see no interruption in service at all.  Authenticated traffic that cannot be served by the CDN will still see a brief interruption.\nDeployment philosophy\nPlatform.sh values consistency over availability, acknowledging that it is nearly impossible to have both.  Because the deploy hook may make database changes that are incompatible with the previous code version it is unsafe to have both old and new code running in parallel (on different servers), as that could result in data loss.  We believe that a minute of planned downtime for authenticated users is preferable to a risk of race conditions resulting in data corruption, especially with a CDN continuing to serve anonymous traffic uninterrupted.\nThat brief downtime applies only to changes pushed to the production branch. Deployments to staging or to a development branch have no impact on the production environment and will cause no downtime.\n<!--\n## Service overview\n\nAdd image here once it's updated.\n-->\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Dedicated", "title": "Architecture", "url": "/dedicated/architecture.html", "documentId": "76a9e2c20278c31a8f8b5344e4c6bcce7016254e", "text": "\n                        \n                            \n                                \n                                \n                                Platform.sh Dedicated cluster specifications\nArchitecture\nPlatform.sh Dedicated clusters are launched into a Triple Redundant configuration consisting of 3 virtual machines (VMs).  This is an N+1 configuration that is sized to withstand the total loss of any one of the 3 members of the cluster without incurring any downtime.  Each instance hosts the entire application stack, allowing this architecture superior fault tolerance to traditional N-Tier installations. Moreover, the Cores assigned to production are solely for production.\nStorage\nEach Dedicated cluster comes with 50GB of storage per environment by default.  This storage is intended for customer data - databases, search indexes, user uploaded files, etc. - and can be subdivided in any way that the customer wishes.  50GB is only the default amount; more storage can be added easily as a line item in the contract and can be added at anytime that the project requires: at contract renewal or at any point in the term.\nDefault storage is based on the default SSD block-storage offering for each cloud. Extra provisioned IOPS can be discussed with your sales representative.\nCompatible services\n\n\n\nService\nVersions\n\n\n\n\nPHP\n5.6, 7.0, 7.1 (ZTS), 7.2 (ZTS), 7.3 (ZTS)\n\n\nNodeJS\n9.8, 10\n\n\nMariaDB\n10.0 Galera, 10.1 Galera, 10.2 Galera\n\n\nRabbitMQ\n3.6\n\n\nSolr\n4.10, 6.3, 6.6\n\n\nElasticSearch\n1.7, 2.4, 5.2, 6.5\n\n\nRedis\n3.2, 5.0\n\n\nMemcached\n1.4\n\n\n\nYour application will be able to connect to each service by referencing the exact same environment variables as a Grid environment.  While the configuration of the service will be performed by our team, the application configuration is the same and your code should be the same.  See the services documentation for service-specific details.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Dedicated", "title": "Upgrading the UI", "url": "/dedicated/overview/upgrading.html", "documentId": "e59bdd91cd6a9dee36abd67654795c22478d82fe", "text": "\n                        \n                            \n                                \n                                \n                                Upgrading to the Integrated UI\nOlder Platform.sh Dedicated projects (created prior to October 2017) used a separate Git repository for Production and Staging.  That also necessitated running most configuration changes through a ticket, and maintaining separate SSH credentials for each environment.\nThese older projects can be upgraded to the new Integrated UI, which eliminates the extra Git repositories, many \"must be a ticket\" configuration changes, and makes the Production and Staging environments available in the UI.\nTo add these environments to the Project Web Interface, review this entire document, complete a few preparatory steps, and submit a ticket. Your ticket is added to a queue for updating existing Dedicated projects.  The process may take time to complete, so check your ticket for details, timing, and other important information.\nWe recommend this upgrade for all users.\nNew Features\nThe new Project Web Interface provides the following features for the Pro plan Staging and Production environments:\n\nAdd and manage user access to the environments\nSync code between Staging and Production to Integration environments\nMerge code from Integration environment to Staging environment to Production environment\nAdd and manage environment variables\nManage build and deploy hooks with the .platform.app.yaml file\nManage PHP versions and variables with the .platform.app.yaml file\nManage cron jobs with the .platform.app.yaml file\nConfigure environment settings\nAccess the environments using SSH and URL\nView status, build logs, and deployment history\n\nYou must still submit a support ticket to update and modify the following in the Staging and Production environments information:\n\nRedirects from routes.yaml file\nManaging PHP extensions\nManaging mounts\n\nYou cannot perform the following:\n\nBranch from the Staging and Production environments\nSynchronize data from the Staging and Production environments\nSnapshot the Staging and Production environments\n\nBranching hierarchy\nBefore converting your project, the branches include a repository for Development, Staging, and Production. Each repository has a master branch with deployment targets configured for Staging and Production.\nAfter converting your project, the hierarchical relationships appear in your Project Web Interface with two, main environment branches for Staging and Production:\n\nBefore you upgrade\nWhen we add Staging and Production access to the Project Web Interface, we leverage the user accounts, branch user permissions, and environment variables from your Development master environment.\nTo prepare, verify that your settings and environment variables are correct.\n\nVerify code matches across environments\nVerify user account access\nPrepare variables\n\nVerify code\nWe strongly recommend working in your local development environment, deploying to Development, deploying to Staging, and, finally, deploying to Production. All code should match 100% across each of these environments. Before submitting a ticket, make sure you sync your code. This process creates a new branch of code for Staging and Production environments.\nIf you have additional code, such as new extensions in your Production environment without following this workflow, then deployments from Integration or Staging overwrite your Production code.\nVerify user account access\nWe recommend verifying your user account access and permissions set in the Integration environment. When adding Staging and Production to the Project Web Interface, the process includes all user accounts and settings. You can modify the settings and values for these environments after they are added.\n\nLog in to your Platform.sh account.\nFrom your project, click Master to view the environment information and settings.\nClick  Configure environment.\nClick the Users tab to review the user accounts and permission configurations.\nAdd, delete, or update users, if needed.\n\nPrepare variables\nWhen we convert your project to the new Project Web Interface, we add variables from Development environment to the Staging and Production environments. You can review, modify, and add variables through the current Project Web Interface prior to conversion.\n\nLog in to your Platform.sh account.\nFrom your project, click the master branch to view the environment information and settings.\nClick  Configure environment.\nOn the Variables tab, review the environment variables.\nTo create a new variable, click Add Variable.\nTo update an existing variable, click Edit next to the variable.\n\nFor environment-specific variables, including sensitive data and values, you can add those variables after we update your Project Web Interface. Environment variables defined in .platform.app.yaml or a .environment file will continue to work. You can add and manage these variables via SSH and CLI commands directly into the Staging and Production environments.\nEnter a ticket for updating the Project Web Interface\nEnter a Support ticket with the suggested title \u201cConnect Stg / Prod to Project\u2019s UI\u201d. In the ticket, request to have your project enabled with Staging and Production in the UI and confirm that you've taken the steps above to prepare your project.\nWe will review the infrastructure and settings, create user and environment variables for Staging and Production environments, and update the ticket with results.\nOnce started the process usually takes less than an hour.  There should be no downtime on your production site, although you should not push any code to Git while the upgrade is in progress.\nWhen done, you can access review your project through the Project Web Interface.\n(Optional) Migrate environment variables\nAfter conversion, you can manually migrate specific environment variables for Staging and Production.\n\nOpen a terminal and checkout a branch in your local environment.\nList all environment variables: platform variable:list\nLog in to your Platform.sh account.\nClick the Projects tab and the name of your project.\nClick the Staging or Production environment.\nOn the Variables tab, review the environment variables.\nEnter the variable name and value.\nSelect the Override checkbox if you want variables in the Project Web Interface to override local CLI or database values.\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Dedicated", "title": "Differences from the Grid", "url": "/dedicated/overview/grid.html", "documentId": "38d39828ca4507e83607a1479481f60edc8e3542", "text": "\n                        \n                            \n                                \n                                \n                                Differences from Platform.sh on the Grid\nWhen using Platform.sh Dedicated, a few configuration options and tools function differently from Platform.sh on the Grid, aka the Development Environment.\nPHP\nPlatform.sh Dedicated comes with pdo, apcu, curl, gd, imagick, ldap, mcrypt, mysqli, redis, soap, and opcache extensions enabled by default.\nIn addition, we can enable enchant, gearman, geoip, gmp, http, pgsql, pinba, pspell, recode, tidy, xdebug, oci8 (PHP5.6 only), or any extension with a pre-existing package in the Debian Apt repository if desired.  Please request such extensions via a ticket.\nCustom php.ini files are not supported on Platform.sh Dedicated. However, all PHP options that can be changed at runtime are still available. For example the memory limit can be changed using ini_set('memory_limit','1024M');\nPHP options that can we can change via support ticket include:\n\nmax_execution_time\nmax_input_time\nmax_input_vars\nmemory_limit\npost_max_size\nrequest_order\nupload_max_filesize\n\nXdebug\nPlatform.sh runs a second PHP-FPM process on all Dedicated clusters that has Xdebug enabled, but is only used if a request includes the appropriate Xdebug header.  That means it's safe to have Xdebug \"always on\", as it will be ignored on most requests.\nTo obtain the key you will need to file a ticket to have our support team provide it for you.  Staging and Production have separate keys.  Set that key in the Xdebug helper for your browser, and then whenever you have Xdebug enabled the request will use the alternate development PHP-FPM process with Xdebug.\nCron tasks may be interrupted by deploys\nOn Platform.sh Grid projects, a running cron task will block a deployment until it is complete.  On Platform.sh Dedicated, however, a deploy will terminate a running cron task.\nSpecifically, when a deploy to either Production or Staging begins, any active cron tasks are sent a SIGTERM message so that they can terminate gracefully if needed.  If they are still running 2 seconds later a SIGKILL message will be sent to forcibly terminate the process.\nFor that reason, it's best to ensure your cron tasks can receive a SIGTERM message and terminate gracefully.\nConfiguration &amp; change management\nSome configuration parameters for Dedicated clusters cannot be managed via the YAML configuration files, and for those parameters you will need to open a support ticket to have the change applied.  Further, the .platform/routes.yaml and .platform/services.yaml files do not automatically apply.  Those will apply on the development environments but not on the staging and production instances.  Any existing service upgrades or new service additions to staging and production will require a support ticket.\nIt is possible to run different configurations for some (but not all) options between staging and production, such as cron tasks.  By default we will make configuration changes to both instances unless you request otherwise.\nSpecifically:\n\nCron commands\nWorker instances\nService versions and configuration (everything in .platform/services.yaml)\nRoute, domain, and redirect configuration (everything in .platform/routes.yaml)\nApplication container version\nAdditional PHP extensions\nWeb server configuration (the web.locations section of .platform.app.yaml)\n\nCron\nCron tasks may run up to once per minute.  (They are limited to once every 5 minutes on Platform.sh Grid.)\nCron tasks are always interpreted in UTC time.\nLogs\nLogs are available on the Dedicated Cluster at a different path than on Platform.sh Grid.  Specifically, the can be found in:\n/var/log/platform/&lt;application-name&gt;/\nThis folder contains the application, cron, error and deployment logs.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Docs", "title": "Changelog", "url": "/changelog.html", "documentId": "4bdc38a38b93f36b26bbe017003b90482796b24a", "text": "\n                        \n                            \n                                \n                                \n                                Changelog\n2020\n\nApril 2020\n\nWe now offer Xdebug on PHP containers.\n\n\nMarch 2020\n\nGo 1.14: We now support Go 1.14.\nRuby 2.7: We now support Ruby 2.7.\n.NET Core: We now support .NET Core 3.1.\nMemcached 1.6: We now support Memcached 1.6.\nSolr 8.4: We now support Solr 8.4.\n\n\nFebruary 2020\n\nMemcached 1.5: We now support Memcached 1.5.\nCharacter set and collation are now configurable on MySQL/MariaDB.\n\n\nJanuary 2020\n\nRabbitMQ: We now support RabbitMQ virtual host configuration\n\n\n\n2019\n\nDecember 2019\n\nPython 3.8: We now support Python 3.8.\nNode.js 12: We now support Node.js 12.\nRabbitMQ 3.8: We now support RabbitMQ 3.8.\n\n\nOctober 2019\n\nPHP 7.4: We now support PHP 7.4.\n\n\nOctober 2019\n\nProjects can now have larger application containers in development environments.\n\n\nSeptember 2019\n\n\"Dark mode\" now available in the Console.\nGo 1.13: We now support Go 1.13.\n\n\nJuly 2019\n\nElasticsearch 7.2: We now support Elasticsearch 7.2.\nElasticsearch 5.2 and 5.4 support is now deprecated\nKafka 2.2: We now support Kafka 2.2.\nJava 13: We now support Java 13.\n\n\nJune 2019\n\nJava: We support and documented the use of Java runtimes 8, 11, and 12, that includes examples that use the Java Config Reader.\nHeadless Chrome: Users can now define a Headless Chrome service to access a service container with a headless browser, which can be used for automated UI testing.\n\n\nMay 2019\n\nInfluxDB: We now support InfluxDB 1.7.\nSolr 7 &amp; 8: We now support Solr 7.7 and 8.0.\n\n\nApril 2019\n\nNetwork storage service: Users can now define a Network storage service for sharing files between containers.\nKafka message queue service: Users can now define a Kafka service for storing, reading and analysing streaming data.\nManagement Console: Images and wording updated throughout entire documentation alongside Management Console release.\n\n\nMarch 2019\n\nRuby 2.6: A new version of Ruby is now available.\nGo 1.12: We now support Go 1.12.\nElasticsearch 6.5: We now support Elasticsearch 6.5.\n\n\nJanuary 2019\n\nRabbitMQ 3.7: We now support RabbitMQ 3.7.\nSolr 7: We now support Solr 7.6.\nVarnish: We now offer Varnish 5.2 and 6.0.\n\n\n\n2018\n\nDecember 2018\n\nElasticsearch 5.4: We now offer Elasticsearch 5.4.\nImproved Bash support: Bash history on application containers now persists between logins.\nPHP 7.3: We now support PHP 7.3.\nPostgreSQL 10.0 and 11.0: We now support PostgreSQL 10.0 and 11.0 with an automated upgrade path.\nRuby 2.5 out of beta: We now fully support Ruby 2.5.\n\n\nOctober 2018\n\nRedis updates: Redis 4.0 and 5.0 are now supported.\nGo language support: Go is now a fully supported language platform.\n\n\nSeptember 2018\n\nPython 3.7 support: We now support Python 3.7.\n\n\nAugust 2018\n\nNew public Canadian region: Our new Canadian region is now open for business.\n\n\nJuly 2018\n\nSecurity and Compliance: We have created a new \"Security and Compliance\" section to help customers address common questions relating to GDPR, Data Collection, Data Retention, Encryption, and similar topics.\n\n\nJune 2018\n\nNode.js 10: We now offer Node.js version 10.  All releases in the 10.x series will be included in that container.\nMongoDB 3.6: We now offer MongoDB 3.2, 3.4, and 3.6.  Note that upgrading from MongoDB 3.0 requires upgrading through all intermediary versions.\n\n\nMarch 2018\n\nWeb Application Firewall (WAF): Platform.sh is securing your applications and you don't need to change anything. Read more on our blog post.\n\n\nFebruary 2018\n\npost_deploy hook added: Projects can now run commands on deploy but without blocking new requests.\n\n\n\n2017\n\nDecember 2017\n\nNew project subdomains: The routes generated for subdomains and literal domains in development environments will now use . instead of translating them to ---, for projects created after this date.\n!include tag support in YAML files: All YAML configuration files now support a generic !include tag that can be used to embed one file within another.\nExtended mount definitions: A new syntax has been added for defining mount points that is more self-descriptive and makes future extension easier.\nBlocking older TLS versions: It is now possible to disable support for HTTPS requests using older versions of TLS.  TLS 1.0 is known to be insecure in some circumstances and some compliance standards require a higher minimum supported version.\n{all} placeholder for routes: A new placeholder is available in routes.yaml files that matches all configured domains.\nGitLab source code integration: Synchronize Git repository host on GitLab to Platform.sh.\n\n\nNovember 2017\n\nPHP 7.2 supported: With the release of PHP 7.2.0, Platform.sh now offers PHP 7.2 containers on Platform Professional.\n\n\nSeptember 2017\n\nHealth notifications: Low-disk warnings will now trigger a notification via email, Slack, or PagerDuty.\n\n\nAugust 2017\n\nWorker instances: Applications now support worker instances.\n\n\nJuly 2017\n\nNode.js 8.2: Node.js 8.2 is now available.\n\n\nJune 2017\n\nMemcache 1.4: Memcache 1.4 is now available as a caching backend.\nCustom static headers in .platform.app.yaml: Added support for setting custom headers for static files in .platform.app.yaml.  See the example for more information.\n\n\nMay 2017\n\nCode-driven variables in .platform.app.yaml: Added support for setting environment variables via .platform.app.yaml.\nPython 3.6, Ruby 2.4, Node.js 6.10: Added support for updated versions of several languages.\n\n\nApril 2017\n\nSupport for automatic SSL certificates: All production environments are now issued an SSL certificate automatically through Let's Encrypt.  See the routing documentation for more information.\nMariaDB 10.1: MariaDB 10.1 is now available (accessible as mysql:10.1).  Additionally, both MariaDB 10.0 and 10.1 now use the Barracuda file format with innodb_large_prefix enabled, which allows for much longer indexes and resolves issues with some UTF-8 MB use cases.\n\n\nMarch 2017\n\nElasticsearch 2.4 and 5.2 with support for plugins: Elasticsearch 2.4 and 5.2 are now available.  Both have a number of optional plugins avaialble.  See the Elasticsearch documentation for more information.\nInfluxDB 1.2: A new service type is available for InfluxDB 1.2, a time-series database.  See the InfluxDB documentation for more information.\n\n\nFebruary 2017\n\nHHVM 3.15 and 3.18: Two new HHVM versions are now available.\n\n\nJanuary 2017\n\nSupport for Multiple MySQL databases and restricted users: MySQL now supports multiple databases, and restricted users per MySQL service.  See the MySQL documentation for details or read our blog post.\n\nSupport for Persistent Redis services: Added a redis-persistent service that is appropriate for persistent key-value data. The redis service is still available for caching.  See the Redis documentation for details.\n\nSupport Apache Solr 6.3 with multiple cores: Added an Apache 6.3 service, which can be configured with multiple cores.  See the Solr documentation for details.\n\nSupport for HTTP/2: Any site configured with HTTPS will now automatically support HTTP/2.  Read more on our blog post.\n\n\n\n2016\n\nDecember 2016\n\nSupport Async PHP: Deploy applications like ReactPHP and Amp which allow PHP to run as a single-process asynchronous process.  Read more on our blog post.\nPthreads: Multithreaded PHP: Our PHP 7.1 containers are running PHP 7.1 ZTS, and include the Pthreads extension. Read more on our blog post.\nPHP 7.1: Service is documented here.\nSupport .environment files: This file will get sourced as a bash script by the system when a container boots up, as well as on all SSH logins. Feature is documented here.\nSupport web.commands.start for PHP: That option wasn't available for PHP as PHP only has one applicable application runner, PHP-FPM. It is now available for PHP.  Read more on our blog post.\n\n\nNovember 2016\n\nCustomizable build flavor: Added a none build flavor which will not run any specific command during the build process.  Use it if your application requires a custom build process which can be defined in your build hook. Read more in our blog post.\n\n\nOctober 2016\n\nPostgreSQL 9.6: Service is documented here.\nPostgreSQL extensions: Read more in our blog post.\nNode.js 6.8: Language is documented here.\n\n\nSeptember 2016\n\nPython 2.7 &amp; 3.5: Language is documented here.\nRuby 2.3: Language is documented here.\n\n\nAugust 2016\n\nSupport Gitflow: Read more in our blog post.\n\n\nJuly 2016\n\nBlock Httpoxy security vulnerability: We bypass the Httpoxy security vulnerability by blocking the Proxy header from incoming HTTP headers. Read more in our blog post.\nRemove default configuration files: We are removing the default configuration files that were previously used if your project didn't include one. You now need to include configuration files to deploy your applications on Platform.sh. Read more in our blog post.\n\n\nJune 2016\nJune update is summarized in our blog post.\n\nNew PLATFORM_PROJECT_ENTROPY variable: New variable which has a random value, stable throughout the project's life. It can be used for Drupal hash salt for example (in our Drupal 8 example). It is documented here\nExtend PLATFORM_RELATIONSHIPS variable: Expose the hostname and IP address of each service in the PLATFORM_RELATIONSHIPS environment variable.\nServices updates: Update MongoDB client to 3.2.7, Node.js to 4.4.5, Blackfire plugin to 1.10.6, Nginx to 1.11.1.\n\n\nMay 2016\nMay update is summarized in our blog post.\n\nPre-warms Composer cache before executing Composer: The composer build flavor now pre-warms the Composer cache before executing Composer.\nNew image processing packages (advancecomp, jpegoptim, libjpeg-turbo-progs, optipng, pngcrush): Various image processing packages were added: advancecomp, jpegoptim, libjpeg-turbo-progs, optipng, pngcrush.\nSecurity updates: Including imagetragick, glibc issue, various Java, OpenSSL and OpenSSH issues, along with some Git CLI vulnerabilities.\n\n\nApril 2016\n\nWhite label capabilities (Magento Enterprise Cloud Edition): Support for Platform.sh white label offering.  First launch at Magento Imagine 2016 in Las Vegas of Magento Enterprise Cloud Edition.\n\n\nMarch 2016\n\nCloudWatt deployment: Platform.sh is now available on Cloudwatt Orange Business Services hosted infrastructure. Read more in our blog post.\n\n\nJanuary 2016\n\nRedis 3.0: Service is documented here.\n\n\n\n2015\n\nDecember 2015\n\nNode.js 0.12, 4.4 &amp; 6.2: Read more in our blog post\n\n\nNovember 2015\n\nJava Ant &amp; Maven build scripts: Java Ant and Maven build scripts is supported for PHP 5.6 and up.  Your application can pull and use most of Java dependency. Read more in our blog post\n\n\nOctober 2015\n\nMariaDB/MySQL 10.0: Service is documented here.\nMongoDB 3.0: Service is documented here.\n\n\nSeptember 2015\n\nPHP 5.4, 5.5 &amp; 5.6: Read more in our blog post\nRabbitMQ 3.5: Service is documented here.  Read more in our blog post\nHHVM 3.9 &amp; 3.12: Read more in our blog post\n\n\nJuly 2015\n\nDocumentation 3.0 release: Read more in our blog post.\n\n\nJune 2015\n\nBitbucket integration: This add-on allows you to deploy any branch or pull request on a fully isolated Platform.sh environment with a dedicated URL.  Read more in Bitbucket's blog post.\nPostgreSQL 9.3: Service is documented here.\n\n\nMay 2015\n\nUI 2.0 release: Read more in our blog post.\n\n\nFebruary 2015\n\nBlackfire integration: PHP applications come pre-installed with the Blackfire Profiler developed by SensioLabs.  Read more in our blog post.\n\n\nJanuary 2015\n\nRedis 2.8: Service is documented here.\n\n\n\n2014\n\nNovember 2014\n\nRead more about this release in our blog post.\n\nHTTP caching per route: Support for HTTP caching at the web server level, finely configurable on a per-route basis.\nCustom PHP configurations: Support for tweaking the PHP configuration, by enabling / disabling extensions and shipping your own php.ini.\nBuild dependencies: Support for specifying build dependencies, i.e. PHP, Python, Ruby or Node.js tools (like sass, grunt, uglifyjs and more) that you want to leverage to build your PHP application.\n\nElasticsearch 0.90, 1.4 &amp; 1.7: Service is documented here.\n\n\n\nOctober 2014\n\nAutomated protective block: Platform.sh provides a unique approach to protect your applications from known security issues.  An automated protective blocking system which works a bit like an antivirus: it compares the code you deploy on Platform.sh with a database of signatures of known security issues in open source projects. This feature is documented here.  Read more in our blog post.\nSolr 4.10: Service is documented here.\n\n\nJuly 2014\n\nMariaDB/MySQL 5.5: Service is documented here.\nSolr 3.6: Service is documented here.\n\n\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Dedicated", "title": "Backups", "url": "/dedicated/overview/backups.html", "documentId": "734da73bcb5ba564036b4efbe367683df2eac03a", "text": "\n                        \n                            \n                                \n                                \n                                Backups\nPlatform.sh takes a byte-for-byte snapshot of Dedicated production environments every six (6) hours.  They are retained on a sliding scale, so more recent time frames have more frequent backups.\n\n\n\nTime frame\nBackup retention\n\n\n\n\nDays 1-3\nEvery backup\n\n\nDays 4-6\nOne backup per day\n\n\nWeeks 2-6\nOne backup per week\n\n\nWeeks 8-12\nOne bi-weekly backup\n\n\nWeeks 12-22\nOne backup per month\n\n\n\nPlatform.sh Dedicated creates the backup using snapshots to encrypted elastic block storage (EBS) volumes. An EBS snapshot is immediate, but the time it takes to write to the simple storage service (S3) depends on the volume of changes.\n\nRecovery Point Objective (RPO) is 6 hours (maximum time to last backup).\nRecovery Time Objective (RTO) depends on the size of the storage. Large EBS volumes take more time to restore.\n\nThese backups are only used in cases of catastrophic failure and can only be restored by Platform.sh. A ticket must be opened by the customer to request a restoration.\nThe restoration process may take a few hours, depending on the infrastructure provider in use.  In the ticket, specify if you want backups of files, MySQL, or both.  Uploaded files will be placed in an SSH-accessible directory on the Dedicated Cluster.  MySQL will be provided as a MySQL dump file on the server.  You may restore these to your site at your leisure.  (We will not proactively overwrite your production site with a backup; you are responsible for determining a \"safe\" time to restore the backup, or for selectively restoring individual files if desired.)\nCustomers are welcome to make their own backups using standard tools (mysqldump, rsync, etc.) at their own leisure.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Dedicated", "title": "Onboarding", "url": "/dedicated/overview/onboarding.html", "documentId": "9a60dab330ce8384ad2049f621873f21c9371871", "text": "\n                        \n                            \n                                \n                                \n                                Onboarding process\nOn-Boarding a new Dedicated client is a three phase process that begins the moment your contract is closed with your sales representative.  As part of the onboarding service, you will continue to work with the Solutions Engineer who was present during the technical discovery and analysis during the pre-sales phase.  During the entire on-boarding process, your Solutions Engineer is available to assist with questions and prioritize any tickets that may be submitted through the help desk.\n\n\n Phase Meetings Description \n\n\nSetup\n\n\n  \n    Introduction\n    Hand-off\n  \n\n\n\n  The Solutions Engineer briefly reviews the development workflow that was discussed during the sales process.  A dedicated Slack channel for your team\u2019s onboarding process will be set up and members of your team invited.  This will allow for a quicker feedback loop during the onboarding process.\n  The Solutions Engineer will introduce you to the Platform Dedicated workflow process, provision resources, and hand them off to the client.\n\n\n\n\n\nDevelopment\n\n\n  \n    Developer Workflow Consultation\n  \n\n\n\n  The customer has access to all the resources necessary to develop, migrate, and test the project on the Platform.sh infrastructure - development, staging, and production.  If necessary, developers may request an additional training session (held by the Solutions Engineer) to discuss the development workflow and best practices in detail.\n  Once the application is live on your staging environment, a staging CDN distribution will be created so that proper configuration of your CDN can begin.\n\n\n\nGo-Live\n\n\n  \n    Pre-launch\n    Debrief\n  \n\n\n\n  Customer notifies their Solutions Engineer through a support ticket of the intention to go live.  The Solutions Engineer may also reach out to the customer a few days before the approximate cut-over date if one was provided.  The Solutions Engineer reviews the infrastructure, notes any risks, and discusses the go live process.\n  The production configuration of your CDN will be created with the configuration mirroring that of your staging distribution, and will be configured to pull from your production environment.\n\n\n\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Dedicated", "title": "Security and Privacy", "url": "/dedicated/overview/security.html", "documentId": "5fcf33abfc4ce970a73f3b77a231244e638ae262", "text": "\n                        \n                            \n                                \n                                \n                                Security &amp; Data privacy\nUpdates &amp; upgrades\nPlatform.sh updates the core software of the Dedicated Cluster (operating system, web server, PHP, MySQL, etc.) periodically, and after any significant security vulnerability is disclosed.  These updates are deployed automatically with no additional work required by the user.  We attempt to maintain parity with your development environment, but we do not guarantee absolute parity of point versions of your Dedicated environments with their corresponding development environments.  I.e, your development environment may have a PHP container running 5.6.30, but your production environment may lag behind at 5.6.22.  We can upgrade point releases on request and will always upgrade the underlying software in the event of security release.\nUpdates to application software (PHP code, Javascript, etc.) are the responsibility of the customer.\nProject isolation\nAll Dedicated Clusters are single-tenant.  The three VMs are exclusively used by a single customer and each Dedicated cluster is launched into its own isolated network (VPC on AWS, equivalent on other providers).  The network is firewalled to incoming connections; only ports 22 (SSH), 80 (HTTP), 443 (HTTPS), 2221 (SFTP) are opened to incoming traffic.  There are no exceptions for this rule, so any incoming web service requests, ETL jobs, or otherwise will need to transact over one of these protocols.\nOutgoing TCP traffic is not firewalled.  Outgoing UDP traffic is disallowed.\nThe Development Environment deploys each branch as a series of containers hosted on a shared underlying VM.  Many customers will generally share the same VM.  However, all containers are whitelisted to connect only to other containers in their same environment, and even then only if an explicit \"relationship\" has been defined by the user via configuration file.\nSecurity incident handling procedure\nShould Platform.sh become aware of a security incident \u2014 such as an active or past hacking attempt, virus or worm, or data breach \u2014 senior personnel including the CTO will be promptly notified.  \nOur security incident procedures include isolating the affected systems, collecting forensic evidence for later analysis including a byte-for-byte copy of the affected system, and finally restoring normal operations. Once normal service is restored we perform a root cause analysis to determine exactly what happened.  A Reason for Outage report may be provided to the customer upon request that summarizes the incident, cause, and steps taken.\nPlatform.sh will cooperate with relevant law enforcement, and inform law enforcement in the event of an attempted malicious intrusion.  Depending on the type of incident the root cause analysis may be conducted by law enforcement rather than Platform.sh personnel.\nPlatform.sh will endeavor to notify affected customers within 24 hours in case of a personal data breach and 72 hours in case of a project data breach.\nUnder the European General Data Protection Regulation (GPDR), Platform.sh is required to notify our supervising authority within 72 hours of a discovered breach that may result in risk to the rights and freedoms of individuals.  Our supervising authority is the French Commission Nationale de l'Informatique et des Libert\u00e9s.\nAudit trail\nAs part of the security incident process we record a log of all steps taken to identify, isolate, and respond to the incident.  This log may include:\n\nA byte-for-byte copy of the affected systems\nHow the intrusion was detected\nThe steps taken to contain the intrusion\nAny contact with 3rd parties, including law enforcement\nAny conclusions reached regarding the root cause\n\nEncryption\nAWS\nAWS EBS Volumes are encrypted on Platform.sh Dedicated sites are fully encrypted. Keys are managed by AWS\u2019s KMS (Key Management Service). AWS automatically rotates these keys every three years. In some cases, temporary storage (eg swap) is stored on unencrypted local storage volumes.\nAzure\nBy default, data is encrypted using Microsoft Managed Keys for Azure Blobs, Tables, Files and Queues.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Dedicated", "title": "Incident Monitoring", "url": "/dedicated/overview/monitoring.html", "documentId": "b2634ca3bdac22921bf33970f4439723d1ad9af3", "text": "\n                        \n                            \n                                \n                                \n                                Resource and incident monitoring\nAll of our Dedicated clusters are monitored 24/7 to ensure uptime and to measure server metrics such as available disk space, memory and disk usage, and several dozen other metrics that give us a complete picture of the health of your application\u2019s infrastructure.  Alerting is set up on these metrics, so if any of them goes outside of normal bounds an operations engineer can react accordingly to maintain the uptime and performance of your cluster.\nThese alerts are sent to our support and operations teams, and are not directly accessible to the customer.\nMonitoring systems\nPlatform.sh uses well-known open source tooling to collect metrics and to alert our staff if any of these metrics goes out of bounds.  That includes the use of Munin for collecting time-series data on server metrics, and dashboarding of these metrics so that our team can monitor trends over time.  It also includes use Nagios as a point in time alerting system for our operations staff.\nThese tools are internal Platform.sh tools only.\nA third-party availability monitoring system is configured for every Dedicated project. The customer can be subscribed to email alerts upon request.\nApplication performance monitoring\nPlatform.sh does not provide application-level performance monitoring.  However, we strongly recommend that customers leverage application monitoring themselves.\nPlatform.sh is a Blackfire.io reseller. You can contact your sales representative to get a quote for whatever size cluster is running your application. Platform.sh also supports New Relic APM. After you have signed up with New Relic and gotten your license key, open a support ticket so that it can be installed on your project. New Relic infrastructure monitoring is not supported.\nAvailability incident handling procedure\nAutomated monitoring may trigger alerts that will page the on-call engineer, or the end-user may file an urgent priority ticket.  PagerDuty will page the on-call using several methods. The on-call engineer responds to the alerts and begins to triage the issue.\nCloud infrastructure issues are handled by the customer success team. Application problems are escalated to an application support specialist if an agreement is part of the customer subscription.  Otherwise, they are returned to the user and may be downgraded.\nWhen a Urgent/High issue is escalated it will page the on-call application support specialist.  Application support may also escalate infrastructure issues back as Urgent/High.\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Dedicated", "title": "Overview", "url": "/dedicated/overview.html", "documentId": "2ae48d1fc9d2528cdc620eb9f0ba4db02b8d8afc", "text": "\n                        \n                            \n                                \n                                \n                                Platform.sh Dedicated\nPlatform.sh Dedicated is a robust, redundant layer on top of Platform.sh Professional.  It is well-suited for those who like the Platform.sh development experience but need more resources and redundancy for their production environment.  It is available only with an Enterprise contract.\nPlatform.sh Dedicated consists of two parts: The Development Environment and the Dedicated Cluster.\nThe Development Environment\nThe Development Environment is a normal Platform.sh Grid account, with all of the capabilities and workflows of Platform.sh Professional.  The one difference is that the master branch will not be associated with a domain and thus will never be \"production\".\nThe Dedicated Cluster\nThe Dedicated Cluster is a three-Virtual Machine redundant configuration provisioned by Platform.sh for each customer.  Every service is replicated across all three virtual machines in a failover configuration (as opposed to sharding), allowing a site to remain up even if one of the VMs is lost entirely.\nThe build process for your application is identical for both the Development Environment and the Dedicated Cluster.  However, because the VMs are provisioned by Platform.sh, not as a container, service configuration must be done by Platform.sh's Customer Success team.  By and large the same flexibility is available but only via filing a support ticket.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Tutorials", "title": "Migrating to Platform.sh", "url": "/tutorials/migrating.html", "documentId": "0835509706743355e6563bf4fe91ad49347e3e73", "text": "\n                        \n                            \n                                \n                                \n                                Migrating to Platform.sh\nMoving an already-built site to Platform.sh is generally straightforward.  For the most part, the only part that will vary from one framework to another is the details of the Platform.sh configuration files.  See the Featured Frameworks section or our project templates for more project-specific documentation.\nPreparation\nFirst, assemble your Git repository as appropriate, on your master branch.  Be sure to include the Platform.sh configuration files, as you will not be able to push the repository to Platform.sh otherwise!\nFor some applications, such as Drupal you will need to dump configuration to files before proceeding.  You will also need to provide appropriate configuration to read the credentials for your services at runtime and integrate them into your application's configuration.  The details of that integration will vary between systems.  Be sure to see the appropriate project templates for our recommended configuration.\n\nGo Templates\nJava Templates\nNode.js Templates\nPHP Templates\nPython Templates\n\nIn the management console, click + Add project to create a new Platform.sh project. When asked to select a template pick \"Create a blank project\".\nPush your code\nWhen creating a new project, the management console will provide two commands to copy and paste similar to the following:\ngit remote add platform nodzrdripcyh6@git.us.platform.sh:nodzrdripcyh6.git\ngit push -u platform master\n\nThe first will add a Git remote for the Platform.sh repository named platform.  The name is significant as the Platform.sh CLI will look for either platform or origin to be the Platform.sh repository, and some commands may not function correctly otherwise.  The second will push your repository's master branch to the Platform.sh master branch.  Note that a project must always start with a master branch, or deploys to any other environment will fail.\nWhen you push, a new environment will be created using your code and the provided configuration files.  The system will flag any errors with the configuration if it can.  If so, correct the error and try again.\nImport your database\nYou will need to have a dump or backup of the database you wish to start from.  The process is essentially the same for each type of persistent data service.  See the MySQL, PostgreSQL, or MongoDB documentation as appropriate.\nImport your files\nContent files (that is, files that are not intended as part of your code base so are not in Git) can be uploaded to your mounts using the Platform.sh CLI or by using rsync. You will need to upload each directory's files separately.  Suppose for instance you have the following file mounts defined:\nmounts:\n    'web/uploads':\n        source: local\n        source_path: uploads\n    'private':\n        source: local\n        source_path: private\n\nWhile using the CLI and rsync are the most common solutions for uploading files to mounts, you can also use SCP.\nPlatform.sh CLI\nThe easiest way to import files to your project mounts is by using the Platform.sh CLI mount:upload command. To upload to each of directories above, we can use the following commands.\nplatform mount:upload --mount web/uploads --source ./uploads\nplatform mount:upload --mount private --source ./private\n\nrsync\nYou can also use rsync to upload each directory.  The platform ssh --pipe command will return the SSH URL for the current environment as an inline string that rsync can recognize. To use a non-default environment, use the -e switch after --pipe.  Note that the trailing slash on the remote path means rsync will copy just the files inside the specified directory, not the directory itself.\nrsync -az ./private `platform ssh --pipe`:/app/private/\nrsync -az ./web/uploads `platform ssh --pipe`:/app/web/uploads\n\n\nnotes\nIf you're running rsync on MacOS, you should add --iconv=utf-8-mac,utf-8 to your rsync call.\n\nSee the rsync documentation for more details on how to adjust the upload process.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Tutorials", "title": "Third-party resources", "url": "/tutorials/third-party.html", "documentId": "a0c61312491a3c5a12ecfd2f8774d4adc72fa243", "text": "\n                        \n                            \n                                \n                                \n                                Platform.sh Third-Party Resources\nThis is a Big List of known third party resources for Platform.sh. These resources are not vetted by Platform.sh, but may be useful for people working with the platform.\nBlogs\n\n\"The future of the PHP PaaS is here: Our journey to Platform.sh\", by Marcus Hausammann\nAn introduction to Platform.sh from Chris Ward\n\nGuides\nGetting started &amp; workflow\n\nSet up your Mac for Platform.sh using MAMP by @owntheweb\nHow Platform.sh can simplify your contribution workflow on GitHub by Micka\u00ebl Andrieu from Akeneo\nA guide in French on deploying to Platform.sh by Thomas Asnar [FR]\nNacho Digital has a guide on moving an existing site to Platform.sh\nAll the stuff you need for a pro-dev-flow using platform.sh as your deploy target  again by https://www.thinktandem.io\n\nWorking with Platform.sh\n\nHow to connect to your MySQL database using Sequel Pro\nHow to set up XDebug\nOfficial Symfony documentation on deploying to Platform.sh\nOfficial Sylius documentation on deploying to Platform.sh\nHow to install Apache Tika on Platform.sh\nHow to store complete logs at AWS S3 by Contextual Code\nAutomated SSL Certificates Export on Platform.sh by Contextual Code\nA Platform.sh region migration tool by Contextual Code\n\nDrupal\n\nModifying distribution make files for Platform.sh\nPlatform.sh Drupal 8 Development Workflow by @JohnatasJMO\nSyslogging is not supported on Platform.sh, instead, you can Log using Monolog to keep log files out of the database (and/or use whatever processors &amp; handlers you want)\n\nMagento\n\nDeploying Magento 2 with Redis on Platform.sh by @rafaelcgstz\n\nSylius\n\nThe Sylius documentation has a solid set of instructions for setting up Sylius with Platform.sh.\n\nExamples\nPlatform.sh lists maintained examples on its Github page, with some cross-referencing from http://docs.platform.sh. Examples listed below could work fine, or may be out-of-date or unmaintained. Use at your own risk.\nNodeJS\n\n\n\nFramework\nCredit\nDate added\n\n\n\n\nMEAN stack\n@OriPekelman\nMay 2017\n\n\n\nPython\n\n\n\nFramework\nCredit\nDate added\n\n\n\n\nPython Flask using gunicorn\n@etoulas\nMay 2017\n\n\nOdoo Open Source ERP and CRM\n@OriPekelman\nMay 2017\n\n\n\nPHP\n\n\n\nFramework\nCredit\nDate added\n\n\n\n\nAkeneo example\n@maciejzgadzaj\nMay 2017\n\n\nAPI Platform with a ReactJS client admin\n@GuGuss\nMay 2017\n\n\nBackdrop example\n@gmoigneu\nMay 2017\n\n\nHeadless Drupal 8 with Angular\n@GuGuss\nMay 2017\n\n\nHeadless Drupal 8 with React.js\n@systemseed\nAug 2018\n\n\nJoomla example\n@gmoigneu\nMay 2017\n\n\nLaravel example\n@JGrubb\nMay 2017\n\n\nMoodle example\n@JGrubb\nMay 2017\n\n\nMouf framework example\nThe Coding Machine\nMay 2017\n\n\nFlow Framework support package\nDominique Feyer\nJul 2017\n\n\nNeos CMS support package\nDominique Feyer\nJul 2017\n\n\nSilex example\n@JGrubb\nMay 2017\n\n\nSilverstripe example\n@gmoigneu\nMay 2017\n\n\nThunder example\nmaintained by the MD Systems team\nMay 2017\n\n\nWooCommerce example\n@Liip\nMay 2017\n\n\nGrav example\nMike Crittenden\nAugust 2017\n\n\n\nRuby\n\n\n\nFramework\nCredit\nDate added\n\n\n\n\nJekyll example\n@JGrubb\nMay 2017\n\n\n\nRust\n\n\n\nFramework\nCredit\nDate added\n\n\n\n\nRust with Rocket and webasm\nRoyall Spence\nJuly 2018\n\n\n\nIntegrations\n\nIntegrate GitLab with Platform.sh using Gitlab-CI, by @Axelerant\nRunning Behat tests from CircleCI to a Platform.sh environment, by Matt Glaman\nPlatform.sh's original (unsupported) scripts for GitLab https://gist.github.com/pjcdawkins/0b3f7a6da963c129030961f0947746c4. Platform.sh now supports Gitlab natively.\nAn adapter from platform.sh webhook to slack incoming webhook that can be hosted on a platform.sh app https://github.com/hanoii/platformsh2slack\nHow to call the NewRelic API on deploy (by @christopher-hopper)\nA helper utility for running browser based tests on CircleCI against a Platform.sh environment. https://github.com/xendk/dais\n\nTools &amp; development\n\nMySQL disk space monitor https://github.com/galister/platformsh_mysqlmon\nCreate deploy commands you can run from composer, using Symfony\nA small tool from Hanoii https://github.com/hanoii/drocal\nScript to sync a Drupal site from Production to Local https://github.com/pjcdawkins/platformsh-sync\nMatt Pope's Platform.sh automated mysql and files backup script\n\nDevelopment environments\n\nBeetbox, a pre-provisioned L*MP stack for Drupal and other frameworks, with Platform.sh CLI integration\nA Docker image with the Platform.sh CLI on it https://github.com/maxc0d3r/docker-platformshcli\nSome tips on using Platform.sh with DrupalVM https://github.com/geerlingguy/drupal-vm/issues/984\nVagrant with Ansiblefor Platform.sh, opinionated towards Drupal, by @mglaman.\n\nAnsible\n\nPlaybook for setting up Vagrant and VirtualBox for use with a Platform.sh project\nPixelArt's Platform.sh CLI role\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Tutorials", "title": "Changing regions", "url": "/tutorials/region-migration.html", "documentId": "9767e6abb17d7c4d9a338a85c5d78e824b6892c3", "text": "\n                        \n                            \n                                \n                                \n                                Region migration\nPlatform.sh is available in a number of different Regions.  Each region is a self-contained copy of Platform.sh in a single datacenter.  When you first create a project you can specify which region it should be in.\nPlatform.sh does not offer an automated way to migrate a project from one region to another after it is created.  However, the process to do so manually is fairly straightforward and scriptable.\nWhy migrate between regions?\n\nDifferent datacenters are located in different geographic areas, and you may want to keep your site physically close to the bulk of your user base for reduced latency.\nOnly selected regions offer European Sovereign hosting.  If you created a project in a non-Sovereign region you may need to migrate to a Sovereign region.\nSome regions are running older versions of the Platform.sh orchestration system that offers fewer features.  In particular, the US-1 and EU-1 regions do not offer XL and 2XL plans, self-terminating builds in case of a build process that runs too long, or distributing environments across different grid hosts.  If you are on one of those regions and desire those features you will need to migrate to the newer US-2 or EU-2 regions.\n\nScripted migration process\nAlthough not directly supported by Platform.sh, an agency named Contextual Code has built a bash migration script to automate most common configurations.  If your site is a typical single application with a single SQL database, the script should take care of most of the process for you.  (If you have additional backend systems you may need to do some additional work manually, as documented below.)\n0. Preparation\n\nPlan a timeframe in which to handle the migration.  You will want to avoid developing any new code during that period (as your Git repository will change) and be prepared for a brief site outage when you migrate.  You are essentially relaunching the site, just with the same host as previously.  Plan accordingly.\nSet your DNS Time-to-Live as low as possible.\n\n1. Create a new project\nCreate a new Platform.sh Project in the desired region.  You can initially create it as a Development project and change the plan size immediately before switching over or go ahead and use the desired size from the beginning.  When the system asks if you want to use an existing template or provide your own code, select provide your own code.  However, you do not need to push any code to it yet.  Note the new project's ID from the URL.\n2. Download and invoke the script\nDownload the Platform Migration tool from Contextual Code.  The README file explains the step it uses in more detail.  With a typical site it will carry you through the full process of transfering code, data, configuration, and the domain name to your new project.\nNote: You will still need to update your DNS record with your registrar to point to the new project when you are ready to go live.  Also be aware that the script does not transfer external integrations \u2014 such as health notifications or 3rd party Git provider integrations \u2014 so you will need to re-enable those manually.\n3. Remove the old project\nOnce the new project is running and the DNS has fully propagated you can delete the old project.\nManual migration process\n0. Preparation\n\nPlan a timeframe in which to handle the migration.  You will want to avoid developing any new code during that period (as your Git repository will change) and be prepared for a brief site outage when you migrate.  You are essentially relaunching the site, just with the same host as previously.  Plan accordingly.\nSet your DNS Time-to-Live as low as possible.\n\n1. Create and populate a new project\nCreate a new Platform.sh project in the desired region.  You can initially create it as a Development project and change the plan size immediately before switching over or go ahead and use the desired size from the beginning.  When the system asks if you want to use an existing template or provide your own code, select provide your own code.\nMake a Git clone of your existing project.  Then add a Git remote to the new project, using the Git URL shown in the management console.  Push the code for at least your master branch to the new project.  (You can also transfer other branches if desired.  That's optional.)\nAlternatively, if you are using a 3rd party Git repository (GitHub, BitBucket, GitLab, etc.), you can add an integration to the new project just as you did the old one.  It will automatically mirror your 3rd party repository exactly the same way as the old project and you won't need to update it manually.\nCopy your existing user files on the old project to your computer using rsync.  See the exporting page for details.  Then use rsync to copy them to the same directory on the new project.  See the migrating page for details.\nExport your database from the old project and import it into the new project.  Again, see the exporting and migration pages, as well as the instructions for your specific database services.\nRe-enter any project or environment variables you've defined on your old project in your new project.\nAdd any users to your new project that you want to continue to have access.\nIf you have any 3rd party integrations active, especially the Health Notification checks, add them to the new project.\n2. Maintain the mirror\nMost sites have generated data in Solr, Elasticsearch, or similar that needs to be regenerated.  Take whatever steps are needed to reindex such systems.  That may simply be allowing cron to run for a while, or your system may have a command to reindex everything faster.  That will vary by your application.\nYou can also periodically re-sync your data.  For rsync the process should be quite fast as long as you maintain your local copy of it, as rsync will transfer only content that has changed.  For the database it may take longer depending on the size of your data.\nDepending on your site's size and your schedule, you can have the old and new project overlapping for only an hour or two or several weeks.  That's up to you.  Be sure to verify that the new site is working as desired before continuing.\n3. Launch the new site\nOnce your new project is on the right production plan size you can cut over to it.  Add your domain name(s) to your new project.  If you have a custom SSL certificate you will need to add that at the same time.  (Because the projects are in separate regions it's safe to add the domain name to both at the same time, which reduces apparent downtime.)\nIf possible, put your site into read-only mode or maintenance mode.  Then do one final data sync (code and database) to ensure the new project starts with all fo the data from the old one.\nOnce the domain is set, update your DNS provider's records to point to the new site.  Run platform environment:info edge_hostname -p &lt;NEW_PROJECT_ID&gt; to get the domain name to point the CNAME at.\nIt may take some time for the DNS change and SSL change to propagate.  Until it does, some browsers may not see the new site or may get an SSL mismatch error.  In most cases that will resolve itself in 1-3 hours.\n4. Remove the old project\nOnce the new project is running and the DNS has fully propagated you can delete the old project.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Tutorials", "title": "Exporting data", "url": "/tutorials/exporting.html", "documentId": "19b6ca630786d3312d552202c83b288aa2b75c51", "text": "\n                        \n                            \n                                \n                                \n                                Exporting data\nPlatform.sh aims to be a great host, but we never want to lock you in to our service. Your code and your data belong to you, and you should always be able to download your site's data for local development, backup, or to \"take your data elsewhere\".\nDownloading code\nYour application's code is maintained in Git.  Because Git is a distributed system it is trivial to download your entire code history with a simple git clone or platform get command.\nDownloading files\nYour application runs on a read-only file system, so it cannot be edited.  That means there's nothing to download from most of it that isn't already in your Git repository.\nThe only files to download are from any writable file mounts you may have defined in your .platform.app.yaml file.  The easiest way to download those is using the rsync tool.  For instance, suppose you have a mounts section that defines one web-accessible directory and one non-web-accessible directory:\nmounts:\n    'web/uploads':\n        source: local\n        source_path: uploads\n    'private':\n        source: local\n        source_path: private\n\nUsing the CLI\nThe CLI provides a useful mount command for accessing mount data.\nplatform mount:list\nDownloading a mount is then as simple as running the following:\nplatform mount:download\nUsing rsync\nTo use rsync to download each directory, we can use the following commands.  The platform ssh --pipe command will return the SSH URL for the current environment as an inline string that rsync can recognize. To use a non-default environment, use the -e switch after --pipe.  Note that the trailing slash on the remote path means rsync will copy just the files inside the specified directory, not the directory itself.\nrsync -az `platform ssh --pipe`:/app/private/ ./private/\nrsync -az `platform ssh --pipe`:/app/web/uploads ./uploads/\n\n\nnotes\nIf you're running rsync on MacOS, you should add --iconv=utf-8,utf-8-mac to your rsync call.\n\nSee the rsync documentation for more details on how to adjust the download process.\nDownload data from services\nThe mechanism for downloading from each service (such as your database) varies.  For services designed to hold non-persistent information (such as Redis or Solr) it's generally not necessary to download data as it can be rebuilt from the primary data store.\nTo download data from persistent services (MySQL, PostgreSQL, MongoDB, or InfluxDB), see each service's page for instructions.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Tutorials", "title": "Authenticated Composer", "url": "/tutorials/composer-auth.html", "documentId": "7958adef45bbdf641e707cdc11fc96b782b7e497", "text": "\n                        \n                            \n                                \n                                \n                                Authenticated Composer repositories\nSome PHP projects may need to leverage a private, third party Composer repository in addition to the public Packagist.org repository.  Often, such third party repositories require authentication in order to download packages, and not everyone is comfortable putting those credentials into their Git repository source code (for obvious reasons).\nTo handle that situation, you can define a env:COMPOSER_AUTH project variable which allows you to set up authentication as an environment variable. The contents of the variable should be a JSON formatted object containing an http-basic object (see composer-auth specifications).\nThe advantage is that you can control who in your team has access to those variables.\nSpecify a third party repository in composer.json\nFor this example, consider that there are several packages we want to install from a private repository hosted at my-private-repos.example.com.  List that repository in your composer.json file.\n{\n    \"repositories\": [\n        {\n            \"type\": \"composer\",\n            \"url\": \"https://my-private-repos.example.com\"\n        }\n    ]\n}\n\nSet the env:COMPOSER_AUTH project variable\nSet the Composer authentication by adding a project level variable called env:COMPOSER_AUTH as JSON and available only during build time.\nThat can be done through the management console or via the command line, like so:\nplatform variable:create --level project --name env:COMPOSER_AUTH \\\n  --json true --visible-runtime false --sensitive true --visible-build true \\\n  --value '{\"http-basic\": {\"my-private-repos.example.com\": {\"username\": \"your-username\", \"password\": \"your-password\"}}}'\n\nThe env: prefix will make that variable appear as its own Unix environment variable available by Composer during the build process. The optional --no-visible-runtime flag means the variable will only be defined during the build hook, which offers slightly better security.\nNote: The authentication credentials may be cached in your project's build container, so please make sure you clear the Composer cache upon changing any authentication credentials. You can use the platform project:clear-build-cache command.\nBuild your application with Composer\nYou simply need to enable the default Composer build mode in your .platform.app.yaml:\nbuild:\n    flavor: \"composer\"\n\nIn that case, Composer will be able to authenticate and download dependencies from your authenticated repository.\nPrivate repository hosting\nTypically, a private dependency will be hosted in a private Git repository.  While Platform.sh supports private repositories for the site itself, that doesn't help for pulling in third party dependencies from private repositories unless they have the same SSH keys associated with them.\nFortunately, most private Composer tools (including Satis, Toran Proxy, and Private Packagist) mirror tagged releases of dependencies and serve them directly rather than hitting the Git repository.  Therefore as long as your dependencies specify tagged releases there should be no need to authenticate against a remote Git repository and there should be no authentication issue.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Wordpress", "title": "Redis", "url": "/frameworks/wordpress/redis.html", "documentId": "772d3230fd4e36062fe2f33ba55b1b272fc8d04c", "text": "\n                        \n                            \n                                \n                                \n                                Using Redis with WordPress\nThere are a number of Redis libraries for WordPress, only some of which are compatible with Platform.sh.  We have tested and recommend devgeniem/wp-redis-object-cache-dropin, which requires extremely little configuration.\nRequirements\nAdd a Redis service\nFirst you need to create a Redis service.  In your .platform/services.yaml file, add the following:\nrediscache:\n    type: redis:5.0\n\nThat will create a service named rediscache, of type redis, specifically version 5.0.\nExpose the Redis service to your application\nIn your .platform.app.yaml file, we now need to open a connection to the new Redis service.  Under the relationships section, add the following:\nrelationships:\n    redis: \"rediscache:redis\"\n\nThe key (left side) is the name that will be exposed to the application in the PLATFORM_RELATIONSHIPS variable.  The right hand side is the name of the service we specified above (rediscache) and the endpoint (redis).  If you named the service something different above, change rediscache to that.\nAdd the Redis PHP extension\nBecause the Redis extension for PHP has been known to have BC breaks at times, we do not bundle a specific verison by default.  Instead, we provide a script to allow you to build your desired version in the build hook.  See the PHP-Redis page for a simple-to-install script and instructions.\nAdd the Redis library\nIf using Composer to build WordPress, you can install the WP-Redis library with the following Composer command:\ncomposer require devgeniem/wp-redis-object-cache-dropin\n\nThen commit the resulting changes to your composer.json and composer.lock files.\nConfiguration\nTo enable the WP-Redis cache the object-cache.php file needs to be copied from the downloaded package to the wp-content directory.  Add the following line to the bottom of your build hook:\ncp -r wp-content/wp-redis-object-cache-dropin/object-cache.php web/wp/wp-content/object-cache.php\n\nIt should now look something like:\nhooks:\n    build: |\n        set -e\n\n        bash install-redis.sh 5.1.1\n\n        cp -r wp-content/wp-redis-object-cache-dropin/object-cache.php web/wp/wp-content/object-cache.php\n\nNext, place the following code in the wp-config.php file, somewhere before the final require_once(ABSPATH . 'wp-settings.php'); line.\nif (!empty($_ENV['PLATFORM_RELATIONSHIPS']) &amp;&amp; extension_loaded('redis')) {\n    $relationships = json_decode(base64_decode($_ENV['PLATFORM_RELATIONSHIPS']), true);\n\n    $relationship_name = 'redis';\n\n    if (!empty($relationships[$relationship_name][0])) {\n        $redis = $relationships[$relationship_name][0];\n        define('WP_REDIS_CLIENT', 'pecl');\n        define('WP_REDIS_HOST', $redis['host']);\n        define('WP_REDIS_PORT', $redis['port']);\n    }\n}\n\nThat will define 3 constants that the WP-Redis extension will look for in order to connect to the Redis server.  If you used a different name for the relationship above, change $relationship_name accordingly.  This code will have no impact when run on a local development environment.\nThat's it.  There is no Plugin to enable through the WordPress administrative interface.  Commit the above changes and push.\nVerifying Redis is running\nRun this command in a SSH session in your environment redis-cli -h redis.internal info. You should run it before you push all this new code to your repository.\nThis should give you a baseline of activity on your Redis installation. There should be very little memory allocated to the Redis cache.\nAfter you push this code, you should run the command and notice that allocated memory will start jumping.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Frameworks", "title": "WordPress", "url": "/frameworks/wordpress.html", "documentId": "899c9915e2de02669de39e7ac1cb1f12a55f322a", "text": "\n                        \n                            \n                                \n                                \n                                WordPress\nThe recommended way to deploy WordPress on Platform.sh is using Composer.  The most popular and supported way to do so is with the John Bloch script.\nPlatform.sh strongly recommends starting new WordPress projects from our WordPress Template, which is built using Composer and includes the WP-CLI by default.  It also includes modifications to the configuration files necessary to connect to a database on Platform.sh automatically.\nPlugin compatibility\nPlatform.sh does not blacklist any WordPress plugins.  However, some plugins are known to require write access to their own file system as part of their setup process.  That is not possible on Platform.sh.  The file system where code lives is read-only for security reasons and cannot be written to from the application, only during the build hook.\nIn some cases that can be worked around by copying a file as part of the build hook process.  In other cases the plugin is simply incompatible with Platform.sh.  If you find a plugin that tries to write to its own directory we recommend filing an issue with that plugin, as such behavior should be viewed as a security bug.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "TYPO3", "title": "FAQ", "url": "/frameworks/typo3/faq.html", "documentId": "2d4f756fe57134f9a5f89024e6054f748b284f55", "text": "\n                        \n                            \n                                \n                                \n                                TYPO3 Frequently Asked Questions (FAQ)\nWhy are there warnings in the install tool?\nThe TYPO3 install tool doesn't yet fully understand when you are working on a cloud envirionment and may warn you that some folders are not writable.\nDon't worry, your TYPO3 installation will be fully functional.\nHow do I add extensions?\nTYPO3 extension can easily be added using composer. Just use the platform.sh CLI tool to run composer, as follows:\nplatform get &lt;project id&gt; -e &lt;branch name&gt;\ncomposer require typo3-ter/[extension name]\ngit add composer.*\ngit commit\ngit push\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Frameworks", "title": "TYPO3", "url": "/frameworks/typo3.html", "documentId": "e971dc71bd404c22827b648de104b4e2bee0e941", "text": "\n                        \n                            \n                                \n                                \n                                Getting started\nPrerequisites\nComposer\nComposer is a tool for dependency management in PHP. It allows you to declare the dependent libraries your project needs and it will install them in your project for you.\n\n\nInstall Composer\n\n\nConfigure your app\nThe ideal .platform.app.yaml file will vary from project project, and you are free to customize yours as needed.  A recommended baseline TYPO3 configuration is listed below, and can also be found in our TYPO3 template project.\n# This file describes an application. You can have multiple applications\n# in the same project.\n\n# The name of this app. Must be unique within a project.\nname: app\n\n# The type of the application to build.\ntype: php:7.0\nbuild:\n  flavor: composer\n\nruntime:\n    extensions:\n        - redis\n        - imagick\n        - xdebug\n\n# The relationships of the application with services or other applications.\n# The left-hand side is the name of the relationship as it will be exposed\n# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand\n# side is in the form `:`.\nrelationships:\n    database: 'mysqldb:mysql'\n    redis: 'rediscache:redis'\n\n# The configuration of app when it is exposed to the web.\nweb:\n    locations:\n      '/':\n        # The public directory of the app, relative to its root.\n        # check if web convention\n        root: 'web'\n        passthru: '/index.php'\n        index:\n          - 'index.php'\n        allow: false\n        rules:\n          # Allow access to common static files.\n          '\\.(jpe?g|png|gif|svgz?|css|js|map|ico|bmp|eot|woff2?|otf|ttf)$':\n            allow: true\n          '^/robots\\.txt$':\n            allow: true\n          '^/sitemap\\.xml$':\n            allow: true\n      '/uploads':\n        root: 'web/uploads'\n        scripts: false\n        allow: true\n        passthru: '/index.php'\n      '/fileadmin':\n        root: 'web/fileadmin'\n        scripts: false\n        allow: true\n        passthru: '/index.php'\n      '/typo3temp/assets':\n        root: 'web/typo3temp/assets'\n        scripts: false\n        allow: true\n        passthru: '/index.php'\n        rules:\n          '\\.js\\.gzip$':\n            headers:\n              Content-Type: text/javascript\n              Content-Encoding: gzip\n          '\\.css\\.gzip$':\n            headers:\n              Content-Type: text/css\n              Content-Encoding: gzip\n      '/typo3conf/LocalConfiguration.php':\n        allow: false\n      '/typo3conf/AdditionalConfiguration.php':\n        allow: false\n\n# The size of the persistent disk of the application (in MB).\ndisk: 2048\n\n# The mounts that will be performed when the package is deployed.\nmounts:\n  \"web/typo3temp/assets\": \"shared:files/tmp-assets\"\n  \"web/typo3temp/var\": \"shared:files/tmp-var\"\n  \"web/uploads\": \"shared:files/uploads\"\n  \"web/fileadmin\": \"shared:files/fileadmin\"\n  \"web/typo3conf/writeable\": \"shared:files/config\"\n  \"web/typo3conf/l10n\": \"shared:files/l10n\"\n\n# The hooks that will be performed when the package is deployed.\nhooks:\n  build: |\n    set -e\n    vendor/bin/typo3cms install:generatepackagestates --activate-default\n    if [ ! -f web/typo3conf/writeable/LocalConfiguration.php ]; then\n      cd web\n      touch FIRST_INSTALL\n      cd typo3conf\n      ln -sf writeable/LocalConfiguration.php LocalConfiguration.php\n      ln -sf writeable/ENABLE_INSTALL_TOOL ENABLE_INSTALL_TOOL\n    fi;\n    if [ ! -f /app/web/typo3conf/writeable/PackageStates.php ]; then\n      cd /app/web\n      cd typo3conf\n      ln -sf writeable/PackageStates.php PackageStates.php\n    fi;\n\n  deploy: |\n    set -e\n    if [ ! -f web/typo3conf/writeable/installed.lock ]; then\n      touch web/typo3conf/writeable/ENABLE_INSTALL_TOOL\n      cp web/typo3/sysext/core/Configuration/FactoryConfiguration.php web/typo3conf/writeable/LocalConfiguration.php\n      vendor/bin/typo3cms install:setup --non-interactive --admin-user-name='admin' --admin-password='password' --site-setup-type='none' --site-name='TYPO3 on Platform.sh' --skip-integrity-check\n      touch web/typo3conf/writeable/installed.lock\n    fi;\n    vendor/bin/typo3cms install:fixfolderstructure\n    vendor/bin/typo3cms extension:setupactive\n    vendor/bin/typo3cms database:updateschema\n    vendor/bin/typo3cms cache:flush\n\ncrons:\n  typo3:\n    spec: \"*/5 * * * *\"\n    cmd: \"vendor/bin/typo3cms scheduler:run\"\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Symfony", "title": "FAQ", "url": "/frameworks/symfony/faq.html", "documentId": "93bf64bad6499304ec051dc7ceedf5163a91a198", "text": "\n                        \n                            \n                                \n                                \n                                Symfony Frequently Asked Questions (FAQ)\nHow do I store my session files?\nIf you get the following error:\nfailed: Read-only file system (30) in /app/app/cache/dev/classes.php line 420\n\nthat's because Symfony is trying to write into: /var/lib/php5/ which\nis read-only.\nThe solution is to mount a sessions folder into Platform.sh and write\nsessions in that folder.\nSimply edit your .platform.app.yaml and add a mounts there:\nmounts:\n...\n    \"app/sessions\":\n        source: local\n        source_path: sessions\n...\n\nThen, add this line at the top of your app_dev.php:\nini_set('session.save_path', __DIR__.'/../app/sessions' );\n\nWhy does my newly cloned Symfony install throw errors?\nYou may encounter the WSOD (white screen of death) when you first clone\na new Symfony project from your platform. This is likely because of\nmissing dependencies.\nYou will need to install composer first and then run the following\ncommand:\ncd my_project_name/\ncomposer install\n\nWhy do I get 'Permission denied' in a deploy hook?\nIf you get the following error during a deploy hook:\nLaunching hook 'app/console cache:clear'.\n/bin/dash: 1: app/console: Permission denied\n\nThis means that you might have committed the executable file (in this case app/console) without the execute bit set.\nRun this to fix the problem:\nchmod a+x app/console\ngit add app/console\ngit commit -m \"Fix the console script execute permission.\"\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Frameworks", "title": "Symfony", "url": "/frameworks/symfony.html", "documentId": "e5838518cfe5440e98bd02b79f4cba773b6c67f0", "text": "\n                        \n                            \n                                \n                                \n                                Getting started\nPrerequisites\nComposer\nComposer is a tool for dependency management in PHP. It allows you to declare the dependent libraries your project needs and it will install them in your project for you.\n\n\nInstall Composer\n\n\nConfigure your app\nThe ideal .platform.app.yaml file will vary from project project, and you are free to customize yours as needed.  It also varies somewhat from one Symfony version to the next as Symfony has evolved.  See the appropriate repository below for your Symfony version.\n\nSymfony 3\nSymfony 4\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Frameworks", "title": "Hibernate", "url": "/frameworks/hibernate.html", "documentId": "2e3f71b1fe74b6cbde9e449bf60c53446fd45260", "text": "\n                        \n                            \n                                \n                                \n                                Hibernate\nHibernate ORM is an object-relational mapping tool for the Java programming language. It provides a framework for mapping an object-oriented domain model to a relational database. Hibernate handles object-relational impedance mismatch problems by replacing direct, persistent database accesses with high-level object handling functions.\nServices\nThe configuration reader library for Java is used in these examples, so be sure to check out the documentation for installation instructions and the latest version.\nMySQL\nMySQL is an open-source relational database technology. Define the driver for MySQL, and the Java dependencies. Then determine the SessionFactory client programmatically:\nimport org.hibernate.Session;\nimport org.hibernate.SessionFactory;\nimport org.hibernate.Transaction;\nimport org.hibernate.cfg.Configuration;\nimport sh.platform.config.Config;\nimport sh.platform.config.Hibernate;\n\npublic class HibernateApp {\n\n    public static void main(String[] args) {\n        Config config = new Config();\n        Configuration configuration = new Configuration();\n        configuration.addAnnotatedClass(Address.class);\n        final Hibernate credential = config.getCredential(\"database\", Hibernate::new);\n        final SessionFactory sessionFactory = credential.getMySQL(configuration);\n        try (Session session = sessionFactory.openSession()) {\n            Transaction transaction = session.beginTransaction();\n            //...\n            transaction.commit();\n        }\n    }\n}\n\n\nNote:\nYou can use the same MySQL driver for MariaDB as well if you wish to do so.\n\nMariaDB\nMariaDB is an open-source relational database technology. Define the driver for MariaDB, and the Java dependencies. Then determine the SessionFactory client programmatically:\nimport org.hibernate.Session;\nimport org.hibernate.SessionFactory;\nimport org.hibernate.Transaction;\nimport org.hibernate.cfg.Configuration;\nimport sh.platform.config.Config;\nimport sh.platform.config.Hibernate;\n\npublic class HibernateApp {\n\n    public static void main(String[] args) {\n        Config config = new Config();\n        Configuration configuration = new Configuration();\n        configuration.addAnnotatedClass(Address.class);\n        final Hibernate credential = config.getCredential(\"database\", Hibernate::new);\n        final SessionFactory sessionFactory = credential.getMariaDB(configuration);\n        try (Session session = sessionFactory.openSession()) {\n            Transaction transaction = session.beginTransaction();\n            //...\n            transaction.commit();\n        }\n    }\n}\n\nPostgreSQL\nPostgreSQL is an open-source relational database technology. Define the driver for PostgreSQL, and the Java dependencies. Then determine the SessionFactory client programmatically:\nimport org.hibernate.Session;\nimport org.hibernate.SessionFactory;\nimport org.hibernate.Transaction;\nimport org.hibernate.cfg.Configuration;\nimport sh.platform.config.Config;\nimport sh.platform.config.Hibernate;\n\npublic class HibernateApp {\n\n    public static void main(String[] args) {\n        Config config = new Config();\n        Configuration configuration = new Configuration();\n        configuration.addAnnotatedClass(Address.class);\n        final Hibernate credential = config.getCredential(\"database\", Hibernate::new);\n        final SessionFactory sessionFactory = credential.getPostgreSQL(configuration);\n        try (Session session = sessionFactory.openSession()) {\n            Transaction transaction = session.beginTransaction();\n            //...\n            transaction.commit();\n        }\n    }\n}\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Frameworks", "title": "Spring", "url": "/frameworks/spring.html", "documentId": "6a4497f1ddf4f1a3fb1318a85d2703a9d0259674", "text": "\n                        \n                            \n                                \n                                \n                                Spring\nThe Spring Framework provides a comprehensive programming and configuration model for modern Java-based enterprise applications - on any kind of deployment platform. Platform.sh is flexible, and allows you to use Spring Framework in several flavors such as Spring MVC and Spring Boot.\nServices\nThe configuration reader library for Java is used in these examples, so be sure to check out the documentation for installation instructions and the latest version.\nMongoDB\nYou can use Spring Data MongoDB to use MongoDB with your application by first determining the MongoDB client programmatically.\nimport com.mongodb.MongoClient;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.data.mongodb.config.AbstractMongoConfiguration;\nimport sh.platform.config.Config;\nimport sh.platform.config.MongoDB;\n\n@Configuration\npublic class MongoConfig extends AbstractMongoConfiguration {\n\n    private Config config = new Config();\n\n    @Override\n    @Bean\n    public MongoClient mongoClient() {\n        MongoDB mongoDB = config.getCredential(\"database\", MongoDB::new);\n        return mongoDB.get();\n    }\n\n    @Override\n    protected String getDatabaseName() {\n        return config.getCredential(\"database\", MongoDB::new).getDatabase();\n    }\n}\n\nApache Solr\nYou can use Spring Data Solr to use Solr with your application by first determining the Solr client programmatically.\nimport org.apache.solr.client.solrj.impl.HttpSolrClient;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.data.solr.core.SolrTemplate;\nimport sh.platform.config.Config;\nimport sh.platform.config.Solr;\n\n@Configuration\npublic class SolrConfig {\n\n    @Bean\n    public HttpSolrClient elasticsearchTemplate() {\n        Config config = new Config();\n        final Solr credential = config.getCredential(\"solr\", Solr::new);\n        final HttpSolrClient httpSolrClient = credential.get();\n        String url = httpSolrClient.getBaseURL();\n        httpSolrClient.setBaseURL(url.substring(0, url.lastIndexOf('/')));\n        return httpSolrClient;\n    }\n\n    @Bean\n    public SolrTemplate solrTemplate(HttpSolrClient client) {\n        return new SolrTemplate(client);\n    }\n}\n\nRedis\nYou can use Spring Data Redis to use Redis with your application by first determining the Redis client programmatically.\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.data.redis.connection.jedis.JedisConnectionFactory;\nimport org.springframework.data.redis.core.RedisTemplate;\nimport org.springframework.data.redis.serializer.GenericToStringSerializer;\n\n@Configuration\npublic class RedisConfig {\n\n\n    @Bean\n    JedisConnectionFactory jedisConnectionFactory() {\n        Config config = new Config();\n        RedisSpring redis = config.getCredential(\"redis\", RedisSpring::new);\n        return redis.get();\n    }\n\n    @Bean\n    public RedisTemplate&lt;String, Object&gt; redisTemplate() {\n        final RedisTemplate&lt;String, Object&gt; template = new RedisTemplate&lt;String, Object&gt;();\n        template.setConnectionFactory(jedisConnectionFactory());\n        template.setValueSerializer(new GenericToStringSerializer&lt;Object&gt;(Object.class));\n        return template;\n    }\n\n}\n\nMySQL\nMySQL is an open-source relational database technology. Spring has robust integration with this technology: Spring Data JPA.\nThe first step is to choose the database that you would like to use in your project. Define the driver for MySQL and the Java dependencies, then determine the DataSource client programmatically:\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport sh.platform.config.Config;\nimport sh.platform.config.MySQL;\n\nimport javax.sql.DataSource;\n\n@Configuration\npublic class DataSourceConfig {\n\n    @Bean(name = \"dataSource\")\n    public DataSource getDataSource() {\n        Config config = new Config();\n        MySQL database = config.getCredential(\"database\", MySQL::new);\n        return database.get();\n    }\n}\n\n\nNote:\nYou can use the same MySQL driver for MariaDB as well if you wish to do so.\n\nMariaDB\nMariaDB is an open-source relational database technology. Spring has robust integration with this technology: Spring Data JPA.\nThe first step is to choose the database that you would like to use in your project. Define the driver for MariaDB and the Java dependencies, then determine the DataSource client programmatically:\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport sh.platform.config.Config;\nimport sh.platform.config.MariaDB;\n\nimport javax.sql.DataSource;\n\n@Configuration\npublic class DataSourceConfig {\n\n    @Bean(name = \"dataSource\")\n    public DataSource getDataSource() {\n        Config config = new Config();\n        MariaDB database = config.getCredential(\"database\", MariaDB::new);\n        return database.get();\n    }\n}\n\nPostgreSQL\nPostgreSQL is an open-source relational database technology. Spring has robust integration with this technology: Spring Data JPA.\nThe first step is to choose the database that you would like to use in your project. Define the driver for PostgreSQL and the Java dependencies, then determine the DataSource client programmatically:\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport sh.platform.config.Config;\nimport sh.platform.config.PostgreSQL;\n\nimport javax.sql.DataSource;\n\n@Configuration\npublic class DataSourceConfig {\n\n    @Bean(name = \"dataSource\")\n    public DataSource getDataSource() {\n        Config config = new Config();\n        PostgreSQL database = config.getCredential(\"database\", PostgreSQL::new);\n        return database.get();\n    }\n}\n\nRabbitMQ\nYou can use Spring JMS to use RabbitMQ with your application by first determining the RabbitMQ client programmatically.\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.jms.annotation.EnableJms;\nimport org.springframework.jms.connection.CachingConnectionFactory;\nimport org.springframework.jms.support.converter.MappingJackson2MessageConverter;\nimport org.springframework.jms.support.converter.MessageConverter;\nimport org.springframework.jms.support.converter.MessageType;\nimport sh.platform.config.Config;\nimport sh.platform.config.RabbitMQ;\n\nimport javax.jms.ConnectionFactory;\n\n@Configuration\n@EnableJms\npublic class JMSConfig {\n\n    private ConnectionFactory getConnectionFactory() {\n        Config config = new Config();\n        final RabbitMQ rabbitMQ = config.getCredential(\"rabbitmq\", RabbitMQ::new);\n        return rabbitMQ.get();\n    }\n\n    @Bean\n    public MessageConverter getMessageConverter() {\n        MappingJackson2MessageConverter converter = new MappingJackson2MessageConverter();\n        converter.setTargetType(MessageType.TEXT);\n        converter.setTypeIdPropertyName(\"_type\");\n        return converter;\n    }\n\n    @Bean\n    public CachingConnectionFactory getCachingConnectionFactory() {\n        ConnectionFactory connectionFactory = getConnectionFactory();\n        return new CachingConnectionFactory(connectionFactory);\n    }\n}\n\nTemplates\n\nSpring Boot MySQL\nSpring Boot MongoDB\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Drupal8", "title": "FAQ", "url": "/frameworks/drupal8/faq.html", "documentId": "0d4261cee9eec93c67f4b027e740421a8594b896", "text": "\n                        \n                            \n                                \n                                \n                                Drupal Frequently Asked Questions (FAQ)\nHow can I import configuration on production?\nIf you don't want to do so manually, include the following lines in your deploy hook in .platformsh.app.yaml:\ndrush -y updatedb\ndrush -y config-import\n\nThat will automatically run update.php and import configuration on every new deploy.\nThe above configuration is included by default if you used our Drupal 8 example repository or created a project through the management console.\nI'm getting a PDO Exception 'MySQL server has gone away'\nNormally, this means there is a problem with the MySQL server container\nand you may need to increase the storage available to MySQL to resolve\nthe issue. Ballooning MySQL storage can be caused by a number of items:\n\nA large number of watchdog entries being captured. Fix the errors\n being generated or disable database logging.\nCron should run at regular intervals to ensure cache\n tables get cleared out.\n\nWhy do I get \"MySQL cannot connect to the database server\"?\nIf you are having a problem connecting to the database server, you will\nneed force a re-deployment of the database container. To do so, you can\nedit the service definition to add or remove a small amount of storage and\nthen push.\nCan I use the name of the session cookie for caching?\nFor Drupal sites, the name of the session cookie is based on a hash of the \ndomain name. This means that it will actually be consistent for a specific \nwebsite and can safely be used as a fixed value.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Frameworks", "title": "Jakarta EE", "url": "/frameworks/jakarta.html", "documentId": "8f1718c25fc552ba3d1c797e1853f374f2242510", "text": "\n                        \n                            \n                                \n                                \n                                Jakarta EE/ Eclipse MicroProfile\nEclipse MicroProfile is a community dedicated to optimizing the Enterprise Java mission for microservice-based architectures. The goal is to define a microservices application platform that is portable across multiple runtimes. Currently, the leading players in this group are IBM, Red Hat, Tomitribe, Payara, the London Java Community (LJC), and SouJava.\nJava Enterprise Edition (Java EE) is an umbrella that holds specifications and APIs with enterprise features, like distributed computing and web services. Widely used in Java, Java EE runs on reference runtimes that can be anything from microservices to application servers that handle transactions, security, scalability, concurrency, and management for the components it\u2019s deploying. Now, Enterprise Java has been standardized under the Eclipse Foundation with the name Jakarta EE.\nServices\nThe configuration reader library for Java is used in these examples, so be sure to check out the documentation for installation instructions and the latest version.\nMongoDB\nYou can use Jakarta NoSQL/JNoSQL to use MongoDB with your application by first determining the MongoDB client programmatically.\nimport com.mongodb.MongoClient;\nimport jakarta.nosql.document.DocumentCollectionManager;\nimport jakarta.nosql.document.DocumentCollectionManagerFactory;\nimport org.jnosql.diana.mongodb.document.MongoDBDocumentConfiguration;\nimport sh.platform.config.Config;\nimport sh.platform.config.MongoDB;\n\nimport javax.annotation.PostConstruct;\nimport javax.enterprise.context.ApplicationScoped;\nimport javax.enterprise.inject.Disposes;\nimport javax.enterprise.inject.Produces;\n\n@ApplicationScoped\nclass DocumentManagerProducer {\n\n    private DocumentCollectionManagerFactory managerFactory;\n\n    private MongoDB mongoDB;\n\n    @PostConstruct\n    public void init() {\n        Config config = new Config();\n        this.mongoDB = config.getCredential(\"database\", MongoDB::new);\n        final MongoClient mongoClient = mongoDB.get();\n        MongoDBDocumentConfiguration configuration = new MongoDBDocumentConfiguration();\n        this.managerFactory = configuration.get(mongoClient);\n    }\n\n    @Produces\n    public DocumentCollectionManager getManager() {\n        return managerFactory.get(mongoDB.getDatabase());\n    }\n\n    public void destroy(@Disposes DocumentCollectionManager manager) {\n        this.manager.close();\n    }\n}\n\nApache Solr\nYou can use Jakarta NoSQL/JNoSQL to use Solr with your application by first determining the Solr client programmatically.\nimport jakarta.nosql.document.DocumentCollectionManager;\nimport jakarta.nosql.document.DocumentCollectionManagerFactory;\nimport org.apache.solr.client.solrj.impl.HttpSolrClient;\nimport org.jnosql.diana.solr.document.SolrDocumentConfiguration;\nimport sh.platform.config.Config;\nimport sh.platform.config.Solr;\n\nimport javax.annotation.PostConstruct;\nimport javax.enterprise.context.ApplicationScoped;\nimport javax.enterprise.inject.Disposes;\nimport javax.enterprise.inject.Produces;\n\n@ApplicationScoped\nclass DocumentManagerProducer {\n\n    private DocumentCollectionManagerFactory managerFactory;\n\n    @PostConstruct\n    public void init() {\n        Config config = new Config();\n        Solr solr = config.getCredential(\"database\", Solr::new);\n        final HttpSolrClient httpSolrClient = solr.get();\n        SolrDocumentConfiguration configuration = new SolrDocumentConfiguration();\n        this.managerFactory = configuration.get(httpSolrClient);\n    }\n\n    @Produces\n    public DocumentCollectionManager getManager() {\n        return managerFactory.get(\"collection\");\n    }\n\n    public void destroy(@Disposes DocumentCollectionManager manager) {\n        this.manager.close();\n    }\n}\n\nElasticsearch\nYou can use Jakarta NoSQL/JNoSQL to use Elasticsearch with your application by first determining the Elasticsearch client programmatically.\nimport jakarta.nosql.document.DocumentCollectionManager;\nimport jakarta.nosql.document.DocumentCollectionManagerFactory;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.jnosql.diana.elasticsearch.document.ElasticsearchDocumentConfiguration;\nimport sh.platform.config.Config;\nimport sh.platform.config.Elasticsearch;\n\nimport javax.annotation.PostConstruct;\nimport javax.enterprise.context.ApplicationScoped;\nimport javax.enterprise.inject.Disposes;\nimport javax.enterprise.inject.Produces;\n\n@ApplicationScoped\nclass DocumentManagerProducer {\n\n    private DocumentCollectionManagerFactory managerFactory;\n\n    @PostConstruct\n    public void init() {\n        Config config = new Config();\n        Elasticsearch elasticsearch = config.getCredential(\"database\", Elasticsearch::new);\n        final RestHighLevelClient client = elasticsearch.get();\n        ElasticsearchDocumentConfiguration configuration = new ElasticsearchDocumentConfiguration();\n        this.managerFactory = configuration.get(client);\n    }\n\n    @Produces\n    public DocumentCollectionManager getManager() {\n        return managerFactory.get(\"collection\");\n    }\n\n    public void destroy(@Disposes DocumentCollectionManager manager) {\n        this.manager.close();\n    }\n}\n\nRedis\nYou can use Jakarta NoSQL/JNoSQL to use Redis with your application by first determining the Redis client programmatically.\nimport jakarta.nosql.keyvalue.BucketManager;\nimport org.jnosql.diana.redis.keyvalue.RedisBucketManagerFactory;\nimport org.jnosql.diana.redis.keyvalue.RedisConfiguration;\nimport redis.clients.jedis.JedisPool;\nimport sh.platform.config.Config;\nimport sh.platform.config.Redis;\n\nimport javax.annotation.PostConstruct;\nimport javax.enterprise.context.ApplicationScoped;\nimport javax.enterprise.inject.Disposes;\nimport javax.enterprise.inject.Produces;\n\n@ApplicationScoped\nclass BucketManagerProducer {\n\n    private static final String BUCKET = \"olympus\";\n\n    private RedisBucketManagerFactory managerFactory;\n\n    @PostConstruct\n    public void init() {\n        Config config = new Config();\n        Redis redis = config.getCredential(\"redis\", Redis::new);\n        final JedisPool jedisPool = redis.get();\n        RedisConfiguration configuration = new RedisConfiguration();\n        managerFactory = configuration.get(jedisPool);\n    }\n\n    @Produces\n    public BucketManager getManager() {\n        return managerFactory.getBucketManager(BUCKET);\n    }\n\n    public void destroy(@Disposes BucketManager manager) {\n        manager.close();\n    }\n\n}\n\nMySQL\nMySQL is an open-source relational database technology, and Jakarta EE supports a robust integration with it: JPA.\nThe first step is to choose the database that you would like to use in your project. Define the driver for MySQL and the Java dependencies. Then determine the DataSource client programmatically:\nimport sh.platform.config.Config;\nimport sh.platform.config.JPA;\n\nimport javax.annotation.PostConstruct;\nimport javax.enterprise.context.ApplicationScoped;\nimport javax.enterprise.inject.Disposes;\nimport javax.enterprise.inject.Produces;\nimport javax.persistence.EntityManager;\nimport javax.persistence.EntityManagerFactory;\n\n@ApplicationScoped\nclass EntityManagerConfiguration {\n\n    private EntityManagerFactory entityManagerFactory;\n\n    private EntityManager entityManager;\n\n    @PostConstruct\n    void setUp() {\n        Config config = new Config();\n        final JPA credential = config.getCredential(\"postgresql\", JPA::new);\n        entityManagerFactory = credential.getMySQL(\"jpa-example\");\n        this.entityManager = entityManagerFactory.createEntityManager();\n    }\n\n    @Produces\n    @ApplicationScoped\n    EntityManagerFactory getEntityManagerFactory() {\n        return entityManagerFactory;\n    }\n\n    @Produces\n    @ApplicationScoped\n    EntityManager getEntityManager() {\n        return entityManager;\n    }\n\n    void close(@Disposes EntityManagerFactory entityManagerFactory) {\n        entityManagerFactory.close();\n    }\n\n    void close(@Disposes EntityManager entityManager) {\n        entityManager.close();\n    }\n\n}\n\n\nNote:\nYou can use the same MySQL driver for MariaDB as well if you wish to do so.\n\nMariaDB\nMariaDB is an open-source relational database technology, and Jakarta EE supports a robust integration with it: JPA.\nThe first step is to choose the database that you would like to use in your project. Define the driver for MariaDB and the Java dependencies. Then determine the DataSource client programmatically:\nimport sh.platform.config.Config;\nimport sh.platform.config.JPA;\n\nimport javax.annotation.PostConstruct;\nimport javax.enterprise.context.ApplicationScoped;\nimport javax.enterprise.inject.Disposes;\nimport javax.enterprise.inject.Produces;\nimport javax.persistence.EntityManager;\nimport javax.persistence.EntityManagerFactory;\n\n@ApplicationScoped\nclass EntityManagerConfiguration {\n\n    private EntityManagerFactory entityManagerFactory;\n\n    private EntityManager entityManager;\n\n    @PostConstruct\n    void setUp() {\n        Config config = new Config();\n        final JPA credential = config.getCredential(\"postgresql\", JPA::new);\n        entityManagerFactory = credential.getMariaDB(\"jpa-example\");\n        this.entityManager = entityManagerFactory.createEntityManager();\n    }\n\n    @Produces\n    @ApplicationScoped\n    EntityManagerFactory getEntityManagerFactory() {\n        return entityManagerFactory;\n    }\n\n    @Produces\n    @ApplicationScoped\n    EntityManager getEntityManager() {\n        return entityManager;\n    }\n\n    void close(@Disposes EntityManagerFactory entityManagerFactory) {\n        entityManagerFactory.close();\n    }\n\n    void close(@Disposes EntityManager entityManager) {\n        entityManager.close();\n    }\n\n}\n\nPostgreSQL\nPostgreSQL is an open-source relational database technology, and Jakarta EE supports a robust integration with it: JPA.\nThe first step is to choose the database that you would like to use in your project. Define the driver for PostgreSQL and the Java dependencies. Then determine the DataSource client programmatically:\nimport sh.platform.config.Config;\nimport sh.platform.config.JPA;\n\nimport javax.annotation.PostConstruct;\nimport javax.enterprise.context.ApplicationScoped;\nimport javax.enterprise.inject.Disposes;\nimport javax.enterprise.inject.Produces;\nimport javax.persistence.EntityManager;\nimport javax.persistence.EntityManagerFactory;\n\n@ApplicationScoped\nclass EntityManagerConfiguration {\n\n    private EntityManagerFactory entityManagerFactory;\n\n    private EntityManager entityManager;\n\n    @PostConstruct\n    void setUp() {\n        Config config = new Config();\n        final JPA credential = config.getCredential(\"postgresql\", JPA::new);\n        entityManagerFactory = credential.getPostgreSQL(\"jpa-example\");\n        entityManager = entityManagerFactory.createEntityManager();\n    }\n\n    @Produces\n    @ApplicationScoped\n    EntityManagerFactory getEntityManagerFactory() {\n        return entityManagerFactory;\n    }\n\n    @Produces\n    @ApplicationScoped\n    EntityManager getEntityManager() {\n        return entityManager;\n    }\n\n    void close(@Disposes EntityManagerFactory entityManagerFactory) {\n        entityManagerFactory.close();\n    }\n\n    void close(@Disposes EntityManager entityManager) {\n        entityManager.close();\n    }\n\n}\n\nTransaction\nTo any Eclipse Microprofile or any non-JTA application is essential to point out, CDI does not provide transaction management implementation as part of its specs. Transaction management is left to be implemented by the programmer through the interceptors, such as the code below.\nimport javax.annotation.Priority;\nimport javax.inject.Inject;\nimport javax.interceptor.AroundInvoke;\nimport javax.interceptor.Interceptor;\nimport javax.interceptor.InvocationContext;\nimport javax.persistence.EntityManager;\nimport javax.persistence.EntityTransaction;\nimport javax.transaction.Transactional;\n\n@Transactional\n@Interceptor\n@Priority(Interceptor.Priority.APPLICATION)\npublic class TransactionInterceptor {\n\n    @Inject\n    private EntityManager manager;\n\n    @AroundInvoke\n    public Object manageTransaction(InvocationContext context) throws Exception {\n        final EntityTransaction transaction = manager.getTransaction();\n        transaction.begin();\n        try {\n            Object result = context.proceed();\n            transaction.commit();\n            return result;\n        } catch (Exception exp) {\n            transaction.rollback();\n            throw exp;\n        }\n    }\n}\n\nFurthermore, Apache Delta Spike has a post for treating this problem.\nTemplates\n\nApache Tomee\nThorntail\nPayara Micro\nKumuluzEE\nHelidon\nOpen Liberty\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "eZ Platform", "title": "Fastly", "url": "/frameworks/ez/fastly.html", "documentId": "1380af539b79b474b2693e2ccd686ea4b045a0d8", "text": "\n                        \n                            \n                                \n                                \n                                eZ Platform Enterprise with Fastly\neZ Platform Enterprise is a \"commercial extended\" version of ez Platform that includes, among other things, support for push-based purging on the Fastly CDN.\nRemove Varnish configuration\nAs of eZ Platform 1.13.5, 2.4.3 and 2.5.0, Varnish is enabled by default when deploying on Platform.sh. In order to use Fastly, Varnish must be disabled:\n\nRemove environment variable SYMFONY_TRUSTED_PROXIES: \"TRUST_REMOTE\" in .platform.app.yaml\nRemove the Varnish service in .platform/services.yaml\nIn .platform/routes.yaml, change routes to use app instead of the varnish service you removed in previous step:\n\n \"https://{default}/\":\n     type: upstream\n-     upstream: \"varnish:http\"\n+     upstream: \"app:http\"\n\nSetting up eZ Platform to use Fastly\neZ Platform's documentation includes instructions on how to configure eZ Platform for Fastly.  Follow the steps there to prepare eZ Platform for Fastly.\nSet credentials on Platform.sh\nThe best way to provide the Fastly credentials and configuration to eZ Platform on Platform.sh is via environment variables.  That way private credentials are never stored in Git.\nUsing the CLI, run the following commands to set the configuration on your master environment.  (Note that they will inherit to all other environments by default unless overridden.)\nplatform variable:create -e master --level environment env:HTTPCACHE_PURGE_TYPE --value 'fastly'\nplatform variable:create -e master --level environment env:FASTLY_SERVICE_ID --value 'YOUR_ID_HERE'\nplatform variable:create -e master --level environment env:FASTLY_KEY --value 'YOUR_ID_HERE'\n\nReplacing YOUR_ID_HERE with the Fastly Service ID and Key obtained from Fastly.\nNote: On a Platform.sh Dedicated Cluster, set those values on the production branch instead:\nplatform variable:set -e production env:HTTPCACHE_PURGE_TYPE fastly\nplatform variable:set -e production env:FASTLY_SERVICE_ID YOUR_ID_HERE\nplatform variable:set -e production env:FASTLY_KEY YOUR_ID_HERE\n\nConfigure Fastly and Platform.sh\nSee the alternate Go-live process for Fastly on Platform.sh.  This process is the same for any application.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Frameworks", "title": "eZ Platform", "url": "/frameworks/ez.html", "documentId": "8ac420caed3d438db61d6e313866aca005d8aaf9", "text": "\n                        \n                            \n                                \n                                \n                                eZ Platform\neZ Platform is a Composer-based PHP CMS, and as such fits well with the Platform.sh model.  As a Symfony-based application its setup is very similar to Symfony.\neZ Platform comes pre-configured for use with Platform.sh in version 1.13 and later. Version 2.5 and later is recommended. Those are the only versions that are supported.  Appropriate Platform.sh configuration files are included in the eZ Platform application itself, but of course may be modified to suit your particular site if needed.\nCache and sessions\nBy default, eZ Platform is configured to use a single Redis instance for both the application cache and session storage.  You may optionally choose to use a separate Redis instance for session storage in case you have a lot of authenticated traffic (and thus there would be many session records).\nTo do so, uncomment the redissession entry in the .platform/services.yaml file and the corresponding relationship in the .platform.app.yaml file.  The bridge code that is provided with eZ Platform 1.13 and later will automatically detect the additional Redis service and use it for session storage.\nOn a Dedicated instance, we strongly recommend using two separate Redis instances for Cache and Sessions.  The service and relationship names that ship with the default Platform.sh configuration in eZ Platform should be used as-is.  To ensure the development environment works like Production, uncomment the redissession entry in the .platform/services.yaml file and the corresponding relationship in the .platform.app.yaml file.  The bridge code that is provided with eZ Platform 1.13 and later will automatically detect the additional Redis service and use it for session storage.\nBy default, on Dedicated instances we will configure both Cache and Session storage in \"persistent\" mode, so that data is not lost in case of a system or process restart.  That reduces the potential for cache stampede issues or inadvertently logging people out.\nModifying an existing eZ Platform project\nIf you have an existing eZ Platform project that was upgraded from a previous version, or want to resynchronize with the latest recommended configuration, please see the eZ Platform official repository.\nIn particular, see:\n\nThe .platform.app.yaml file, which automatically builds eZ Platform in dev mode or production mode depending on your defined project-level variables.\nThe .platform directory\nThe platformsh.php configuration file, which does the work of mapping Platform.sh environment variables into eZ Platform.  It also will automatically  enable Redis-based cache and session support if detected.\n\nLocal Development with eZ Platform 2.x and later\neZ Systems provide a tool called eZ Launchpad for local development on top of a Docker stack. It improves Developer eXperience and reduces complexity for common actions by simplifying your interactions with Docker containers. eZ Launchpad is ready to work with Platform.sh.\nIt serves as a wrapper that allows you to run console commands from within the container without logging into it explicitly. For example to run bin/console cache:clear inside the PHP container do:\n~/ez sfrun cache:clear\n\neZ Launchpad installation\neZ Launchpad's approach is to stay as decoupled as possible from your development machine and your remote hosting whether you are Linux or Mac OSX. To install run:\ncurl -LSs https://ezsystems.github.io/launchpad/install_curl.bash | bash\n\nThen you can start to use it to initialize your eZ Platform project on top Docker.\n~/ez init\n\nor create the Docker stack based on an existing project\ngit clone yourproject.git application\ncd application\n~/ez create\n\nYou will find more details on the eZ Launchpad documentation.\nAt this time you will have a working eZ Platform application with many services including Varnish, Solr, Redis etc.\nPlatform.sh integration\nTo generate the key files for Platform.sh (.platform.app.yaml and .platform) run:\n~/ez platformsh:setup\n\neZ Launchpad will generate the files for you and you are then totally free to fine tune them.\nSolr specificity\nSolr is fully functional with eZ Launchpad but it is not enabled by default on Platform.sh. You will have to set it up manually following the current documenation here: https://github.com/ezsystems/ezplatform/blob/master/.platform/services.yaml#L37.\nActions needed are:\n\nGenerate the Solr configuration thanks to the script provided by eZ Systems.\nPut the result in the .platform at the root of your project.\nAdd the service in the .platform/services.yaml.\nAdd the relationship in the .platform.app..yaml.\n\nEnvironment variables (optional)\neZ Launchpad allows you to define environment variables in the provisioning/dev/docker-compose.yml file. You may use that to set Platform.sh variables to match Platform.sh environments so that you can keep your environment behavior in sync.\nSuch variables have to be set in the engine container.\n# provisioning/dev/docker-compose.yml\nengine:\n    environment:\n        - ASIMPLEVARIABLE=avalue\n        - PLATFORM_RELATIONSHIPS=A_BASE64_ENCODED_VALUE\n\nLocal development with Platform.sh\nThanks to eZ Launchpad you are able to be work 100% locally: untethered. We have the whole project working offline on our local machine.\n\nPlatform.sh also provides a smooth SSH tunnels integration described in the tethered page.\n\nLocal services are provided by the Docker stack but there are minimum day-to-day tasks that you might need with Platform.sh.\nThe main ones are:\n\nDownstream database synchronization: Getting it from the remote to the local.\nDownstream file storage synchronization: Getting it from the remote to the local.\n\nTo help you with that, Platform.sh provides a CLI that you probably already have. If you don't, see the install guide.\nCombined together, eZ Launcphad and Platform.sh CLI make those actions straight forward and simple.\nDatabase and storage synchronization\nplatform db:dump --gzip -f ezplatform.sql.gz -d data/ -y\nplatform mount:download -m ezplatform/web/var --target=ezplatform/web/var/ -y\n~/ez/importdata\n\nThe two first lines get the remote database and storage from the remote environment and stores it locally in data/. The third tells to eZ Launchpad to import those data in the Docker stack.\n\nThe storage (images and files) synchronization is optional. eZ Platform provides a placeholder generator mechanism which allows you to forget about the real images for your local.\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Drupal8", "title": "SimpleSAML", "url": "/frameworks/drupal8/simplesaml.html", "documentId": "4adbb1dd16f8b313cd8d1e06f704d3ed2692f951", "text": "\n                        \n                            \n                                \n                                \n                                SimpleSAML\nSimpleSAMLphp is a library for authenticating a PHP-based application against a SAML server, such as Shibboleth.  Although Drupal has modules available to authenticate using SimpleSAML some additional setup is required.\nThe following configuration assumes you are building Drupal 8 using Composer.  If not, you will need to download the library manually and adjust some paths accordingly.\nDownload the library and Drupal module\nThe easiest way to download SimpleSAMLphp is via Composer.  The following command will add both the Drupal module and the PHP library to your composer.json file.\ncomposer require simplesamlphp/simplesamlphp drupal/externalauth drupal/simplesamlphp_auth\n\nOnce that's run, commit both composer.json and composer.lock to your repository.\nInclude SimpleSAML cookies in the cache key\nThe SimpleSAML client uses additional cookies besides the Drupal session cookie that need to be whitelisted for the cache.  To do so, modify your routes.yaml file for the route that points to your Drupal site and add two additional cookies to the cache.cookies line.  It should end up looking approximately like this:\n\"https://{default}/\":\n    type: upstream\n    upstream: \"app:http\"\n    cache:\n      enabled: true\n      cookies: ['/^SS?ESS/', '/^Drupal.visitor/', 'SimpleSAMLSessionID', 'SimpleSAMLAuthToken']\n\nCommit this change to the Git repository.\nExpose the SimpleSAML endpoint\nThe SimpleSAML library's www directory needs to be publicly accessible.  That can be done by mapping it directly to a path in the Application configuration.  Add the following block to the web.locations section of .platform.app.yaml:\n web:\n    locations:\n        '/simplesaml':\n            root: 'vendor/simplesamlphp/simplesamlphp/www'\n            allow: true\n            scripts: true\n            index:\n                - index.php\n\nThat will map all requests to example.com/simplesaml/ to the vendor/simplesamlphp/www directory, allowing static files there to be served, PHP scripts to execute, and defaulting to index.php.\nCreate a configuration directory\nYour SimpleSAMLphp configuration will need to be outside of the vendor directory.  The composer require will download a template configuration file to vendor/simplesamlphp/simplesamlphp/config.\nRather than modifying that file in place (as it won't be included in Git), copy the vendor/simplesamlphp/simplesamlphp/config directory to simplesamlphp/config (in your application root).  It should contain two files, config.php and authsources.php.\nAdditionally, create a simplesamlphp/metadata directory.  This directory will hold your IdP definitions.  Consult the SimpleSAMLphp documentation and see the examples in vendor/simplesamlphp/simplesamlphp/metadata-templates.\nNext, you need to tell SimpleSAMLphp where to find that directory using an environment variable.  The simplest way to set that is to add the following block to your .platform.app.yaml file:\nvariables:\n    env:\n        SIMPLESAMLPHP_CONFIG_DIR: /app/simplesamlphp/config\n\nCommit the whole simplesamplphp directory and .platform.app.yaml to Git.\nConfigure SimpleSAML to use the database\nSimpleSAMLphp is able to store its data either on disk or in the Drupal database.  Platform.sh strongly recommends using the database.\nOpen the file simplesamlphp/config/config.php that you created earlier.  It contains a number of configuration properties that you can adjust as needed.  Some are best edited in-place and the file already includes ample documentation, specifically:\n\nauth.adminpassword\ntechnicalcontact_name\ntechnicalcontact_email\n\nOthers are a little more involved.  In the interest of simplicity we recommend simply pasting the following code snippet at the end of the file, as it will override the default values in the array.\n// Set SimpleSAML to log using error_log(), which on Platform.sh will\n// be mapped to the /var/log/app.log file.\n$config['logging.handler'] = 'errorlog';\n\n// Set SimpleSAML to use the metadata directory in Git, rather than\n// the empty one in the vendor directory.\n$config['metadata.sources'] = [\n   ['type' =&gt; 'flatfile', 'directory' =&gt;  dirname(__DIR__) . '/metadata'],\n];\n\n// Setup the database connection for all parts of SimpleSAML.\nif (isset($_ENV['PLATFORM_RELATIONSHIPS'])) {\n  $relationships = json_decode(base64_decode($_ENV['PLATFORM_RELATIONSHIPS']), TRUE);\n  foreach ($relationships['database'] as $instance) {\n    if (!empty($instance['query']['is_master'])) {\n      $dsn = sprintf(\"%s:host=%s;dbname=%s\", \n        $instance['scheme'],\n        $instance['host'], \n        $instance['path']\n      );\n      $config['database.dsn'] = $dsn; \n      $config['database.username'] = $instance['username'];\n      $config['database.password'] = $instance['password'];\n\n      $config['store.type'] = 'sql';\n      $config['store.sql.dsn'] = $dsn;\n      $config['store.sql.username'] = $instance['username'];\n      $config['store.sql.password'] = $instance['password'];\n      $config['store.sql.prefix'] = 'simplesaml';\n\n    }\n  }\n}\n\n// Set the salt value from the Platform.sh entropy value, provided for this purpose.\nif (isset($_ENV['PLATFORM_PROJECT_ENTROPY'])) {\n  $config['secretsalt'] = $_ENV['PLATFORM_PROJECT_ENTROPY'];\n}\n\nGenerate SSL certificates (optional)\nDepending on your Identity Provider (IdP), you may need to generate an SSL/TLS certificate to connect to the Service Provider (SP).  If so, you should generate the certificate locally following the instructions in the SimpleSAMLphp documentation.  Whatever your resulting idP file is should be placed in the simplesamlphp/metadata directory.  The certificate should be placed in the simplesamlphp/cert directory.  (Create it if needed.)\nThen add the following line to your simplesamlphp/config/config.php file to tell the library where to find the certificate:\n$config['certdir'] = dirname(__DIR__) . '/cert';\n\nDeploy\nCommit all changes and deploy the site, then enable the simplesamlphp_auth module within Drupal (usually by enabling it locally and pushing a config change).\nConsult the module documentation for further information on how to configure the module itself.  Note that you should not check the \"Activate authentication via SimpleSAMLphp\" checkbox in the module configuration until you have the rest of the configuration completed or you may be locked out of the site.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Drupal8", "title": "Elasticsearch", "url": "/frameworks/drupal8/elasticsearch.html", "documentId": "1bf10195c54a2f04b6b83ecb154fa893b37060ae", "text": "\n                        \n                            \n                                \n                                \n                                Using Elasticsearch with Drupal 8.x\nRequirements\nAdd the Drupal modules\nYou will need to add the Search API and Elasticsearch Connector modules to your project. If you are using composer, the easiest way to add them is to simply run:\n$ composer require drupal/search_api drupal/elasticsearch_connector\n\nAnd then commit the changes to composer.json and composer.lock.\nAdd an Elasticsearch service\nFirst you need to create an Elasticsearch service.  In your .platform/services.yaml file, add or uncomment the following:\nelasticsearch:\n    type: elasticsearch:6.5\n    disk: 2014\n\nThe above definition defines a single Elasticsearch 6.5 server.  Because Elasticsearch defines additional indexes dynamically there is no need to define custom endpoints.\nExpose the Elasticsearch service to your application\nIn your .platform.app.yaml file, you now need to open a connection to the new Elasticsearch service.  Under the relationships section, add or uncomment the following:\nrelationships:\n    elasticsearch: 'elasticsearch:elasticsearch'\nConfiguration\nBecause Drupal defines connection information via the Configuration Management system, you will need to first define an Elasticsearch \"Cluster\" at admin/config/search/elasticsearch-connector.  Note the \"machine name\" the server is given.\nThen, paste the following code snippet into your settings.platformsh.php file.\n\nnote\nIf you do not already have the Platform.sh Config Reader library installed and referenced at the top of the file, you will need to install it with composer require platformsh/config-reader and then add the following code before the block below:\n$platformsh = new \\Platformsh\\ConfigReader\\Config();\nif (!$platformsh-&gt;inRuntime()) {\n  return;\n}\n\n\n\nEdit the value of $relationship_name if you are using a different relationship.\n\nEdit the value of $es_server_name to match the machine name of your cluster in Drupal.\n\n\n// Update these values to the relationship name (from .platform.app.yaml)\n// and the machine name of the server from your Drupal configuration.\n$relationship_name = 'elasticsearch';\n$es_cluster_name = 'YOUR_CLUSTER_HERE';\nif ($platformsh-&gt;hasRelationship($relationship_name)) {\n  $platformsh-&gt;registerFormatter('drupal-elastic', function($creds) {\n    return sprintf('http://%s:%s', $creds['host'], $creds['port']);\n  });\n\n  // Set the connector configuration to the appropriate value, as defined by the formatter above.\n  $config['elasticsearch_connector.cluster.' . $es_cluster_name]['url'] = $platformsh-&gt;formattedCredentials($relationship_name, 'drupal-elastic');\n}\n\nCommit that code and push.  The specified cluster will now always point to the Elasticsearch service.  Then configure Search API as normal.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Drupal8", "title": "Solr", "url": "/frameworks/drupal8/solr.html", "documentId": "4024580cc7e877e41fb4820f9c33446627935bc2", "text": "\n                        \n                            \n                                \n                                \n                                Using Solr with Drupal 8.x\nThe Drupal Search API Solr module has a somewhat involved setup process, as it requires live access to the Solr server in order to generate the configuration files for it.  The following procedure is therefore necessary to ensure each step is able to proceed.\nSearch API Solr stores its configuration in the Drupal Configuration API.  However, that system does not easily support environment-aware information.  The setup process therefore depends on config-overrides in settings.platformsh.php, which may need to be modified slightly depending on  your Solr configuration.\nSearch API Solr requires Solr 6.6 or higher, and recommends Solr 8 or higher.\nAdvanced Solr service configuration and implementation in other frameworks other than Drupal can be found at the Solr services page.\nSteps\n0. Upgrade Symfony Event Dispatcher\nIf you are running Drupal older than 9.0, a small workaround will be needed.  The Solarium library used by Search API Solr requires the 4.3 version of the Symfony Event Dispatcher, whereas Drupal core ships with 3.4.  The Search API Solr issue queue has a more detailed description of the problem.\nAs noted there, the workaround for now is to run:\ncomposer require symfony/event-dispatcher:\"4.3.4 as 3.4.35\"\nin your project root and commit the resulting change to composer.json and composer.lock.  That will cause Composer to install the 4.3 version of Event Dispatcher.  Once this issue is resolved in core this step will no longer be necessary.\n1. Add the Drupal modules\nYou will need to add the Search API and Search API Solr modules to your project. If you are using composer, the easiest way to add them is to simply run:\n$ composer require drupal/search_api_solr\n\nAnd then commit the changes to composer.json and composer.lock.\n2. Add a default Solr service\nAdd the following to your .platform/services.yaml file.\nsearch:\n    type: solr:8.0\n    disk: 1024\n    configuration:\n        cores:\n            maincore:\n                conf_dir: {}\n        endpoints:\n            main:\n                core: maincore\n\nThe above definition defines a single Solr 8.0 server.  That server has 1 core defined: maincore, which will use a default configuration.  (The default configuration is not suitable for production but will allow the module to connect to it.)\nIt then defines one endpoint, main, which is connected to the maincore.\n3. Expose the Solr service to your application\nIn your .platform.app.yaml file, we now need to open a connection to the new Solr service.  Under the relationships section, add the following:\nrelationships:\n    solrsearch: 'search:main'\n\nThat is, the application's environment would include a solrsearch relationship that connects to the main endpoint, which is the maincore core.\nThe key (left side) is the name that will be exposed to the application in the PLATFORM_RELATIONSHIPS variable.  The right hand side is the name of the service specified above (search) and the endpoint (main).  If you named the service or endpoint something than different above, change those values accordingly.\n4. Add auto-configuration code to settings.platformsh.php\nThe configuration can be managed from settings.platformsh.php by adding the following code snippet.  It will override the environment-specific parts of the configuration object with the correct values to connect to the Platform.sh Solr instance.\n\nnote\nIf you do not already have the Platform.sh Config Reader library installed and referenced at the top of the file, you will need to install it with composer require platformsh/config-reader and then add the following code before the block below:\n$platformsh = new \\Platformsh\\ConfigReader\\Config();\nif (!$platformsh-&gt;inRuntime()) {\n  return;\n}\n\n\n\nEdit the value of $relationship_name if you are using a different relationship.\n\nEdit the value of $solr_server_name if you want to configure a Solr server in Drupal other than the default server automatically created by Search API Solr module.\n\n\n$platformsh-&gt;registerFormatter('drupal-solr', function($solr) {\n    // Default the solr core name to `collection1` for pre-Solr-6.x instances.\n    return [\n      'core' =&gt; substr($solr['path'], 5) ? : 'collection1',\n      'path' =&gt; '',\n      'host' =&gt; $solr['host'],\n      'port' =&gt; $solr['port'],\n    ];\n  });\n\n// Update these values to the relationship name (from .platform.app.yaml)\n// and the machine name of the server from your Drupal configuration.\n$relationship_name = 'solrsearch';\n$solr_server_name = 'default_solr_server';\nif ($platformsh-&gt;hasRelationship($relationship_name)) {\n  // Set the connector configuration to the appropriate value, as defined by the formatter above.\n  $config['search_api.server.' . $solr_server_name]['backend_config']['connector_config'] = $platformsh-&gt;formattedCredentials($relationship_name, 'drupal-solr');\n}\n\nIf you are connecting to multiple Solr cores, repeat the second block above for each relationship/server, modifying the two variables accordingly.\nCommit all of the changes above and then push to deploy.\n5. Enable the modules\nOnce the site is deployed, go to the /admin/modules page and enable the \"Search API Solr\" module.  Also enable the \"Search API Solr Search Defaults\" module in order to get a default server configuration.  If you would rather create one yourself you may do so but then you must change the value of $solr_server_name in the code snippet in settings.platformsh.php.\n6. Export and modify configuration\nIn the Drupal admin area, go to /admin/config/search/search-api and select your server.  (If you used the Search Defaults module, it will be named simply \"Solr Server\").  First verify that the module is able to connect to your Solr instance by ensuring that the \"Server connection\" reports \"The Solr server could be reached.\"\nYou can now generate a config.zip file using the button at the top of the page.  That will produce a Solr configuration that is customized for your current field configuration.  Extract the file into the .platform directory of your site.  It should unpack into a directory named solr_8.x_config or similar.\nInside that directory, locate the solrcore.properties file.  In that file, delete the entry for solr.install.dir.  Its default value will not work and it is not required for Solr to operate.  (The server already knows its installation directory.)\nFinally, move that directory to .platform/, and update the conf_dir to point to it.  The services.yaml entry should now look approximately like this:\nsearch:\n    type: solr:8.0\n    disk: 1024\n    configuration:\n        cores:\n            maincore:\n                conf_dir: !archive \"solr_8.x_config/\"\n        endpoints:\n            main:\n                core: maincore\n\nAdd the new directory and updated services.yaml to Git, commit, and push.\n\nnote\nIf you change your Solr configuration in Drupal, say to change the Solr field configuration, you may need to regenerate your configuration.  If so, repeat this entire step.\n\n7. Verify that it worked\nReturn to the Drupal UI for your server page.  After deploying and reloading the page, the \"Core Connection\" field should now read \"The Solr core could be accessed\".\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Drupal8", "title": "Memcached", "url": "/frameworks/drupal8/memcached.html", "documentId": "f56febf402debd8fe913a3f97068c5e365024485", "text": "\n                        \n                            \n                                \n                                \n                                Using Memcached with Drupal 8.x\nPlatform.sh recommends using Redis for caching with Drupal 8 over Memcached, as Redis offers better performance when dealing with larger values as Drupal tends to produce.  However, Memcached is also available if desired and is fully supported.\nRequirements\nAdd a Memcached service\nFirst you need to create a  Memcached service.  In your .platform/services.yaml file, add or uncomment the following:\ncacheservice:\n    type: memcached:1.4\n\nThat will create a service named cacheservice, of type memcached, specifically version 1.4.\nExpose the Memcached service to your application\nIn your .platform.app.yaml file, we now need to open a connection to the new Memcached service.  Under the relationships section, add the following:\nrelationships:\n    cache: \"cacheservice:memcached\"\n\nThe key (left side) is the name that will be exposed to the application in the PLATFORM_RELATIONSHIPS variable.  The right hand side is the name of the service we specified above (cacheservice) and the endpoint (memcached).  If you named the service something different above, change cacheservice to that.\nAdd the Memcached PHP extension\nYou will need to enable the PHP Memcached extension.  In your .platform.app.yaml file, add the following right after the type block:\n# Additional extensions\nruntime:\n    extensions:\n        - memcached\n\nAdd the Drupal module\nYou will need to add the Memcache module to your project.  If you are using Composer to manage your Drupal 8 site (which we recommend), simply run:\ncomposer require drupal/memcache\n\nThen commit the resulting changes to your composer.json and composer.lock files.\n\nnote\nYou must commit and deploy your code before continuing, then enable the module. The memcache \nmodule must be enabled before it is configured in the settings.platformsh.php file.\n\nConfiguration\nThe Drupal Memcache module must be configured via settings.platformsh.php.\nPlace the following at the end of settings.platformsh.php. Note the inline comments, as you may wish to customize it further.  Also review the README.txt file that comes with the memcache module, as it has a more information on possible configuration options. For instance, you may want to consider using memcache for locking as well and configuring cache stampede protection.\nThe example below is intended as a \"most common case\".\n\nif (getenv('PLATFORM_RELATIONSHIPS') &amp;&amp; extension_loaded('memcached')) {\n  $relationships = json_decode(base64_decode(getenv('PLATFORM_RELATIONSHIPS')), TRUE);\n\n  // If you named your memcached relationship something other than \"cache\", set that here.\n  $relationship_name = 'cache';\n\n  if (!empty($relationships[$relationship_name])) {\n    // This is the line that tells Drupal to use memcached as a backend.\n    // Comment out just this line if you need to disable it for some reason and\n    // fall back to the default database cache. \n    $settings['cache']['default'] = 'cache.backend.memcache';\n\n    foreach ($relationships[$relationship_name] as $endpoint) {\n      $host = sprintf(\"%s:%d\", $endpoint['host'], $endpoint['port']);\n      $settings['memcache']['servers'][$host] = 'default';\n    }\n  }\n\n  // By default Drupal starts the cache_container on the database.  The following\n  // code overrides that.\n  // Make sure that the $class_load-&gt;addPsr4 is pointing to the right location of\n  // the memcache module.  The value below should be correct if memcache was installed\n  // using Drupal Composer.\n  $memcache_exists = class_exists('Memcache', FALSE);\n  $memcached_exists = class_exists('Memcached', FALSE);\n  if ($memcache_exists || $memcached_exists) {\n    $class_loader-&gt;addPsr4('Drupal\\\\memcache\\\\', 'modules/contrib/memcache/src');\n\n    // If using a multisite configuration, adapt this line to include a site-unique\n    // value.\n    $settings['memcache']['key_prefix'] = getenv('PLATFORM_ENVIRONMENT');\n\n    // Define custom bootstrap container definition to use Memcache for cache.container.\n    $settings['bootstrap_container_definition'] = [\n      'parameters' =&gt; [],\n      'services' =&gt; [\n        'database' =&gt; [\n          'class' =&gt; 'Drupal\\Core\\Database\\Connection',\n          'factory' =&gt; 'Drupal\\Core\\Database\\Database::getConnection',\n          'arguments' =&gt; ['default'],\n        ],\n        'settings' =&gt; [\n          'class' =&gt; 'Drupal\\Core\\Site\\Settings',\n          'factory' =&gt; 'Drupal\\Core\\Site\\Settings::getInstance',\n        ],\n        'memcache.settings' =&gt; [\n          'class' =&gt; 'Drupal\\memcache\\MemcacheSettings',\n          'arguments' =&gt; ['@settings'],\n        ],\n        'memcache.factory' =&gt; [\n          'class' =&gt; 'Drupal\\memcache\\Driver\\MemcacheDriverFactory',\n          'arguments' =&gt; ['@memcache.settings'],\n        ],\n        'memcache.backend.cache.container' =&gt; [\n          'class' =&gt; 'Drupal\\memcache\\DrupalMemcacheInterface',\n          'factory' =&gt; ['@memcache.factory', 'get'],\n          'arguments' =&gt; ['container'],\n        ],\n        'lock.container' =&gt; [\n          'class' =&gt; 'Drupal\\memcache\\Lock\\MemcacheLockBackend',\n          'arguments' =&gt; ['container', '@memcache.backend.cache.container'],\n        ],\n        'cache_tags_provider.container' =&gt; [\n          'class' =&gt; 'Drupal\\Core\\Cache\\DatabaseCacheTagsChecksum',\n          'arguments' =&gt; ['@database'],\n        ],\n        'cache.container' =&gt; [\n          'class' =&gt; 'Drupal\\memcache\\MemcacheBackend',\n          'arguments' =&gt; ['container', '@memcache.backend.cache.container', '@lock.container', '@memcache.config', '@cache_tags_provider.container'],\n        ],\n      ],\n    ];\n  }\n}\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Drupal8", "title": "Redis", "url": "/frameworks/drupal8/redis.html", "documentId": "b761d0feb611e4cc93b86e672ceb8ea2c9e06f1e", "text": "\n                        \n                            \n                                \n                                \n                                Using Redis with Drupal 8.x\nIf you are using the Platform.sh-provided Drupal 8 template, most of this work is already done for you.  All you need to do is uncomment a the Redis relationship in .platform.app.yaml after your site is installed and Redis-based caching should \"just work\".\nIf you are working from an older repository or migrating a pre-built site to Platform.sh, see the instructions below.\nRequirements\nAdd a Redis service\nFirst you need to create a Redis service.  In your .platform/services.yaml file, add or uncomment the following:\nrediscache:\n    type: redis:5.0\n\nThat will create a service named rediscache, of type redis, specifically version 5.0.\nExpose the Redis service to your application\nIn your .platform.app.yaml file, we now need to open a connection to the new Redis service.  Under the relationships section, add the following:\nrelationships:\n    redis: \"rediscache:redis\"\n\nThe key (left side) is the name that will be exposed to the application in the PLATFORM_RELATIONSHIPS variable.  The right hand side is the name of the service we specified above (rediscache) and the endpoint (redis).  If you named the service something different above, change rediscache to that.\nAdd the Redis PHP extension\nBecause the Redis extension for PHP has been known to have BC breaks at times, we do not bundle a specific verison by default.  Instead, we provide a script to allow you to build your desired version in the build hook.  See the PHP-Redis page for a simple-to-install script and instructions.\nAdd the Drupal module\nYou will need to add the Redis module to your project.  If you are using Composer to manage your Drupal 8 site (which we recommend), simply run:\ncomposer require drupal/redis\n\nThen commit the resulting changes to your composer.json and composer.lock files.\nNote that the Redis module does not need to be enabled in Drupal except for diagnostic purposes.  The configuration below is sufficient to leverage its functionality.\nConfiguration\nTo make use of the Redis cache you will need to set some Drupal variables. The configuration is a bit more complex than can be easily represented in Platform.sh's environment variables configuration, so using settings.php directly is the recommended approach.\nPlace the following at the end of settings.platformsh.php. Note the inline comments, as you may wish to customize it further.  Also review the README.txt file that comes with the redis module, as it has a great deal more information on possible configuration options. For instance, you may wish to not use Redis for the persistent lock if you have a custom module that needs locks to persist for more than a few seconds.\nThe example below is intended as a \"most common case\".  (Note: This example assumes Drupal 8.2 or later.)\n\nnote\nIf you do not already have the Platform.sh Config Reader library installed and referenced at the top of the file, you will need to install it with composer require platformsh/config-reader and then add the following code before the block below:\n$platformsh = new \\Platformsh\\ConfigReader\\Config();\nif (!$platformsh-&gt;inRuntime()) {\n  return;\n}\n\n\n// Set redis configuration.\nif ($platformsh-&gt;hasRelationship('redis') &amp;&amp; !drupal_installation_attempted() &amp;&amp; extension_loaded('redis')) {\n  $redis = $platformsh-&gt;credentials('redis');\n\n  // Set Redis as the default backend for any cache bin not otherwise specified.\n  $settings['cache']['default'] = 'cache.backend.redis';\n  $settings['redis.connection']['host'] = $redis['host'];\n  $settings['redis.connection']['port'] = $redis['port'];\n\n  // Apply changes to the container configuration to better leverage Redis.\n  // This includes using Redis for the lock and flood control systems, as well\n  // as the cache tag checksum. Alternatively, copy the contents of that file\n  // to your project-specific services.yml file, modify as appropriate, and\n  // remove this line.\n  $settings['container_yamls'][] = 'modules/contrib/redis/example.services.yml';\n\n  // Allow the services to work before the Redis module itself is enabled.\n  $settings['container_yamls'][] = 'modules/contrib/redis/redis.services.yml';\n\n  // Manually add the classloader path, this is required for the container cache bin definition below\n  // and allows to use it without the redis module being enabled.\n  $class_loader-&gt;addPsr4('Drupal\\\\redis\\\\', 'modules/contrib/redis/src');\n\n  // Use redis for container cache.\n  // The container cache is used to load the container definition itself, and\n  // thus any configuration stored in the container itself is not available\n  // yet. These lines force the container cache to use Redis rather than the\n  // default SQL cache.\n  $settings['bootstrap_container_definition'] = [\n    'parameters' =&gt; [],\n    'services' =&gt; [\n      'redis.factory' =&gt; [\n        'class' =&gt; 'Drupal\\redis\\ClientFactory',\n      ],\n      'cache.backend.redis' =&gt; [\n        'class' =&gt; 'Drupal\\redis\\Cache\\CacheBackendFactory',\n        'arguments' =&gt; ['@redis.factory', '@cache_tags_provider.container', '@serialization.phpserialize'],\n      ],\n      'cache.container' =&gt; [\n        'class' =&gt; '\\Drupal\\redis\\Cache\\PhpRedis',\n        'factory' =&gt; ['@cache.backend.redis', 'get'],\n        'arguments' =&gt; ['container'],\n      ],\n      'cache_tags_provider.container' =&gt; [\n        'class' =&gt; 'Drupal\\redis\\Cache\\RedisCacheTagsChecksum',\n        'arguments' =&gt; ['@redis.factory'],\n      ],\n      'serialization.phpserialize' =&gt; [\n        'class' =&gt; 'Drupal\\Component\\Serialization\\PhpSerialize',\n      ],\n    ],\n  ];\n}\n\nThe example.services.yml file noted above will also use Redis for the lock and flood control systems.\nThe redis module is able to use Redis as a queue backend, however, that should not be done on an ephemeral Redis instance as that could result in lost items when the Redis service instance is restarted or fills up.  If you wish to use Redis for the queue we recommend using a separate persistent Redis instance.  See the Redis documentation page for more information.\nVerifying Redis is running\nRun this command in a SSH session in your environment redis-cli -h redis.internal info. You should run it before you push all this new code to your repository.\nThis should give you a baseline of activity on your Redis installation. There should be very little memory allocated to the Redis cache.\nAfter you push this code, you should run the command and notice that allocated memory will start jumping.\nClear SQL cache tables\nOnce you've confirmed that your site is using Redis for caching, you can and should purge any remaining cache data in the MySQL database as it is now just taking up space.  TRUNCATE any table that begins with cache except for cache_form.  Despite its name cache_form is not part of the cache system proper and thus should not be moved out of SQL.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Drupal8", "title": "Developing with Drupal", "url": "/frameworks/drupal8/developing-with-drupal.html", "documentId": "db1a96b990e4cf9c161658b5cad54e6607666d5f", "text": "\n                        \n                            \n                                \n                                \n                                Developing with Drupal\nPush changes to an environment\nHere, we'll see how to make code changes to an environment.\n\nnote\nYou should never be working on the Master branch since it's supposed to be your production environment.\n\nMake sure you're on a working environment. In this example we're on the\nsprint1 branch:\n$ git checkout sprint1\n\nNow that you're set up on your working branch, you can start developing\non your website by making code changes and pushing those changes to\nPlatform.sh to test them live.\nThere are three common ways you will be making code changes to Platform:\n\nAdd contributed modules, themes, distributions, third-party\nlibraries in the make file\nCreate custom code (modules, themes, profiles, libraries) and\ncommit them to your Platform.sh codebase\nModify the services grid configuration\n\nAdd contributed projects\nEach time you push a commit, Platform.sh will rebuild your environment\nand run the Composer command if a proper composer.json file has been found.\nAdd a Drupal module or theme\nEach Drupal module or theme you want to install on your project should be\nincluded in your composer.json file.  For example:\n$ composer require drupal/token\n\nThat will update your composer.json and composer.lock files, which you can then commit.\nIf you're using Composer, 3rd party PHP libraries can be added in the exact same way as Drupal modules.\nAdd custom code\nTo commit your custom modules, themes, or libraries, add those to the web/modules/custom or web/themes/custom directory and commit them to Git as normal.\nChange the services configuration\nYou can change and define the topology of the services used in an\nenvironment, by modifying the configuration files.\nThis means that you're able to define and configure the services you\nwant to use.\nPush your changes\nWhen you're done, commit your changes to test them on your online\nenvironment.\n$ git add .\n$ git commit -m \"Made changes to my files.\"\n$ git push\n\nYou will see that Platform has found a make file and is starting to\nrebuild your environment.\nWhen it's completed, you can see your changes on your site by clicking\nView this website under the name of Sprint1 environment on the\nPlatform.sh management console.\n\nnote\nThe build process makes no changes to your Git repository.  Your Git repository is the input of the process. A PHP container containing your code and dependencies is the output. You can see the directory structure that has been created by connecting via SSH to the environment. See the information in the Access information below the title of the environment\n\nMerge code changes to Master\nOnce you've got a branch with some changes, you'll want to be able to\npush those changes up to your live environment. Platform.sh has a great\nbutton called Merge that you can click on and it will push the\nappropriate changes to master.\n\nA dialog box will appear that will provide commands to execute future merges from the command line using the Platform.sh CLI.\n\nJust click on the \"Merge\" button again and all of the commits you made on your\nbranch will be merged into the master environment.\nSynchronizing data\nThe easiest way to do that is to use Drush and the sql-sync command.\nYou'll need to have Drush aliases for both your\nPlatform.sh site and your local site. If you are using the CLI and\nyou've run platform get [platform_id] for a project, then your Drush\naliases have already been set up.\nWith the Drush aliases (depending on how yours are set up), you\ncould use a command similar to this:\n$ drush sql-sync @platform.master @platform._local\n\nAn alternate method that is appropriate for larger databases is to use\nthe pipe | to stream the data, instead of making a copy of the dump file.\n$ drush @platform.master sql-dump | drush @platform._local sqlc\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Frameworks", "title": "Drupal 8", "url": "/frameworks/drupal8.html", "documentId": "2bd46a7e1c9c80b36da0962b4f9520a707e621b2", "text": "\n                        \n                            \n                                \n                                \n                                Getting Started\nDrupal 8 and Composer\nThe recommended way to deploy Drupal 8 on Platform.sh is to use Composer. Composer is the PHP package management suite, and is now supported by Drupal 8 (and Drupal 7 in a pinch). There is an unofficial but well-supported Composer flavor of Drupal 8 called Drupal Composer that we recommend.  If you use the Drupal 8 Example Repository or select the Drupal 8 option when creating a new project from a template, that's what you will be using.\nYou can also create your own project directly from that repository and add the Platform.sh-specific configuration files.  Note that you will also need to add the Drupal.org Composer repositories to composer.json if you are not working from our template.\nIf you use Drupal Composer, note that any 3rd party modules, themes, or PHP libraries you install, as well as Drupal core itself, will not be checked into your repository.  They are specifically excluded from Git by a .gitignore file, as they will be re-downloaded when you run composer install or composer update.  Rather than downloading modules or themes using wget or FTP, you can add them using composer.  For example, to add the devel module you would run this command:\n$ composer require drupal/devel\nAnd then commit just the changes to composer.json and composer.lock to your repository.  That also means that to get a working copy of your site locally you will need to run composer install to download all of the necessary libraries and modules.\nWe also strongly recommend installing the Platform.sh Config Reader library, which simplifies access to the Platform.sh environment.  The rest of this documentation assumes you have it installed.\n$ composer require platformsh/config-reader\n\n\nnote\nWhen using Composer, your docroot where most of Drupal lives will be called web, but the vendor directory will be outside of that directory in contrast to how a standard Drupal download .tar.gz file is organized.  The config export directory will also be outside of the web root.  This is normal, expected, and more secure.\n\nFile organization\nYour repository should be laid out as follows:\ncomposer.json\ncomposer.lock\nconfig/\n  sync/\n    &lt;this is where exported configuration will go&gt; \ndrush/\n.git/\n.gitignore\n.platform/\n  routes.yaml\n  services.yaml\n.platform.app.yaml\nscripts/\nweb\n  index.php\n  ... (other Drupal core files)\n  modules/\n    contrib/\n      &lt;empty until composer runs&gt;\n    custom/\n      &lt;your custom modules here&gt;\n  themes/\n    contrib/\n      &lt;empty until composer runs&gt;\n    custom/\n      &lt;your custom themes here&gt;\n  sites/\n    default/\n      settings.php\n      settings.platformsh.php\nChanges to settings.php\nPlatform.sh exposes database configuration, as well as other configuration values such as a hash salt, to PHP as environment variables available either via $_ENV or getenv().  That means you'll need to tell Drupal how to get that information.  Additionally, Drupal needs to be told where the config export directory is, where the private files directory is (which is outside of the web root), and so on.\nThe easiest way to access that information is via a small configuration add-on we provide.  See our recommended settings.php file, which includes a file called settings.platformsh.php.  The latter maps all Platform.sh-provided environment values to Drupal settings, either the Drupal database array or the global $settings object.  If run on a non-Platform.sh server this file does nothing so it is safe to always include.\nIf you need to add additional Platform.sh-specific configuration, such as to enable a Redis server for caching, we recommend also putting it into settings.platformsh.php.\nVanilla Drupal 8\nIf you prefer, Drupal 8 can also be installed \"vanilla\" from Drupal.org, with the entire site checked into the repository. While not recommended it is fully supported.  At the end of the day Platform.sh doesn't care where your files come from, just that you tell the system where they are!\nYou will still need to put the Drupal docroot in a subdirectory of your repository.  We recommend web for consistency but any directory name will do.\nIf using a vanilla Drupal install, your repository should look something like this:\n.git/\n.gitignore\nconfig/\n  sync/\n.platform/\n  routes.yaml\n  services.yaml\n.platform.app.yaml\nweb/\n  index.php\n  ... (other Drupal core files)\n  core/\n  modules/\n  sites/\n  sites/\n    default/\n      settings.php\n      settings.platformsh.php\nNote the settings.php and settings.platformsh.php files.  Both should be identical to the ones used for a Composer-based site.  Also note that the config/sync directory is still outside the docroot.  That is recommended for all Drupal installs generally, and is configured by the settings.php file.\nConfiguring Platform.sh for Drupal\nThe ideal .platform.app.yaml file will vary from project to project, and you are free to customize yours as needed.  A recommended baseline Drupal 8 configuration is listed below, and can also be found in our Drupal 8 template project.\n# This file describes an application. You can have multiple applications\n# in the same project.\n#\n# See https://docs.platform.sh/user_guide/reference/platform-app-yaml.html\n\n# The name of this app. Must be unique within a project.\nname: 'app'\n\n# The runtime the application uses.\ntype: 'php:7.4'\n\n# The relationships of the application with services or other applications.\n#\n# The left-hand side is the name of the relationship as it will be exposed\n# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand\n# side is in the form `:`.\nrelationships:\n    database: 'db:mysql'\n    redis: 'cache:redis'\n\n# The size of the persistent disk of the application (in MB).\ndisk: 2048\n\n# The 'mounts' describe writable, persistent filesystem mounts in the application.\nmounts:\n    '/web/sites/default/files':\n        source: local\n        source_path: 'files'\n    '/tmp':\n        source: local\n        source_path: 'tmp'\n    '/private':\n        source: local\n        source_path: 'private'\n    '/.drush':\n        source: local\n        source_path: 'drush'\n    '/drush-backups':\n        source: local\n        source_path: 'drush-backups'\n    '/.console':\n        source: local\n        source_path: 'console'\n\n# Configuration of the build of this application.\nbuild:\n    flavor: composer\n\n# The hooks executed at various points in the lifecycle of the application.\nhooks:\n    # The build hook runs after Composer to finish preparing up your code.\n    build: |\n        set -e\n        bash install-redis.sh 4.3.0\n    # The deploy hook runs after your application has been deployed and started.\n    deploy: |\n        set -e\n        php ./drush/platformsh_generate_drush_yml.php\n        cd web\n        drush -y cache-rebuild\n        drush -y updatedb\n        drush -y config-import\n\n# The configuration of app when it is exposed to the web.\nweb:\n    # Specific parameters for different URL prefixes.\n    locations:\n        '/':\n            # The folder from which to serve static assets, for this location.\n            #\n            # This is a filesystem path, relative to the application root.\n            root: 'web'\n\n            # How long to allow static assets from this location to be cached.\n            #\n            # Can be a time in seconds, or -1 for no caching. Times can be\n            # suffixed with \"s\" (seconds), \"m\" (minutes), \"h\" (hours), \"d\"\n            # (days), \"w\" (weeks), \"M\" (months, as 30 days) or \"y\" (years, as\n            # 365 days).\n            expires: 5m\n\n            # Whether to forward disallowed and missing resources from this\n            # location to the application.\n            #\n            # Can be true, false or a URI path string.\n            passthru: '/index.php'\n\n            # Deny access to static files in this location.\n            allow: false\n\n            # Rules for specific URI patterns.\n            rules:\n                # Allow access to common static files.\n                '\\.(jpe?g|png|gif|svgz?|css|js|map|ico|bmp|eot|woff2?|otf|ttf)$':\n                    allow: true\n                '^/robots\\.txt$':\n                    allow: true\n                '^/sitemap\\.xml$':\n                    allow: true\n\n                # Deny direct access to configuration files.\n                '^/sites/sites\\.php$':\n                    scripts: false\n                '^/sites/[^/]+/settings.*?\\.php$':\n                    scripts: false\n\n        '/sites/default/files':\n            # Allow access to all files in the public files directory.\n            allow: true\n            expires: 5m\n            passthru: '/index.php'\n            root: 'web/sites/default/files'\n\n            # Do not execute PHP scripts.\n            scripts: false\n\n            rules:\n                # Provide a longer TTL (2 weeks) for aggregated CSS and JS files.\n                '^/sites/default/files/(css|js)':\n                    expires: 2w\n\n# The configuration of scheduled execution.\ncrons:\n    drupal:\n        spec: '*/20 * * * *'\n        cmd: 'cd web ; drush core-cron'\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Drupal7", "title": "FAQ", "url": "/frameworks/drupal7/faq.html", "documentId": "d7647ddd063cb25256cca8cf84d28221e186cc30", "text": "\n                        \n                            \n                                \n                                \n                                Drupal Frequently Asked Questions (FAQ)\nHow should I name my make files?\nIn order for Platform to automatically detect your make file, you need\nto call it project.make.\nYou can also have a specific make file for Drupal core called\nproject-core.make\nWhen I push changes to a make file, does Platform.sh run the update?\nAfter a push, Platform.sh will rebuild your environment and download all\nthe modules that are in your make file.\nIf an update function (hook_update) needs to run, you'll have to\nmanually trigger it by going to /update.php or use the\ndeployment hooks to automatically run the updates.\nHow can I provide a robots.txt file in production?\nIf using the drupal build mode with a Drush make file, place your robots.txt file in your application root, as a sibling of .platform.app.yaml.  It will get moved to the appropriate location automatically by the build process.  For all other cases just include the file in your web root normally.\nOn non-production environments Platform.sh automatically blocks web crawlers using the X-Robots-Tag header.  You can disable that per-environment if needed.\nI'm getting a PDO Exception 'MySQL server has gone away'\nNormally, this means there is a problem with the MySQL server container\nand you may need to increase the storage available to MySQL to resolve\nthe issue. Ballooning MySQL storage can be caused by a number of items:\n\nA large number of watchdog entries being captured. Fix the errors\n being generated or disable database logging.\nCron should run at regular intervals to ensure cache\n tables get cleared out.\nIf you're using Drupal Commerce Core &lt; 1.10, you may have an\n extremely large cache_form\n table. Upgrade to Commerce Core 1.10 to resolve.\n\nWhy do I get \"MySQL cannot connect to the database server\"?\nIf you are having a problem connecting to the database server, you will\nneed force a re-deployment of the database container. To do so, you can\nedit the service definition to add or remove a small amount of storage and\nthen push.\nCan I use the name of the session cookie for caching?\nFor Drupal sites, the name of the session cookie is based on a hash of the\ndomain name. This means that it will actually be consistent for a specific\nwebsite and can safely be used as a fixed value.\nHow can I rebuild the site registry?\nDuring the migration process, one or more modules may have changed\nlocation. This could result in a WSOD (white screen of death), any\nnumber of errors (fatal or otherwise), or just a plain broken site. To\nremedy this situation, the registry will need to be\nrebuilt. To rebuild\nthe Drupal registry on your Platform.sh instance, you will need to do\nthe following:\nFirst, SSH into your web container.\n$ ssh [SSH-URL]\n\nSecond, execute the following commands to download, tweak, and run the\nregistry rebuild.\n$ drush dl registry_rebuild-7.x-2.3 --destination=/app/tmp\n$ sed -i 's/, define_drupal_root()/, '\"'\"'\\/app\\/public'\"'\"'/' /app/tmp/registry_rebuild/registry_rebuild.php\n$ cd /app/public\n$ php ../tmp/registry_rebuild/registry_rebuild.php\n\nCan I use Backup &amp; Migrate?\nThe Backup &amp; Migrate module is a Drupal module that provides automated scheduled dumps of a Drupal site's content.  It does so in the form of an SQL dump and/or tar.gz archived copy of your site's file directory, which can then be optionally uploaded to a remote storage service.\nIn general B&amp;M is not necessary when running on Platform.sh.  Platform.sh's Backup functionality offers a faster, more robust and easier to restore backup, and for exporting data using the Platform.sh CLI is just as effective.\nIf, however, you find it necessary to still run B&amp;M be aware that its resource requirements can be quite high.  B&amp;M requires a great deal of memory in order to create a backup, over and above Drupal's memory requirements.  It is possible for B&amp;M to create a backup in the system's temp folder, then PHP runs out of memory before it can complete sending the backup to a 3rd party or cleaning up the temp file.  In the latter case, a full temp disk can result in other, seemingly unrelated issues such as an inability to upload files.\nIf you find B&amp;M failing or the temp directory filling up mysteriously, try increasing the PHP memory limit to account for B&amp;M's needs.  For example, add the following to .platform.app.yaml:\nvariables:\n    php:\n        memory_limit: 512M\n\nIf that is still insufficient, your site may simply be too large to work effectively with B&amp;M.  We recommend setting up automated scheduled backups instead.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Drupal8", "title": "Drush", "url": "/frameworks/drupal8/drush.html", "documentId": "8d40770e87eb064ff478fd1a9003c327077f92d4", "text": "\n                        \n                            \n                                \n                                \n                                Working with Drush in Drupal 8\nDrush is a command-line shell and scripting interface for Drupal, a veritable Swiss Army knife designed to make life easier for those who spend their working hours hacking away at the command prompt. Drush commands can, for example, be used to clear the Drupal cache, run module and database updates, revert features, perform database imports and dumps, and a whole lot more. You can reference the full set of Drush commands at Drush.org. If you have never used Drush before, you can learn more about it on the Drush GitHub Repository\n\nPlatform.sh's Drupal templates have Drush installed.\nDrush commands can be run in build, deploy, and post_deploy hooks, although remember that as the database is not available at build time many Drush commands will not work at build time.\nIn addition, you can use the Platform.sh CLI to set up Drush aliases easily for all of your project's environments. See the section below on use drush aliases\n\n\nnote\nPlatform's CLI requires Drush 6 or higher.\n\nInstalling Drush\nUse the Platform.sh-provided Drupal examples\nIf you started your project from one of Platform.sh's Drupal templates then Drush is already installed and configured.  There is nothing else for you to do.\nInstall Drush in custom projects using Composer\nThis is the recommended approach.\nRun this command in the project's repository root folder:\n$ composer require drush/drush\n\nCommit the composer.json and composer.lock files and push.\nDrush will then be available at vendor/bin/drush, in the exact same version on your local system and on Platform.sh.\nInstall Drush in custom projects using Build Dependencies\nPlatform.sh supports installing some as system-level tools as build dependencies.\nTo install Drush using this method, use the following in .platform.app.yaml\ndependencies:\n    php:\n        \"drush/drush\": \"^8.0\"\n\nAccessing Drush within the project\nFor Drush to be available on the command line, it must be added to the project's $PATH.\nAdd a new file named .environment to the root of your your project's git repository with this code:\n# Statements in this file will be executed (sourced) by the shell in SSH\n# sessions, in deploy hooks, in cron jobs, and in the application's runtime\n# environment. This file must be placed in the root of the application, not\n# necessarily the git repository's root. In case of multiple applications,\n# each application can have its own .environment file.\n\n# Allow executable app dependencies from Composer to be run from the path.\nif [ -n \"$PLATFORM_APP_DIR\" -a -f \"$PLATFORM_APP_DIR\"/composer.json ] ; then\n  bin=$(composer config bin-dir --working-dir=\"$PLATFORM_APP_DIR\" --no-interaction 2&gt;/dev/null)\n  export PATH=\"${PLATFORM_APP_DIR}/${bin:-vendor/bin}:${PATH}\"\nfi\n\nInstall Drush locally\nYou can install drush globally with Composer. This does not add it to your project.\n$ composer global require drush/drush\n\nAt the end of the installation, you should be able to run:\n$ drush\n\nUse drush aliases\nCreate Drush aliases\nDrush aliases make it easy to manage your development websites. Here's an example of a Drush alias file.\nThe Platform.sh CLI generates Drush aliases for you automatically when you run platform get [project_id]. \nTo see the aliases that are created, run platform drush-aliases and you should get output similar to that below:\n$ platform drush-aliases\nAliases for My Site (tqmd2kvitnoly):\n    @my-site._local\n    @my-site.master\n    @my-site.staging\n    @my-site.sprint1\n\nRecreating Drush aliases\nTo recreate existing aliases, or after pushing a new branch via git to create the new alias, run:\nplatform drush-aliases -r\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Drupal7", "title": "SimpleSAML", "url": "/frameworks/drupal7/simplesaml.html", "documentId": "e01507d8290b825a3a56e87245b2fb9cc5eb6837", "text": "\n                        \n                            \n                                \n                                \n                                SimpleSAML\nSimpleSAMLphp is a library for authenticating a PHP-based application against a SAML server, such as Shibboleth.  Although Drupal has modules available to authenticate using SimpleSAML some additional setup is required.\nThe following setup assumes you're using the drupal build flavor and building your site with Drush Make.  If not, you may need to adjust some paths in the configuration but the basics are the same.\nDownload the library\nFirst, download the 3rd party SimpleSAMLphp library.  When you unpack the tar.gz file it will contain a directory named simplesamplephp-???, where the ??? is the version number of the library.  Place that directory at the root of your application, as a sibling of your .platform.app.yaml file, named simplesamplephp.  (The directory name doesn't really matter but removing the version number means that it won't change in future updates.) \nThe drupal build flavor will move that directory to the public/sites/default/ directory during build.  The rest of the configuration is based on that behavior.\nInclude SimpleSAML cookies in the cache key\nThe SimpleSAML client uses additional cookies besides the Drupal session cookie that need to be whitelisted for the cache.  To do so, modify your routes.yaml file for the route that points to your Drupal site and add two additional cookies to the cache.cookies line.  It should end up looking approximately like this:\n\"https://{default}/\":\n    type: upstream\n    upstream: \"app:http\"\n    cache:\n      enabled: true\n      cookies: ['/^SS?ESS/', '/^Drupal.visitor/', 'SimpleSAMLSessionID', 'SimpleSAMLAuthToken']\n\nCommit this change to the Git repository.\nExpose the SimpleSAML endpoint\nThe SimpleSAML library's www directory needs to be publicly accessible.  That can be done by mapping it directly to a path in the Application configuration.  Add the following block to the web.locations section of .platform.app.yaml:\n web:\n    locations:\n        '/simplesaml':\n            root: 'public/sites/default/simplesamlphp/www'\n            allow: true\n            scripts: true\n            index:\n                - index.php\n\nThat will map all requests to example.com/simplesaml/ to the simplesamlphp/www directory, allowing static files there to be served, PHP scripts to execute, and defaulting to index.php.\nInstall the simpleSAMLphp Authentication module\nYou will need to install the simpleSAMLphp Authentication module.  If using Drush Make then the easiest way to do so is simply to add the following line to your project.make file:\nprojects[simplesamlphp_auth][version] = 2.0-alpha2\n\n(Adjust the version to whatever is current.)\nMuch of the module configuration will depend on your Identity Provider (IdP).  However, the module also need to know the location of your simplesamlphp_auth module.  The easiest way to set it is to include the following at the end of your settings.platformsh.php file:\n// Set the path for the SimpleSAMLphp library dynamically.\n$conf['simplesamlphp_auth_installdir'] = __DIR__ . '/simplesamlphp';\n\nDeploy the site and enable the simplesamlphp_auth module.\nConsult the module documentation for further information on how to configure the module itself.  Note that you should not check the \"Activate authentication via SimpleSAMLphp\" checkbox in the module configuration until you have the rest of the configuration completed or you may be locked out of the site.\nConfigure SimpleSAML to use the database\nSimpleSAMLphp is able to store its data either on disk or in the Drupal database.  Platform.sh strongly recommends using the database.\nOpen the file simplesamlphp/config/config.php.  It contains a number of configuration properties that you can adjust as needed.  Some are best edited in-place and the file already includes ample documentation, specifically:\n\nauth.adminpassword\ntechnicalcontact_name\ntechnicalcontact_email\n\nOthers are a little more involved.  In the interest of simplicity we recommend simply pasting the following code snippet at the end of the file, as it will override the default values in the array.\n// Set SimpleSAML to log using error_log(), which on Platform.sh will\n// be mapped to the /var/log/app.log file.\n$config['logging.handler'] = 'errorlog';\n\n// Set SimpleSAML to use the metadata directory in Git, rather than\n// the empty one in the vendor directory.\n$config['metadata.sources'] = [\n   ['type' =&gt; 'flatfile', 'directory' =&gt;  dirname(__DIR__) . '/metadata'],\n];\n\n// Setup the database connection for all parts of SimpleSAML.\nif (isset($_ENV['PLATFORM_RELATIONSHIPS'])) {\n  $relationships = json_decode(base64_decode($_ENV['PLATFORM_RELATIONSHIPS']), TRUE);\n  foreach ($relationships['database'] as $instance) {\n    if (!empty($instance['query']['is_master'])) {\n      $dsn = sprintf(\"%s:host=%s;dbname=%s\", \n        $instance['scheme'], \n        $instance['host'], \n        $instance['path']\n      );\n      $config['database.dsn'] = $dsn; \n      $config['database.username'] = $instance['username'];\n      $config['database.password'] = $instance['password'];\n\n      $config['store.type'] = 'sql';\n      $config['store.sql.dsn'] = $dsn;\n      $config['store.sql.username'] = $instance['username'];\n      $config['store.sql.password'] = $instance['password'];\n      $config['store.sql.prefix'] = 'simplesaml';\n\n    }\n  }\n}\n\n// Set the salt value from the Platform.sh entropy value, provided for this purpose.\nif (isset($_ENV['PLATFORM_PROJECT_ENTROPY'])) {\n  $config['secretsalt'] = $_ENV['PLATFORM_PROJECT_ENTROPY'];\n}\n\nGenerate SSL certs (optional)\nYou may need to generate an SSL/TLS certificate, depending on your Identity Provider (IdP).  If so, you should generate the certificate locally following the instructions in the SimpleSAMLphp documentation.  Your resulting IdP file should be placed in the simplesamlphp/metadata directory.  The certificate should be placed in the simplesamlphp/cert directory.  (Create it if needed.)\nThen add the following line to your simplesamlphp/config/config.php file to tell the library where to find the certificate:\n$config['certdir'] = dirname(__DIR__) . '/cert';\n\nDeploy\nCommit all changes and deploy the site, then enable the simplesamlphp_auth module within Drupal.\nConsult the module documentation for further information on how to configure the module itself.  Note that you should not check the \"Activate authentication via SimpleSAMLphp\" checkbox in the module configuration until you have the rest of the configuration completed or you may be locked out of the site.\nRecovering from a locked site\nIf SimpleSAML is misconfigured it is possible to find yourself locked out of the site, as it will try to authenticate against a SimpleSAML server, fail, and then disallow other logins.  If that happens, the easiest way to recover it is to disable the SimpleSAML login.  That can be done with the following command:\nplatform ssh \"cd public &amp;&amp; drush vset simplesamlphp_auth_activate 0\"\n\nAlternatively you could log into the server and run the drush command there yourself.\nIf that doesn't work it is likely that the configuration is \"pinned\" using Features or via settings.php.  Instead disable the module entirely, then remove the \"pin\" and re-enable it.\nplatform ssh \"cd public &amp;&amp; drush pm-disable simplesamlphp_auth -y\"\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Drupal7", "title": "Search API", "url": "/frameworks/drupal7/search-api-module.html", "documentId": "2b723c7c30906dfaad766ea6bd743c277bcf46ea", "text": "\n                        \n                            \n                                \n                                \n                                Using Solr with the module Search API on Drupal 7.x\nThis page is about configuring Solr with the module Search API. If your project uses Apache Solr Search then you should follow the instructions Apache Solr Search.\nRequirements\nYou will need to add the Search API and Search API\nSolr modules to your project. The Search API Override module is strongly recommended in order to allow the Solr configuration to be populated from settings.php.\nIf you are using a make file, you can add those lines to your\nproject.make:\nprojects[entity][version] = 1.8\nprojects[search_api][version] = 1.20\nprojects[search_api_solr][version] = 1.11\nprojects[search_api_override][version] = 1.0-rc1\n\nConfiguration\nThe Search API module includes recommended configuration files to use with Drupal.  See the Solr configuration page for details of how to configure your Solr server to use the Drupal configuration files.  Note that the Drupal 7 version of Search API Solr does not include configuration files for Solr 6.  The Drupal 8 version of the module does, however, and should work acceptably.  It can also be customized as desired.\nThe Search API Override module (listed above) allows Search API configuration to be overridden from settings.php.  Once it has been enabled, add the following to your settings.platformsh.php file:\nif (isset($_ENV['PLATFORM_RELATIONSHIPS'])) {\n  $relationships = json_decode(base64_decode($_ENV['PLATFORM_RELATIONSHIPS']), TRUE);\n\n  if (!empty($relationships['solr'])) {\n    // Override search API server settings fetched from default configuration.\n    $conf['search_api_override_mode'] = 'load';\n    foreach ($relationships['solr'] as $endpoint) {\n      $conf['search_api_override_servers'] = array(\n        'MACHINE_NAME_OF_SOLR_SERVER' =&gt; array(\n          'options' =&gt; array(\n            'host' =&gt; $endpoint['host'],\n            'port' =&gt; $endpoint['port'],\n            'path' =&gt; '/' . $endpoint['path'],\n            'http_method' =&gt; 'POST',\n          ),\n        ),\n      );\n    }\n  }\n}\n\nReplace MACHINE_NAME_OF_SOLR_SERVER with the Drupal machine name of the server you want to override.  The solr server must already be defined in Drupal and ideally exported to a Feature.\nRelationships configuration\nIf you did not name the relationship solr in your .platform.app.yaml file, adjust the name accordingly.  Also, if you have multiple Solr cores defined the above foreach() loop will not work.  Most likely you will want to name the relationships by the machine name of the Solr server they should map to and then map each one individually.\nThe file .platform.app.yaml must have the Solr relationship enabled, such as this snippet:\nrelationships:\n    solr: 'solrsearch:solr'\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Drupal7", "title": "Apache Solr Search", "url": "/frameworks/drupal7/apachesolr-module.html", "documentId": "35f459febd6a6df24ea54be00b339aa41b863bb9", "text": "\n                        \n                            \n                                \n                                \n                                Using Solr with the module Apache Solr Search on Drupal 7.x\nThis page is about configuring Solr with the module Apache Solr Search. If your project uses Search API then you should follow the instructions Search API.\nRequirements\nYou will need the module  Apache Solr Search\nIf you are using a make file, you can add those lines to your\nproject.make:\nprojects[apachesolr][version] = 1.8\n\nConfiguration\nThe Apache Solr Search module allows configuration to be overridden from settings.php.  Just add the following to your settings.platformsh.php file:\nif (isset($_ENV['PLATFORM_RELATIONSHIPS'])) {\n  $relationships = json_decode(base64_decode($_ENV['PLATFORM_RELATIONSHIPS']), TRUE);\n\n  if (!empty($relationships['solr'])) {\n    // Override search API server settings fetched from default configuration.\n    foreach ($relationships['solr'] as $endpoint) {\n\n      // If your Solr server's machine name is not \"solr\", update the following line.\n      $environment_machine_name = 'solr';\n\n      $environment_url = \"http://\" . $endpoint['host'] . \":\" . $endpoint['port'] . \"/\" . $endpoint['path'];\n      $conf['apachesolr_default_environment'] = $environment_machine_name;\n      $conf['apachesolr_environments'][$environment_machine_name]['url'] = $environment_url;\n    }\n  }\n}\n\nNote that the Solr server must already be defined in Drupal and ideally exported to a Feature. The most common machine name used is just solr, as above.  If you used a different name adjust the code as appropriate.\nRelationships configuration\nIf you did not name the relationship solr in your .platform.app.yaml file, adjust the name accordingly.  Also, if you have multiple Solr cores defined the above foreach() loop will not work.  Most likely you will want to name the relationships by the machine name of the Solr server they should map to and then map each one individually.\nThe file .platform.app.yaml must have the Solr relationship enabled, such as this snippet:\nrelationships:\n    solr: 'solrsearch:solr'\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Drupal7", "title": "Memcached", "url": "/frameworks/drupal7/memcached.html", "documentId": "83d790678cd491cc3a5c506b19a784bfdad2c998", "text": "\n                        \n                            \n                                \n                                \n                                Using Memcached with Drupal 7.x\nPlatform.sh recommends using Redis for caching with Drupal 7 over Memcached, as Redis offers better performance when dealing with larger values as Drupal tends to produce.  However, Memcached is also available if desired and is fully supported.\nRequirements\nAdd a Memcached service\nFirst you need to create a  Memcached service.  In your .platform/services.yaml file, add or uncomment the following:\ncacheservice:\n    type: memcached:1.4\n\nThat will create a service named cacheservice, of type memcached, specifically version 1.4.\nExpose the Memcached service to your application\nIn your .platform.app.yaml file, we now need to open a connection to the new Memcached service.  Under the relationships section, add the following:\nrelationships:\n    cache: 'cacheservice:memcached'\n\nThe key (left side) is the name that will be exposed to the application in the PLATFORM_RELATIONSHIPS variable.  The right hand side is the name of the service we specified above (cacheservice) and the endpoint (memcached).  If you named the service something different above, change cacheservice to that.\nAdd the Memcached PHP extension\nYou will need to enable the PHP Memcached extension.  In your .platform.app.yaml file, add the following right after the type block:\n# Additional extensions\nruntime:\n    extensions:\n        - memcached\n\nAdd the Drupal module\nYou will need to add the Memcache module to your project.  If you are using a Drush Make file, add the following line to your project.make file:\nprojects[memcache][version] = 1.6\n\nThen commit the \n\nnote\nYou must commit and deploy your code before continuing, then enable the module. The memcache \nmodule must be enabled before it is configured in the settings.platformsh.php file.\n\nConfiguration\nThe Drupal Memcache module must be configured via settings.platformsh.php.\nPlace the following at the end of settings.platformsh.php. Note the inline comments, as you may wish to customize it further.  Also review the README.txt file that comes with the memcache module, as it has a more information on possible configuration options. For instance, you may want to consider using memcache for locking as well and configuring cache stampede protection.\nThe example below is intended as a \"most common case\".\nif (!empty($_ENV['PLATFORM_RELATIONSHIPS']) &amp;&amp; extension_loaded('memcached')) {\n  $relationships = json_decode(base64_decode($_ENV['PLATFORM_RELATIONSHIPS']), true);\n\n  // If you named your memcached relationship something other than \"cache\", set that here.\n  $relationship_name = 'cache';\n\n  if (!empty($relationships[$relationship_name])) {\n    // These lines tell Drupal to use memcached as a backend.\n    // Comment out just these lines if you need to disable it for some reason and\n    // fall back to the default database cache.\n    $conf['cache_backends'][] = 'sites/all/modules/contrib/memcache/memcache.inc';\n    $conf['cache_default_class'] = 'MemCacheDrupal';\n    $conf['cache_class_cache_form'] = 'DrupalDatabaseCache';\n\n    // While we're at it, use Memcache for locking, too.\n    $conf['lock_inc'] = 'sites/all/modules/contrib/memcache/memcache-lock.inc';\n\n    foreach ($relationships[$relationship_name] as $endpoint) {\n      $host = sprintf(\"%s:%d\", $endpoint['host'], $endpoint['port']);\n      $conf['memcache_servers'][$host] = 'default';\n    }\n\n    // If using a multisite configuration, adapt this line to include a site-unique\n    // value.\n    $conf['memcache_key_prefix'] = $PLATFORM_ENVIRONMENT;\n  }\n}\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Drupal7", "title": "Customizing settings.php", "url": "/frameworks/drupal7/customizing-settings-php.html", "documentId": "d40a8a6669bacfbad23469535fd5966b49feb4f0", "text": "\n                        \n                            \n                                \n                                \n                                Customizing settings.php\nFor applications using the drupal build flavor (those based on our Drupal 7\nexample), Platform.sh automatically generates a settings.php file if not present and will always generate a settings.local.php file. This allows the Drupal site to be connected to MySQL without any additional configuration.\nIf you wish to customize either file, we recommend instead using the example files provided in our Drupal 7 project template.  There are two: settings.php and settings.platformsh.php.  The former will automatically include the latter, and all Platform.sh-specific configuration is found in the settings.platformsh.php file.  It will also automatically include a settings.local.php file if found so it will not conflict with your local development workflow.\n\nnote\nYou should never commit a settings.local.php file to your repository.\n\nIf you need to add additional configuration that is specific to Platform.sh, such as connecting to additional services like Redis or Solr, those changes should go in the settings.platformsh.php file.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Drupal7", "title": "Drush", "url": "/frameworks/drupal7/drush.html", "documentId": "1929999fd12fc5d0edb4aa83495b7a52a54dab68", "text": "\n                        \n                            \n                                \n                                \n                                Working with Drush in Drupal 7\nDrush is a command-line shell and scripting interface for Drupal, a veritable Swiss Army knife designed to make life easier for those who spend their working hours hacking away at the command prompt. \nYou can use the CLI to set up Drush aliases, to easily run Drush commands on specific remote Platform.sh environments.\nSee the documentation on Drush in Drupal 8 for installation, drush aliases, and other general information.  The installation procedure is the same for both Drupal 7 and 8.\nDrush make\nPlatform.sh can automatically build your Drupal 7 site using Drush make files. This allows you to easily test specific versions, apply patches and keep your site up to date. It also keeps your working directory much cleaner as since it only contains your custom code.\nYour make file can be called: project.make or drupal-org.make.\nA basic make file looks like this:\napi = 2\ncore = 7.x\n\n; Drupal core.\nprojects[drupal][type] = core\nprojects[drupal][version] = 7.67\nprojects[drupal][patch][] = \"https://drupal.org/files/issues/install-redirect-on-empty-database-728702-36.patch\"\n\n; Drush make allows a default sub directory for all contributed projects.\ndefaults[projects][subdir] = contrib\n\n; Platform indicator module.\nprojects[platform][version] = 1.4\n\nWhen building as a profile, you need a make file for Drupal core called: project-core.make:\napi = 2\ncore = 7.x\n\nprojects[drupal][type] = core\n\nGenerate a make file from an existing site\nIf you want to generate a make file from your existing site, you can run:\n$ drush make-generate project.make\n\nThis will output a make file containing all your contributed modules, themes and libraries.\n\nMake generate command\n\nApply patches\nYou can apply contributed patches to your modules, themes or libraries within your project.make:\nprojects[features][version] = \"2.2\"\nprojects[features][patch][] = \"https://www.drupal.org/files/issues/alter_overrides-766264-45.patch\"\n\nYou can also apply self-hosted patches. Simply create a PATCHES folder at the root of your repository and add the patch as follow:\nprojects[uuid][version] = \"1.0-alpha5\"\nprojects[uuid][patch][] = \"PATCHES/fix-non-uuid-entity-load.patch\"\n\nWork with a DEV version\nWhen you are using a module that is in a DEV version, the best practice is to always target a specific commit ID so that you're always building the same \"version\" of the module:\n; CKEditor module: version 7.x-1.15+2-dev\nprojects[ckeditor][download][revision] = \"b29372fb446b547825dc6c30587eaf240717695c\"\nprojects[ckeditor][download][type] = \"git\"\nprojects[ckeditor][download][branch] = \"7.x-1.x\"\nprojects[ckeditor][type] = \"module\"\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Drupal7", "title": "Migrating", "url": "/frameworks/drupal7/migrating.html", "documentId": "dfda036b368e260e1f46fa375d20b5737e8d5d5e", "text": "\n                        \n                            \n                                \n                                \n                                Migrating an existing Drupal 7 site to Platform.sh\nOnce you've setup the code for your site as a Platform.sh project, you will need to upload your existing database and files directories as well to complete the site.\nImport database\nWith Drush (preferred)\nYou can use drush aliases to import your existing local database into\nPlatform.\nThe aliases here are examples. Use the CLI's platform drush-aliases command to find your own aliases.\ndrush @platform._local sql-dump &gt; backup_database.sql\n\nYou can also sanitize your database prior to import it into Platform.sh by\nrunning:\ndrush @platform._local sql-sanitize\n\nWhen you're ready, export your local database and then import it into\nyour remote Platform.sh environment.\ndrush @platform._local sql-dump &gt; local_database.sql\ndrush @platform.master sql-cli &lt; local_database.sql\n\nWhen the process completes, you can visit the URL of your development\nenvironment and test that the database has been properly imported.\nWithout Drush\nExport your database in an SQL file or in a compressed file.\nCopy it via SSH to the remote environment on Platform into the\n/app/tmp folder which is writable:\nscp database.sql [SSH-URL]:/app/tmp\n\nLog in to the environment via SSH and import the database:\nssh [SSH-URL]\nweb@[PROJECT-ID]-master--php:~$ mysql -h database.internal main &lt; tmp/database.sql\n\nImport files\nWith Drush\nYou can use Drush site aliases to import your existing local files.\n$ drush rsync @platform._local:%files @platform.master:%files\nYou will destroy data from [SSH-URL]:././sites/default/files and replace with data from ~/Sites/platform/sites/default/files/\nDo you really want to continue? (y/n): y\n\n\nnote\nDrush will verify that you are copying and overwriting the proper \"files\" folders, so double-check that information before you type y to continue.\n\nThis step may take some time, but when the process completes, you can\nvisit the URL of your master environment and test that the files\nhave properly been imported.\nWithout Drush\nGo to your Drupal root on your local machine and synchronize the files folder to your remote Platform environment:\n$ rsync -r sites/default/files/. [SSH-URL]:public/sites/default/files/\n\n\nnote\n The local files path may depend of your installation.\n The path in URL may vary depending on what your .platform.app.yaml file specifies as the root path and files mount.\n\nDirectly from server to platform.sh\nIf the files folder is too large to fit on your computer, you can transfer them directly from server to server. If you have a firewall between the origin server and platform.sh, you can use agent-forwarding to enable a direct connection:\n$ ssh -A -t [USER]@[ORIGIN-SERVER] ssh -A -t [SSH-URL]\n$ rsync -a --delete [USER]@[ORIGIN-SERVER]:/var/www/drupal/sites/default/files/ public/sites/default/files\n\n\nnote\nIf you are using a Mac OS computer, you might experience issues where files with non-ascii characters in them don't work after transfer because Mac OS X uses decomposed form (like \"a + \u00a8 = \u00e4\", a form known as NFD), not the usual composed form (\"\u00e4\", a form known as NFC used everywhere else). One workaround is to use the direct server-to-server transfer method mentioned above.\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Drupal7", "title": "Redis", "url": "/frameworks/drupal7/redis.html", "documentId": "124f6efa0fe82181b1fc8961859331f31d7c66a3", "text": "\n                        \n                            \n                                \n                                \n                                Using Redis with Drupal 7.x\nThere are two options for using Redis with Drupal on Platform.sh, you can either use the PhpRedis\nextension or the Predis library.  PhpRedis requires a PHP extension and should therefore be faster in most situations. Predis is written entirely in PHP and so would require no PHP extension to install locally, but at the cost of some performance.\nIf you are unsure which to use, we recommend using PhpRedis.\nRequirements\nAdd a Redis service\nFirst you need to create a Redis service.  In your .platform/services.yaml file,\nadd or uncomment the following:\nrediscache:\n    type: redis:5.0\n\nThat will create a service named rediscache, of type redis, specifically version 3.0.\nExpose the Redis service to your application\nIn your .platform.app.yaml file, you now need to open a connection to the new Redis service.  Under the relationships section, add the following:\nrelationships:\n    redis: \"rediscache:redis\"\n\nThe key (left side) is the name that will be exposed to the application in the PLATFORM_RELATIONSHIPS variable.  The right hand side is the name of the service you specified above (rediscache) and the endpoint (redis).  If you named the service something different above, change rediscache to that.\nAdd the Redis PHP extension\nBecause the Redis extension for PHP has been known to have BC breaks at times, we do not bundle a specific verison by default.  Instead, we provide a script to allow you to build your desired version in the build hook.  See the PHP-Redis page for a simple-to-install script and instructions.\n(Skip this part if using Predis.)\nAdd the Drupal module\nYou will need to add the Redis module to your project.\nIf you are using Drush Make, you can add these lines to your project.make file:\nprojects[redis][version] = 3.15\n\nTo use the Predis library, also add it to your make file:\nlibraries[predis][download][type] = get\nlibraries[predis][download][url] = https://github.com/nrk/predis/archive/v1.0.3.tar.gz\nlibraries[predis][directory_name] = predis\nlibraries[predis][destination] = libraries\n\nConfiguration\nTo make use of the Redis cache you will need to set some Drupal variables. You can either do this in your settings.php file or by setting Platform Variables directly via the management console.  In general, using the settings.php file is easier.\nVia settings.php\nTo configure Drupal 7 to use our Redis server for caching, place the following at the end of settings.php, after the include directive for settings.local.php:\nif (!empty($_ENV['PLATFORM_RELATIONSHIPS'])) {\n  $relationships = json_decode(base64_decode($_ENV['PLATFORM_RELATIONSHIPS']), TRUE);\n  if (!empty($relationships['redis'])) {\n    $conf['redis_client_host'] = $relationships['redis'][0]['host'];\n    $conf['redis_client_port'] = $relationships['redis'][0]['port'];\n    $conf['redis_client_interface'] = 'PhpRedis';\n    $conf['cache_backends'][]       = 'sites/all/modules/contrib/redis/redis.autoload.inc';\n    $conf['cache_default_class']    = 'Redis_Cache';\n    // The 'cache_form' bin must be assigned to non-volatile storage.\n    $conf['cache_class_cache_form'] = 'DrupalDatabaseCache';\n    // The 'cache_field' bin must be transactional.\n    $conf['cache_class_cache_field'] = 'DrupalDatabaseCache';\n  }\n}\n\nIf using Predis, change the PhpRedis reference to Predis (case-sensitive).\nIf your redis module is not installed in sites/all/modules/contrib, modify the\ncache_backends line accordingly.\nVia the management console\nAlternatively, add the following environment variables using the Platform.sh management console.\nNote, if you set a directory in the make file you will need to alter the variables to match.\ndrupal:cache_backends\n[\n   \"sites/all/modules/contrib/redis/redis.autoload.inc\"\n]\n\n\nnote\nRemember to tick the JSON Value box.\n\nUse the actual path to your Redis module in case it is in a different location. For example: sites/all/modules/redis. The\nlocation used above is the default when using Drush on Platform.sh.\ndrupal:redis_client_host\nredis.internal\n\ndrupal:cache_default_class\nRedis_Cache\n\ndrupal:cache_class_cache_form\nDrupalDatabaseCache\n\ndrupal:cache_class_cache_field\nDrupalDatabaseCache\n\nAnd finally, set the client interface to either PhpRedis or Predis.\ndrupal:redis_client_interface\nPhpRedis\n\nOr\nPredis\n\nVerifying Redis is running\nRun this command in a SSH session in your environment redis-cli -h redis.internal info. You should run it before you push all this new code to your repository.\nThis should give you a baseline of activity on your Redis installation. There should be very little memory allocated to the Redis cache.\nAfter you push this code, you should run the command and notice that allocated memory will start jumping.\n\nnote\nIf you use Domain Access and Redis, ensure that your Redis settings (particularly $conf['cache_backends'])\nare included before the Domain Access settings.inc file - see\nthis Drupal.org issue for more information.\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Drupal7", "title": "Using Composer Manager", "url": "/frameworks/drupal7/composer-manager.html", "documentId": "1e08f252272f2692f1f8b3741d036142d095d7d8", "text": "\n                        \n                            \n                                \n                                \n                                Composer manager\nDrupal 7 does not natively support installing packages via Composer.  Although there is a Drupal Composer project for Drupal 7, many projects are still built using a vanilla Drupal download or with Drush Make.  For sites built without Drupal Composer that still want to use modules that have Composer dependences, the most widely used option is the Composer Manager module.  Because of the read-only file system, however, there are specific configuration parameters necessary on Platform.sh.\n1. Install and patch Composer Manager\nInstall the Composer Manager module in the manner appropriate for your site.  There are also 2 patch files needed that have not been committed to the module yet.\nIf you're installing it via Drush Make, add the appropriate lines to your .make file:\nprojects[composer_manager][version] = \"1.8\"\nprojects[composer_manager][patch][] = \"https://www.drupal.org/files/issues/composer_manager-2620348-3.patch\"\nprojects[composer_manager][patch][] = \"https://www.drupal.org/files/issues/composer_manager-relative_realpath-2864297-5.patch\"\n\nIf you're checking the entire codebase into Git, do so with Composer Manager as well.  Then download and apply the two patches in the issues above and commit the result.\nIt's important to uncheck the two \"Automatically...\" options at the config page on admin/config/system/composer-manager/settings. If checked, Drupal tries to update the composer folder. Since this isn't a writable mount, installation of composer based modules will fail due to these writing permissions.\n2. Configure file locations\nComposer Manager works by using a Drush command to aggregate all module-provided composer.json files into a single file, which can then be installed via a normal Composer command.  Both the generated file and the resulting vendor directory must be in the application portion of the file system, that is, not in a writable file mount.  As that is not the default configuration for Composer Manager it will need to be changed.  Add the following lines to your settings.php file:\n$conf['composer_manager_vendor_dir'] = '../composer/vendor';\n$conf['composer_manager_file_dir'] = '../composer';\n\nThe above lines will direct Composer Manager to put the generated composer.json file into ../composer/, and to autoload Composer-based packages from the ../composer/vendor directory.  The paths are relative to the Drupal root.  You may use another location if desired provided that they are not in a writable file mount, and provided the vendor directory is a sibling of wherever the composer.json file will be.\nThen, manually create the composer directory and place a .gitignore file inside it, containing the following, and commit it to Git:\n# Exclude Composer dependencies.\n/vendor\n3. Update the build hook\nCreate a build hook in your .platform.app.yaml file that will install Composer dependencies:\nhooks:\n  build: |\n    # Install Composer dependencies.\n    cd composer\n    composer install --no-interaction --optimize-autoloader --no-dev\n\nReplace composer with whatever the path to the composer directory is. Note that if using the drupal build flavor with Drush Make, then the composer directory may have been moved into the same directory as your modules, like public/sites/default. It can be moved back via another line in the build hook:\nhooks:\n  build: |\n    # Move the Composer directory (back) to the application root.\n    mv public/sites/default/composer composer\n    # Install Composer dependencies.\n    cd composer\n    composer install --no-interaction --optimize-autoloader --no-dev\n\nThe composer install command may also be further customized as desired.\n4. Generate and commit the composer.* files locally\nFrom the Drupal root on your local system, run drush composer-json-rebuild to generate the aggregated composer.json file.  Then, change to the composer directory and run composer install yourself, to generate the composer.lock file and download all dependencies.\nThe composer.json and composer.lock files must be committed to the repository.  The vendor directory must not be.  That way, during build the Composer command in the build hook will download the exact version of all dependent packages listed in the composer.lock file, which helps keep builds consistent and predictable.\nAny time a new module with Composer dependencies is added, or a new version of a dependent library is available, repeat step 4 only.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Drupal7", "title": "Developing with Drupal", "url": "/frameworks/drupal7/developing-with-drupal.html", "documentId": "7d661b84d8582b5e4aca9dfa2f6224d17e37cf62", "text": "\n                        \n                            \n                                \n                                \n                                Developing with Drupal\nPush changes to an environment\nHere, we'll see how to make code changes to an environment.\n\nnote\nYou should never be working on the Master branch since it's supposed to be your production environment.\n\nMake sure you're on a working environment. In this example we're on the\nsprint1 branch:\n$ git checkout sprint1\n\nNow that you're set up on your working branch, you can start developing\non your website by making code changes and pushing those changes to\nPlatform to test them live.\nThere are three common ways you will be making code changes to Platform:\n\nAdd contributed modules, themes, distributions, third-party\nlibraries in the make file\nCreate custom code (modules, themes, profiles, libraries) and\ncommit them to your Platform codebase\nModify the services grid configuration\n\nAdd contributed projects\nEach time you push a commit, Platform.sh will rebuild your environment\nand run the Drush make command if a proper make file has been found.\nAdd a Drupal module\nEach Drupal module you want to install on your project should be\nincluded in the make file. For example, if you want to add Drupal\nCommerce, you need to add the following lines to your project.make:\n; Modules\nprojects[addressfield][version] = \"1.0-beta4\"\nprojects[addressfield][subdir] = \"contrib\"\n\nprojects[ctools][version] = \"1.3\"\nprojects[ctools][subdir] = \"contrib\"\n\nprojects[commerce][version] = \"1.8\"\nprojects[commerce][subdir] = \"contrib\"\n\nprojects[entity][version] = \"1.2\"\nprojects[entity][subdir] = \"contrib\"\n\nprojects[rules][version] = \"2.6\"\nprojects[rules][subdir] = \"contrib\"\n\nprojects[views][version] = \"3.7\"\nprojects[views][subdir] = \"contrib\"\n\nAdd a Drupal theme\nYou'd do the same if you want to add a theme. Add the following lines to\nyour project.make:\n; Zen Theme\nprojects[] = zen\n\nAdd a third-party library\nYou'd do the same if you want to add a third-party library. For our\nexample here, we're adding the HTML5 Boilerplate library. Add the\nfollowing lines to your project.make:\n; Libraries\nlibraries[html5bp][download][type] = \"file\"\nlibraries[html5bp][download][url] = \"http://github.com/h5bp/html5-boilerplate/zipball/v3.0.2stripped\"\n\nAdd custom code\nTo commit your custom modules, themes or libraries, you need to commit\nthem under a modules, themes or libraries folder at the root of\nyour Git repository.\n$ ls\n  libraries/\n  modules/\n  project.make\n  themes/\n\nWhen you push your code, Platform will build your environment and move\nyour modules, themes, libraries to the correct location on your site\n(usually sites/default/).\nChange the services configuration\nYou can change and define the topology of the services used in an\nenvironment, by modifying the configuration files.\nThis means that you're able to define and configure the services you\nwant to use.\nPush your changes\nWhen you're done, commit your changes to test them on your online\nenvironment.\n$ git add .\n$ git commit -m \"Made changes to my make file.\"\n$ git push\n\nYou will see that Platform has found a make file and is starting to\nrebuild your environment.\nWhen it's completed, you can see your changes on your site by clicking\nView this website under the name of Sprint1 environment on the\nPlatform.sh management console.\n\nnote\nThe Drush Make processing doesn't create any file in your Git repository. Your Git repository is the input of the process and not the output. You can see the directory structure that has been created by connecting via SSH to the environment. See the information in the Access information below the title of the environment\n\nMerge code changes to Master\nOnce you've got a branch with some changes, you'll want to be able to\npush those changes up to your live environment. Platform.sh has a great\nbutton called Merge that you can click on and it will push the\nappropriate changes to master.\n\nA dialog box will appear that will provide commands to execute future merges from the command line using the Platform.sh CLI.\n\nJust click on the \"Merge\" button again and all of the commits you made on your\nbranch will be merged into the master environment.\nSynchronizing data\nThe easiest way to do that is to use Drush and the sql-sync command.\nYou'll need to have Drush aliases for both your\nPlatform.sh site and your local site. If you are using the CLI and\nyou've run platform get [platform_id] for a project, then your Drush\naliases have already been set up.\nWith the Drush aliases (depending on how yours are set up), you\ncould use a command similar to this:\n$ drush sql-sync @platform.master @platform._local\n\nAn alternate method that is appropriate for larger databases is to use\nthe pipe | to stream the data, instead of making copies.\n$ drush @platform.master sql-dump | drush @platform._local sqlc\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Frameworks", "title": "Drupal 7", "url": "/frameworks/drupal7.html", "documentId": "09b97d3920a97924892daf94cb6557baddaa4408", "text": "\n                        \n                            \n                                \n                                \n                                Getting Started\nStructure your files\nPlatform.sh is very flexible and allows you to structure your files as you wish within your Git repository, and will build your project based on how your files are organized.\nHere are the three build modes that can be used:\n\nProfile: Platform.sh builds your project like Drupal.org does for distributions.\nProject: Platform.sh builds your make file using drush make. You don't need any Drupal core files nor any contributed modules, themes or libraries within your Git repository.\nVanilla: Platform.sh builds your project as it is in your Git repository. You can push all Drupal files and contributed modules, themes or libraries.\n\nProfile mode\nIf your repository contains a .profile file, Platform.sh builds your project in profile mode. This is similar to what Drupal.org does to build distributions. Everything you have in your repository will be copied to your profiles/[name] folder.\nThis build mode supports having a project.make file for your contributed modules, themes or libraries.\n\nnote\nWhen building as a profile, you need a make file for Drupal core called: project-core.make. See\ndrush make files\n\n.git/\nproject.make\nproject-core.make\nmy_profile.info\nmy_profile.install\nmy_profile.profile\nmodules/\n  features/\n    my_feature_01/\n    ...\n  custom/\n    my_custom_module/\n    ...\nthemes/\n  custom/\n    my_custom_theme/\n    ...\nlibraries/\n  custom/\n    my_custom_libraries/\n    ...\ntranslations/\n  ...\nProject mode\nIf your repository doesn't contain a .profile file, but contains a make file called: project.make (or even drupal-org.make), Platform.sh builds your project using Drush make. Everything you have in your repository will be copied to your sites/default folder.\n.git/\nproject.make\nmodules/\n  features/\n    my_feature_01/\n    ...\n  custom/\n    my_custom_module/\n    ...\nthemes/\n  custom/\n    my_custom_theme/\n    ...\nlibraries/\n  custom/\n    my_custom_libraries/\n    ...\ntranslations/\n  ...\nVanilla mode\nIn Vanilla mode, Platform.sh just takes your repository as-is without any additional reorganization.  This is the behavior when there is no .make or .profile file, or when the build mode is set to none or composer rather than to drupal.\nIt's best to keep your docroot separate from your repository root, as that allows you to store private files outside of the docroot when needed.  For example, your repository layout will likely resemble the following:\n.git/\nprivate/\nweb/\n  index.php\n  ... (other Drupal core files)\n  sites/\n    all/\n      modules/\n      themes/\n    default/\nIf you already have a Drupal 7 site built from a tar.gz download from Drupal.org, this is likely the best path forward.\nConfiguring Platform.sh for Drupal\nThe ideal .platform.app.yaml file will vary from project project, and you are free to customize yours as needed.  A recommended baseline Drupal 7 configuration is listed below, and can also be found in our Drupal 7 template project or Drupal 7 vanilla template project.\n\nnote\nYour database for Drupal must be named \"database\" in the relationships.\n\n# This file describes an application. You can have multiple applications\n# in the same project.\n#\n# See https://docs.platform.sh/user_guide/reference/platform-app-yaml.html\n\n# The name of this app. Must be unique within a project.\nname: 'app'\n\n# The runtime the application uses.\ntype: 'php:7.2'\n\n# Configuration of the build of this application.\nbuild:\n  flavor: drupal\n\n# The build-time dependencies of the app.\ndependencies:\n  php:\n    \"drush/drush\": \"^8.0\"\n\n# The relationships of the application with services or other applications.\n#\n# The left-hand side is the name of the relationship as it will be exposed\n# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand\n# side is in the form `:`.\nrelationships:\n    database: 'db:mysql'\n\n# The configuration of app when it is exposed to the web.\nweb:\n    # Specific parameters for different URL prefixes.\n    locations:\n        '/':\n            # The folder from which to serve static assets, for this location.\n            #\n            # This is a filesystem path, relative to the application root.\n            root: 'public'\n\n            # How long to allow static assets from this location to be cached.\n            #\n            # Can be a time in seconds, or -1 for no caching. Times can be\n            # suffixed with \"s\" (seconds), \"m\" (minutes), \"h\" (hours), \"d\"\n            # (days), \"w\" (weeks), \"M\" (months, as 30 days) or \"y\" (years, as\n            # 365 days).\n            expires: 5m\n\n            # Whether to forward disallowed and missing resources from this\n            # location to the application.\n            #\n            # Can be true, false or a URI path string.\n            passthru: '/index.php'\n\n            # Deny access to static files in this location.\n            allow: false\n\n            # Rules for specific URI patterns.\n            rules:\n                # Allow access to common static files.\n                '\\.(jpe?g|png|gif|svgz?|css|js|map|ico|bmp|eot|woff2?|otf|ttf)$':\n                    allow: true\n                '^/robots\\.txt$':\n                    allow: true\n                '^/sitemap\\.xml$':\n                    allow: true\n\n        '/sites/default/files':\n            # Allow access to all files in the public files directory.\n            allow: true\n            expires: 5m\n            passthru: '/index.php'\n            root: 'public/sites/default/files'\n\n            # Do not execute PHP scripts.\n            scripts: false\n\n            rules:\n                # Provide a longer TTL (2 weeks) for aggregated CSS and JS files.\n                '^/sites/default/files/(css|js)':\n                    expires: 2w\n\n# The size of the persistent disk of the application (in MB).\ndisk: 2048\n\n# The mounts that will be performed when the package is deployed.\nmounts:\n    '/public/sites/default/files':\n        source: local\n        source_path: 'files'\n    '/tmp':\n        source: local\n        source_path: 'tmp'\n    '/private':\n        source: local\n        source_path: 'private'\n    '/drush-backups':\n        source: local\n        source_path: 'drush-backups'\n\n# The hooks executed at various points in the lifecycle of the application.\nhooks:\n    # We run deploy hook after your application has been deployed and started.\n    deploy: |\n        set -e\n        cd public\n        drush -y updatedb\n\n    # The configuration of scheduled execution.\ncrons:\n    drupal:\n        spec: '*/20 * * * *'\n        cmd: 'cd public ; drush core-cron'\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Security", "title": "Updating", "url": "/security/updates.html", "documentId": "1b0a0ec1fc5b92ceccf4c3e70b529b53716c2eb7", "text": "\n                        \n                            \n                                \n                                \n                                Update all the things\nThe Platform.sh Rule: Update Early, Update Often\nPlatform.sh periodically updates its container images for the latest security updates from upstream providers.  (PHP versions, Ruby versions, MariaDB versions, etc.).  These do not always happen immediately but when a security vulnerability is identified and released it tends to be fairly soon after.\nHowever, these updates are not automatically propagated to individual projects as that would involve potential customer downtime.  Instead, the latest available version of every requested container is loaded on each deploy to a given environment.  After a deploy you are always guaranteed to be running the latest Platform.sh-provided version of a container.\nIf you have regular redeploys scheduled for Let's Encrypt SSL certificates then that will also ensure your container versions are up to date at the same time.  For that reason we recommend all customers setup the appropriate cron task to redeploy every two weeks or so.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Security", "title": "Security scans", "url": "/security/pen-test.html", "documentId": "aee84d72714dcd1c1253e5ecb8bd2e2582195d4c", "text": "\n                        \n                            \n                                \n                                \n                                Vulnerability Scanning and Penetration Testing\nPlatform.sh understands the need for application owners to ensure the integrity, and standards compliance, of their applications. Because there could be adverse impacts to other clients which would violate our terms of service, we only permit certain types of tests.\nApproved Activities\n\nVulnerability scanning of your web application. You are free to perform this as often as required without approval from Platform.sh.\nWeb application penetration tests that do not result in high network load.  You are free to perform this as often as required without approval from Platform.sh.\n\nApproved Activities by Prior Arrangement\n\nFor Platform.sh Enterprise-Dedicated customers we do permit infrastructure penetration testing (but not load testing) by prior arrangement.  This requires special advanced preparation. You must submit a support ticket request a minimum of three (3) weeks in advance for us to coordinate this on your behalf.\n\nProhibited Activities\n\nVulnerability scanning of web applications which you do not own.\nDenial of Service tests and any other type of testing which results in heavy network load.\nSocial engineering tests of Platform.sh services including falsely representing yourself as a Platform.sh employee.\nInfrastructure penetration tests for non-Dedicated-Enterprise customers. This includes SSH and database testing.\n\nRate Limits\n\nPlease limit scans to a maximum of 20 Mbps and 50 requests per second in order to prevent triggering denial of service bans.\n\nTroubleshooting\nIf your vulnerability scanning suggests there may be an issue with Platform.sh's service, please ensure your container is updated and retest. If the problem remains, please contact support.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Security", "title": "Protective block", "url": "/security/protective-block.html", "documentId": "3425c7a9ef1f72dd82f77753d5dde7f624eb3b1c", "text": "\n                        \n                            \n                                \n                                \n                                Protective block\nThe Platform.sh service has a protective blocking feature that, under certain circumstances, restricts access to web sites with security vulnerabilities. We use this partial blocking method to prevent exploitation of known security vulnerabilities.\nThe protective block is meant for high impact, low complexity attacks.\nThe Platform.sh security block\nOutdated software often contains known vulnerabilities that can be exploited from the Internet. Sites that can be exploited are protected by Platform.sh. The system partially blocks access to these sites.\nHow the protective block works\nPlatform.sh maintains a database of signatures of known security vulnerabilities in open-source software that are commonly deployed on our infrastructure. The security check only analyze known vulnerabilities in open-source projects like Drupal, Symfony or WordPress. It cannot examine customizations written by Platform.sh customers.\nWe analyze the code of your application:\n\nWhen you push new code to Git\nRegularly when new vulnerabilities are added to our database\n\nIf a vulnerability deemed as critical is detected in your application, Platform.sh is going to reject the Git push.\nWe run two types of blocks:\nFor production websites, we run a \"partial block\" that allows the site to stay mostly online. Depending on the nature of the vulnerability, parts of a request, such as a query string, cookies or any additional headers, may be removed from GET requests. All other requests may be blocked entirely - this could apply to logging in, form submission or product checkout.\nFor development websites, we run complete blocks, and the error message gives you detailed information about the vulnerability.\nUnblocking is automated upon resolution of the security risk. The block is removed soon after a customer applies a security upgrade and removes the vulnerability.\nOpting out of the protective block\nThe protective block is there to protect you against known vulnerabilities in the software you deploy on Platform.sh.\nIf nonetheless you want to opt out of the protective block, you simply need to specify it in your .platform.app.yaml like this:\npreflight:\n   enabled: false\n\nYou can also explicitly opt-out of some specific check like this:\npreflight:\n   enabled: true\n   ignore_rules: [ \"drupal:SA-CORE-2014-005\" ]\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Frameworks", "title": "Deploy on Platform.sh", "url": "/frameworks/deploy-button.html", "documentId": "90ef0512cfc7bd6f477f11f52f99d560a6fc8077", "text": "\n                        \n                            \n                                \n                                \n                                Deploy on Platform.sh\nPlatform.sh offers a number of project templates as part of the Project Creation Wizard to make bootstrapping a new project easy.  However, you can also create arbitrary links to spawn projects on Platform.sh from an arbitrary Git repository or prepared template.\nThere are two ways to create such a link, shown below.  In each case, when a user clicks on the link they will be redirected to create a new Platform.sh project, with the template selection step skipped in favor of the template specified.  If the user does not have a Platform.sh account yet they will be prompted to create one.\nYou may include the link on your own project's website, your company's internal Wiki, or anywhere else a link can go to make launching your code base as simple as possible.\nPreparation\nTo have a deployable template, you need to first prepare the repository.  The Deploy on Platform.sh button will work with any Git repository that is deployable on Platform.sh; that is, it has the necessary .platform.app.yaml, .platform/services.yaml, and .platform/routes.yaml files in place.  See the appropriate documentation for how to define those files.\nThe repository must be available at a publicly-accessible Git URL.  That may be hosted with GitHub, GitLab, Bitbucket, your own custom Git hosting, or any other publicly-accessible Git URL.\n(Optional) Make a template definition file\nYou can create a Deploy on Platform.sh button for any compatible repository; however, you can also provide a YAML template definition file.  A template definition file is a YAML file that references a Git repository but can also include additional information, such as limiting the resulting project to a certain minimum project size or only allowing it to be deployed in certain regions.  Use this mechanism when you want more control over how the template gets deployed.\nThe template definition file may be at any publicly-accessible URL.  It can be in the template repository itself or separate.  Note that if it is in the template repository then it will be included in every deployed user project from that template.  (It won't hurt anything as it has no effect at runtime, but users will have a copy of the file in their code base and may be confused by it.)\nA list of all Platform.sh-supported templates is available on GitHub.  3rd party templates are also available.  You can also create your own template file and host it anywhere you wish.\nThe template definition file's format is documented in the 3rd party template repository.\nMaking a button (The easy way)\nThe easiest way to make a Deploy on Platform.sh button is to use our button builder widget.   You provide it with either the Git URL of the repository or a URL to a corresponding template definition file.\nThe button builder widget will give you an HTML fragment to copy and paste to wherever you want the button hosted.  It will also include a tracking code so we can whose Deploy on Platform.sh button was clicked, but does not add any cookies to the site.\nMaking a button manually\nArbitrary Git repository\nCreate a link in the following form:\nhttps://console.platform.sh/projects/create-project?template=GIT_URL\nWhere GIT_URL is the URL of a publicly-visible Git repository.  For example, to install Platform.sh's Drupal 8 template on GitHub you would use:\nhttps://console.platform.sh/projects/create-project/?template=https://github.com/platformsh-templates/drupal8.git\n(Note that is the URL of the Git repository as if you were cloning it, NOT the URL of the repository's home page on GitHub.)\nA new project will be created and then initialized with whatever code is at the tip of the master branch of that repository.  This method will work for any publicly-visible Git repository, provided that it includes the necessary Platform.sh YAML configuration files.  If those are missing the project will still initialize but fail to build.\nDefined Template\nCreate a link in the following form:\nhttps://console.platform.sh/projects/create-project?template=TEMPLATE_URL\nWhere TEMPLATE_URL is the URL of a publicly-visible template definition file.  For example, to install Platform.sh's Drupal 8 template you would use:\nhttps://console.platform.sh/projects/create-project/?template=https://github.com/platformsh/template-builder/blob/master/templates/drupal8/.platform.template.yaml\nA new project will be created, initialized with whatever code is at the tip of the master branch of the repository referenced by that file, provided that it includes the necessary Platform.sh YAML configuration files.  If those are missing the project will still initialize but fail to build.\nListing a repository\nPlatform.sh welcomes project templates produced by the application vendor.  If you have a Free Software application you want available in the Platform.sh setup wizard, create a template definition file and submit a pull request against the 3rd party templates repository.  The Developer Relations team will review and evaluate the application and template, and may offer feedback before merging.  Generally speaking, we welcome any Free Software application that is actively maintained and runs well on Platform.sh.  Projects released under a non-Free license will not be accepted.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Security", "title": "Compliance guidance", "url": "/security/compliance-guidance.html", "documentId": "a666ab452a06638f5c995a114971b8d1665a10a6", "text": "\n                        \n                            \n                                \n                                \n                                Compliance guidance\nPlatform.sh has many PCI and SOC 2 certified customers using our services. Some requirements are the responsibility of the host and others are the responsibility of the application developer.\nBasic compliance questions can be handled by our support team via a ticket. For more advanced questions or walk-through of a full audit please contact your Platform.sh Account Manager.\nOverview\nPlatform.sh provides a Platform as a Service (PaaS) solution which our customers may use for applications requiring PCI compliance. \n\nnote\nCardholder processing activity is discouraged. Please use a third party processor.\n\nSecurity &amp; Compensating Controls\n\nFor a list of security measures, please see our Security page.\n\nPlease take note that customer environments are deployed in a read-only instance, segregated with GRE and IPSEC tunnels, which often permits compensating controls to be claimed for several PCI requirements.\n\nBecause customers can use our PaaS in a variety of ways, the best approach with auditors is to focus is on \u201cWhat do I, the customer, control/configure and how is it managed in a compliant manner?\u201d\n\n\nResponsibility\nPlatform.sh and customers have shared responsibility for ensuring an up to date and secure system.  Compliance is ultimately the responsibility of the customer, however.\nPlatform.sh is responsible for:\n\nPhysical and Environmental controls - We use third party hosting and thus these requirements are passed through to those providers (e.g. AWS).\nPatch Management - Platform.sh is responsible for patching and fixing underlying system software, management software, and environment images.\nConfiguration Management - Platform.sh maintains the configuration of its infrastructure and devices.\nAwareness and Training - Platform.sh trains its own employees in secure software development and management.\nCapacity Management - Platform.sh is responsible for capacity management of the infrastructure, such as server allocation and bandwidth management.\nAccess Control - Platform.sh is responsible for providing access control mechanisms to customers and for vetting all Platform.sh personnel access.\nBackups - Platform.sh is responsible for backing up the infrastructure and management components of the system.  On Platform.sh Dedicated Enterprise (only), Platform.sh will also backup application code and databases on behalf of customers.\n\nCustomers are responsible for:\n\nPatch Management - Customers are responsible for maintaining and patching application code uploaded to Platform.sh, either written by them or by a third party.\nConfiguration Management - Customers are responsible for the secure configuration of their application, including Platform.sh configuration and routes managed through YAML files.\nAwareness and Training - Customers are responsible for training their own employees and users in secure software practices.\nCapacity Management - Customers are responsible for ensuring their application containers have sufficient resources for their selected tasks.\nAccess Control - Customers are responsible for effectively leveraging available access control mechanisms, including proper access control settings, secrets management, ssh keys management, and the use of two-factor authentication.\nBackups - On Platform.sh Professional customers are responsible for all application and database backups.\n\nThe following sections provide guidance on shared responsibilities to achieve PCI DSS compliance. The list below covers all the sections found in a typical SAQ D although you may not need this level of detail for your efforts. This section is currently using PCI DSS 3.2 as a reference.\nRequirement 1\n\n\n\nRequirement\nResponsibility\nComments\n\n\n\n\n1.1 to 1.3.3\nPlatform\nInfrastructure cannot be modified by the customer\n\n\n1.3.4\nShared\nPlatform is responsible for the infrastructure. Customer is responsible for correct routes.yaml configuration\n\n\n1.3.5 to 1.3.6\nPlatform\nInfrastructure cannot be modified by the customer\n\n\n1.3.7\nShared\nCustomers are responsible for ensuring their application does not leak private IP addresses\n\n\n1.4 to 1.5\nShared\nCustomers are responsible for their devices accessing the environment\n\n\n\nRequirement 2\n\n\n\nRequirement\nResponsibility\nComments\n\n\n\n\n2.1\nShared\nCustomer is responsible to change vendor-supplied defaults in their app\n\n\n2.1.1\nN/A\nThere are no wireless environments connected to the CDE\n\n\n2.2 to 2.2.1\nPlatform\nInfrastructure cannot be modified by the customer\n\n\n2.2.2 to 2.2.5\nShared\nCustomers must ensure their app is configured to only use the necessary services (services.yaml), routes (routes.yaml), and apps (app.yaml)\n\n\n2.3\nShared\nCustomers are responsible for admin access to their app\n\n\n2.4\nShared\nCustomer is responsible for application and data flows. Platform maintains a data flow chart and network diagram for the infrastructure\n\n\n2.5\nShared\nCustomers are responsible for their app\n\n\n2.6\nPlatform\nYes we do\n\n\n\nRequirement 3\n\n\n\nRequirement\nResponsibility\nComments\n\n\n\n\n3.1\nShared\nPlatform.sh is responsible that storage is securely deleted. Customer is responsible for all other provisions.\n\n\n3.2 to 3.4.1\nCustomer\nCustomer responsibility. Customer is also responsible for sanitizing cardholder data copied to non-production sources.\n\n\n3.5 to 3.7\nShared\nCustomer is responsible for data encryption and non-infrastructure key management. We recommend column-level database encryption. If you need data at rest protection, please see our docs about Encryption\n\n\n\nRequirement 4\n\n\n\nRequirement\nResponsibility\nComments\n\n\n\n\nAll\nCustomer\nCustomer is responsible for internet transmission of cardholder data. See also our docs about Encryption\n\n\n\nRequirement 5\n\n\n\nRequirement\nResponsibility\nComments\n\n\n\n\nAll\nShared\nCustomer is responsible for their systems.\n\n\n\nRequirement 6\n\n\n\nRequirement\nResponsibility\nComments\n\n\n\n\nAll\nShared\nCustomer is responsible for their application. Platform.sh is responsible for the infrastructure.\n\n\n\nRequirement 7\n\n\n\nRequirement\nResponsibility\nComments\n\n\n\n\nAll\nShared\nCustomer is responsible for their application and also their project access control. Platform.sh is responsible for the infrastructure.\n\n\n\nRequirement 8\n\n\n\nRequirement\nResponsibility\nComments\n\n\n\n\n8.1 to 8.5\nShared\nCustomer is responsible for their application. Platform.sh is responsible for the infrastructure.\n\n\n8.5.1\nN/A\nThere is no remote access.\n\n\n8.6 to 8.8\nShared\nCustomer is responsible for their application. Platform.sh is responsible for the infrastructure.\n\n\n\nRequirement 9\n\n\n\nRequirement\nResponsibility\nComments\n\n\n\n\nAll\nN/A\nPlatform uses PCI compliant cloud hosting providers\n\n\n\nRequirement 10\n\n\n\nRequirement\nResponsibility\nComments\n\n\n\n\n10.1 to 10.7\nShared\nCustomer is responsible for their application. Platform.sh is responsible for the infrastructure.\n\n\n10.8\nPlatform\nService provider only requirement\n\n\n10.9\nShared\nCustomer is responsible for their application. Platform.sh is responsible for the infrastructure.\n\n\n\nRequirement 11\n\n\n\nRequirement\nResponsibility\nComments\n\n\n\n\n11.1 to 11.1.2\nN/A\nThere are no wireless APs in the Platform.sh hosting providers\n\n\n11.2 to 11.3.4\nShared\nCustomer is responsible for their application. Platform.sh is responsible for the infrastructure.\n\n\n11.3.4.1\nPlatform\nService provider only requirement\n\n\n11.4 to 11.6\nShared\nCustomer is responsible for their application. Platform.sh is responsible for the infrastructure.\n\n\n\nRequirement 12\n\n\n\nRequirement\nResponsibility\nComments\n\n\n\n\n12.1 to 12.4\nShared\nCustomer is responsible for their organization. Platform.sh is responsible for its organization.\n\n\n12.4.1\nPlatform\nService provider only requirement\n\n\n12.5 to 12.8.5\nShared\nCustomer is responsible for their organization. Platform.sh is responsible for its organization.\n\n\n12.9\nPlatform\nService provider only requirement\n\n\n12.10 to 12.10.6\nShared\nCustomer is responsible for their organization. Platform.sh is responsible for its organization.\n\n\n12.11\nPlatform\nService provider only requirement\n\n\n\nAppendix A1\n\n\n\nRequirement\nResponsibility\nComments\n\n\n\n\nAll\nPlatform\nService provider only requirement\n\n\n\nAppendix A2\n\n\n\nRequirement\nResponsibility\nComments\n\n\n\n\nAll\nCustomer\nThis is only applicable if the customer uses POS terminals accessing their environment, otherwise it is likely considered N/A\n\n\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Security", "title": "Encryption", "url": "/security/encryption.html", "documentId": "77f16198dc50422af77937052362ed4fdc742e74", "text": "\n                        \n                            \n                                \n                                \n                                Encryption\nData in Transit\nData in transit between the World and Platform.sh is always encrypted as all of the sites and tools which Platform.sh supports and maintains require TLS or SSH to access. This includes the Platform.sh management console, Accounts site, Git repositories, Documentation, and Helpdesk.\nData in transit between the World and customer applications is encrypted by default.  Only SSH and HTTPS connections are generally accepted, with HTTP request redirected to HTTPS.  Users may opt-out of that redirect and accept HTTP requests via routes.yaml configuration, although that is not recommended.  By default HTTPS connections use an automatically generated Let's Encrypt certificate or users may provide their own TLS certificate.\nData in transit on Platform.sh controlled networks (eg. between the application and a database) may or may not be encrypted, but is nonetheless protected by private networking rules.\nData at Rest\nAll application data is encrypted at rest by default using encrypted ephemeral storage (typically using an AES-256 block cipher). Some Enterprise-Dedicated clusters do not have full encryption at rest.\nIf you have specific audit requirements surrounding data at rest encryption please contact us.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Security", "title": "Backup and restore", "url": "/security/backups.html", "documentId": "38abe1ef2b19808b4cbddcad4e2317ff733f025c", "text": "\n                        \n                            \n                                \n                                \n                                Backup and restore\nBackups vary by our offering and their retention is governed by our data retention policies.\nBelow are details on our Recovery Point Objective (RPO) and Recovery Time Objective (RTO) for our Platform.sh Professional and Platform.sh Enterprise offerings.\nPlatform.sh Professional\nWith Platform.sh Professional, we have enabled our clients to manage their own backup and restore functions. Please see the backup and restore page for details.\nRPO: User defined. The RPO is configured by our clients.\nRTO: Variable. Recovery time will depend upon the size of the data we are recovering\nPlatform.sh Enterprise-Grid and Enterprise-Dedicated\nRPO: 6 hours.  Platform.sh takes a backup of Platform.sh Enterprise environments every 6 hours.\nRTO: Variable. Recovery time will depend upon the size of the data we are recovering\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Security", "title": "Data deletion", "url": "/security/data-deletion.html", "documentId": "a3d4942e1e35f4eca35ccdd0f365e163ecbe1165", "text": "\n                        \n                            \n                                \n                                \n                                Data deletion\nData deletion is handled via our backend providers. When a volume is released back to the provider, the provider will perform a wipe on the data utilizing either NIST 800-88 or DoD 5220.22-M depending upon the offering. This wipe is done immediately before reuse.\nAll projects, except those hosted on Orange Cloud for Business, utilize encrypted volumes. The encryption key is destroyed when we release the volume back to the provider, adding another layer of protection.\nMedia destruction\nMedia destruction is handled via our backend providers. When the provider decommissions media it undergoes destruction as outlined in NIST 800-88.\nData subject removal\nData subject deletion requests where Platform is the controller are handled via a support ticket. For contracts designating Platform as the processor, deletion requests should be sent to the controller and we will forward any that we receive.\nOur product is a Platform as a Service. Platform does not directly edit customer data to ensure data confidentiality, security, and integrity. All data deletion requests for customer data must be handled by the concerned data controller.\nResources\n\nAWS Security Whitepaper\nAzure Data Retention\nGoogle Cloud Platform Compliance Information\nInteroute Compliance Information\nOrange Cloud for Business certifications\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Security", "title": "GDPR", "url": "/security/gdpr.html", "documentId": "fcb9c41c0a244b5726ae02a27aa1df78580194aa", "text": "\n                        \n                            \n                                \n                                \n                                GDPR Overview page\nPlatform.sh has taken numerous steps to ensure GDPR compliance.\nAs part of our measures we have implemented the following:\n\nData Protection Officer: Appointment of a Security Officer who also holds the Data Protection Officer (DPO) role.\nData Breach Policy: We have updated our data breach policy and procedures and have reviewed that all our suppliers are compliant with breach notifications.\nConsent: We've confirmed that all of our customer communication, both business-related and marketing-related, is opt-in and no information is shared with us without a customer's consent.\nData Governance: We have internally audited all of our suppliers on their internal security and their GDPR compliance status and can confirm that our in-scope suppliers are GDPR compliant.\nData Protection by design: We've implemented policies in the company to ensure all of our employees follow the necessary training and protocols around security. In addition, privacy protection is part of every project during instantiation.\nEnhanced Rights: The GDPR provides rights to individuals such as the right to portability, right of rectification, and the right to be forgotten.  We've made sure we comply with these rights. Nearly all information can be edited through a user's account, and we can delete accounts upon request.\nPersonally identifiable information (PII): We've audited our systems to confirm that we encrypt and protect your personal data.\nData Flows: We have identified data, mapped the high level data flow, and mapped data shared with vendors - including cross-border transfers.\nPIA: We have performed an internal Privacy Impact Analysis (PIA) using the CNIL's PIA Software to ensure we comply with the GDPR.\nSecurity: We have created https://platform.sh/security to document our security features.\nData Collection: We've documented information about what data we collect.\nData Retention: We documented information about our data retention.\nDPA: We have revised our Terms of Service and Privacy Policy to align with the GDPR and we offer a pre-signed DPA agreement that can be downloaded at the top of the Privacy Policy\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Web", "title": "Deleting a project", "url": "/administration/web/delete.html", "documentId": "0b77c12a62bb77800f8ec71dc74869ffa575199e", "text": "\n                        \n                            \n                                \n                                \n                                Delete a project\nTo delete a Platform.sh project, including all data, code, and active environments:\n\nGo to your Account Settings, logging in if necessary, via the link in the top-right corner of these docs, or via this link: https://accounts.platform.sh/user.\nLocate the project you wish to delete in the project list.\nHover the gear icon on the project and select \"Delete\".\nConfirm your intent to delete the project.\n\nYou will only be billed for the portion of a month during which the project was active.  If you delete a project part way through the month the cost of the project will be prorated accordingly.\nA user account with no projects associated with it will have no charges.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Security", "title": "Data retention", "url": "/security/data-retention.html", "documentId": "c304c8342c76d2792aae55fb2f58ad27f4368f86", "text": "\n                        \n                            \n                                \n                                \n                                Data retention\nPlatform.sh logs and stores all sorts of data as a normal part of its business.  This information is retained as needed for business purposes and old data is purged.  The retention time varies depending on the type of data stored.\nAccount information\nInformation relating to customer accounts (login information, billing information, etc.) is retained for as long as the account is active with Platform.sh.\nCustomers may request that their account be deleted and all related data be purged by filing an issue ticket.\nSystem logs\nSystem level access and security logs are maintained by Platform.sh for diagnostic purposes.  These logs are not customer-accessible.  These logs are retained for at least 6 months and at most 1 year.\nGeneral system level logs are retained for at least 30 days and at most 1 year.\nPayment processing logs\nLogs related to payment processing are retained for at least 3 months and at most 1 year.  This is consistent with PCI recommendations.\nApplication logs\nApplication logs on each customer environment are retained with the environment.  Individual log files are truncated at 100 MB, regardless of their age.  See the accessing logs page for instructions on how to access them.\nWhen an environment is deleted its application logs are deleted as well.\nGrid Backups\nApplication backups running on the Grid (e.g. If you subscribe to a Platform.sh Professional plan) are retained for at least 7 days.  They will be purged between 7 days and 6 months, at Platform.sh's discretion.\nDedicated backups\nBackups for applications running on a Dedicated instance will follow the schedule documented on our dedicated backups page.\nTombstone backups\nWhen a project is deleted Platform.sh takes a final backup of active environments as well as the Git repository holding user code.  This final backup is to allow Platform.sh to recover a recently-deleted project in case of accident.\nThese \"tombstone\" backups are retained for between 7 days and 6 months.\nAnalytics\nPlatform.sh uses Google Analytics on various web pages, and therefore Google Analytics will store collected data for a period of time.  We have configured our Google Analytics account to store data for 14 months from the time you last accessed our site, which is the minimum Google allows.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Security", "title": "Data collection", "url": "/security/data-collection.html", "documentId": "7d53e071b5862869fc8b8424f28796439d21f38d", "text": "\n                        \n                            \n                                \n                                \n                                Data collection\nAs part of our normal business operations we do collect various pieces of data.\nIn GDPR terms:\n\nArticle 4: Our accounts system contains some (routine) Article 4 items (name, address, phone, etc.) in order to allow us to bill your account appropriately.  This information can be verified, changed, and deleted by logging into your account.\nArticle 9: We don't capture and store any Article 9 special identifiers (such as race, religion, sexual orientation, or other attributes that are irrelevant to our business). \nArticle 30: The only Article 30 items we keep are IP address and Log files. These reside on AWS/Azure/Orange (depending on your hosting), and may be sent to Sentry.io when there are crashes.\n\nApplication logs\nApplication logs are those generated by the host application or application server (such as PHP-FPM).  They are immutable to Customers to prevent tampering. These logs are secured behind key-based SSH so that only the Customer and our relevant teams have access.\nSystem logs\nPlatform.sh records routine system logs.  We do not access Customer-specific system logs or the customer environment unless requested to do so to help solve a problem.\nIn the future, we will be rolling out better log segregation to allow a Customer to get easier access to their own logs for diagnostic purposes.\nAccess logs\nThere are two main types of access logs: web and SSH.\nWeb access logs\nApplication access logs are immutable to Customers to prevent tampering. These logs are secured behind key-based SSH so that only the Customer and our relevant teams have access.\nSSH access logs\nSSH access logs are securely stored in our infrastructure and not accessible to customers.  They can be accessed by Platform.sh support personnel as part of an audit if requested.\nAccess by customers and Platform.sh support personnel to customer environments is logged.  However, we only log the connection itself, not what was done during the session, as that would be a violation of customer privacy.\nVendor data sharing\nWe have identified and mapped all data we collect and share with vendors (such as AWS, Azure, and Orange). We know what we capture and where it goes. All of our vendors have been vetted for security and GDPR compliance.  We have enacted contract amendments and Data Processing Agreements (DPAs) where applicable.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Web", "title": "Adding vouchers", "url": "/administration/web/vouchers.html", "documentId": "192348fe53da6709a2da5fa2520c26cb3976e8ca", "text": "\n                        \n                            \n                                \n                                \n                                Vouchers\nApplying a voucher to your project\nIf you receive a Platform.sh voucher code, you can redeem it as follows:\n\nGo to your Account Settings, logging in if necessary, via the link in the top-right corner of these docs, or via this link: https://accounts.platform.sh/user.\nIn the left navigation click on \"VOUCHERS\"\nOn the page click on \"Add a voucher code\"\nEnter the code and click on the \"ADD CODE\" button\nEt voil\u00e0! Your account will now be credited with additional dollars pounds or euros. \n\nIf you are assessing Platform.sh for your organization and think that you could benefit from a little more oomf in your test project why not contact us to request a voucher? You can tell us more at: https://platform.sh/contact/\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Web", "title": "Configure environments", "url": "/administration/web/configure-environment.html", "documentId": "2f4c3d39e15386bf43972ab06a6981f00f50f85c", "text": "\n                        \n                            \n                                \n                                \n                                Environment configuration\nYou can access an environment's settings by selecting that environment from the Select Environments pull-down menu at the top of the page or by clicking that environment within the Environments graphic on the right side. Click the Settings tab at the top of the screen.\nGeneral\nThe General screen allows you to extend the behavior of a specific environment.\n\nEnvironment name\nThe first setting allows you to modify the name of the environment and view its parent environment.\nStatus\nFrom the Status tab, you can activate or deactivate an environment.\n\nThe Deactivate &amp; Delete Data action will\n\nDeactivate the environment. Unless is is re-activated, it will no longer deploy and it will not be accessible from the web or via SSH.\nDestroy all services running on this environment.\nDelete all data specific to the environment. If the environment is reactivated, it will sync data from its parent environment.\n\nOnce the environment is deactivated, the Git branch will remain on Platform.sh in the <a href=\"../../GLOSSARY.html#inactive-environment\" class=\"glossary-term\" title=\"An environment which is not deployed. You can activate an inactive environment from the\nenvironment configuration page on the Platform.sh management console.\">inactive environment. To delete the branch as well, you need to execute the following:\ngit push origin :BRANCH-NAME\n\nnote\nDeleting the Master environment is forbidden.\n\nOutgoing emails\nFrom this tab, you can allow your application to send emails via a SendGrid SMTP proxy.\n\nChanging this setting will temporarily list the environment's status as \"Building\", as the project re-builds with the new setting. Once it has re-deployed, it will appear once again as \"Active\" in your settings.\nSearch engine visibility\nFrom this tab, you can tell search engines to ignore the site entirely, even if it is publicly visible.\n\nX-Robots-Tag\nBy default, Platform.sh includes an additional X-Robots-Tag header on all non-production environments:\nX-Robots-Tag: noindex, nofollow\nThat tells search engines to not index sites on non-production environments entirely nor traverse links from those sites, even if they are publicly visible.  That keeps non-production sites out of search engine indexes that would dilute the SEO of the production site.  To disable that feature for a non-production environment, use the Platform.sh CLI command below:\nplatform environment:info restrict_robots false\nOr to disable it for a specific environment other than the one that is currently checked out, execute the following:\nplatform environment:info -e ENVNAME restrict_robots false\nwhere ENVNAME is the name of the environment.\nOn a production instance (the master branch, after a domain has been assigned) the search-blocker is disabled and your application can serve a robots.txt file as normal.  However, you must ensure that the file is in your project's web root (the directory where the / location maps to) and your application is configured to serve it.  See the location section in .platform.app.yaml.\nHTTP access control\nYou should not expose your development environments to the whole wide world. Platform.sh allows you to simply implement access control, either by login/password (the equivalent to .htaccess) or by filtering IP addresses or a network using the CIDR format.  That is, 4.5.6.7 and 4.5.6.0/8 are both legal formats.\n\nnote\nChanging access control will trigger a new deploy of the current environment. However, the changes will not propagate to child environments until they are manually redeployed.\n\nThese settings get inherited by branches below the one you are on. That means if you create a staging environment, and you create branches from this one, they will all inherit the same authentication information and you only have to set-it up once.\nYou can also setup authentication with the CLI using the following command platform environment:http-access which also allows you to read the current setup. This eases the integration of CI jobs with Platform.sh as you will not need to hardcode the values in the CI.\nYou can allow or deny access to specific IPs or IP ranges. First switch the access control section to ON. Then add one or more IPs or CIDR IP masks, followed by allow or deny. See the example below. Note that allow entries should come before deny entries in case both of them would match.\n\nFor example, the following configuration will only allow the 1.2.3.4 IP to access your website.\n1.2.3.4/32 allow\n0.0.0.0/0 deny\nAccess\nThe Access screen allows you to manage the users' access on your project.\nYou can invite new users to a specific environment by clicking the Add button and entering their email address, or modify permissions of existing users by clicking the Edit link when hovering the user.\n\n\nnote\nCurrently, permission changes that grant or revoke SSH access to an environment take effect only after the next time that environment is deployed.\n\nSelecting a user will allow you to either edit or remove access to that environment.\nYou can also manage access to users on multiple environments using the project configuration screen.\nVariables\nThe Variables screen allows you to define the variables that will be available on a specific environment.\n\nRoutes\nThe Routes screen describes the configuration features that define the routes of your application. Routes cannot be edited here, but it provides a simple routes configuration example for your project's .platform/routes.yaml file.\n\nConsult the documentation for more information about properly configuring Routes for your project.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Web", "title": "Environments", "url": "/administration/web/environments.html", "documentId": "7a9a2736b8ec60515e63e9b870796ceeb954eb07", "text": "\n                        \n                            \n                                \n                                \n                                Platform.sh Environments\nPlatform.sh helps a coder with the development workflow by making it easy to manage multiple environments, including the Master environment which runs the production website. It's precisely like a \"development\" or a \"staging\" server, except they are created on the fly, and they are absolutely identical copies of their parent environments.\nAn environment is tied to a Git branch, plus all the services that are serving that branch. You can see that as a complete working website. With Bitbucket and GitHub integrations you can even get a \"development server\" for each and every pull request.\nYou can have branches that are not tied to a running instance of your application; these are what we call \"inactive environments\".\nMaster environment\nEvery Platform.sh project starts with a Master environment which corresponds to the Master branch in Git.\nIf you subscribed to a production plan, this environment is your live site and can be mapped to a domain name and a custom SSL certificate.\n\nnote\nYour project must have a master branch: it will not function properly without one.\n\nHierarchy\n\nPlatform.sh brings the concept of a hierarchy between your environments. Each new environment you create is considered a child of the parent environment from which it was branched.\nEach child environment can sync code and/or data down from its parent, and merge code up to its parent. These are used for development, staging, and testing.\nWhen you create a branch or child environment through the Platform.sh management console the branch it was made from will be treated as the parent.  If you create a branch through your local Git checkout and push it to Platform.sh, or synchronize a branch from a 3rd party such as GitHub or Bitbucket, its parent will default to the master branch.\nAny environment's parent can be changed using the Platform.sh CLI with the following command:\nplatform environment:info parent NEW_PARENT\n\nIn this case, the current environment (the branch you're on) will be set to have NEW_PARENT as its parent environment.  The environment to reparent can be set explicitly with the -e option:\nplatform environment:info -e feature-x parent NEW_PARENT\n\nWorkflows\nSince you can organize your environments as you want, you have complete flexibility to create your own workflows.\nThere are no rules you must follow when branching the master environment. You simply need a structure that best fits your workflow:\n\nAgile: a child environment per sprint. Each story in the sprint can have its own environment as a child of the sprint environment.\nDeveloper-centric: one QA environment and a few development environments (per developer, per task...).\nTesting: an operational test environment, a user test environment and a few unit test environments.\nHotfix: one environment for every bug, security, or hotfix that needs deployment.\n\nHere is an example of a possible Agile workflow.\n\nThe administrator creates a Sprint environment and gives each of the developers permission to create new feature environments. Another approach is that the administrator could create an environment for each developer.\n\nAs a feature is completed, the administrator can review the work by accessing the website of the feature environment. The new feature is then merged back into the Sprint environment.\n\n\nThe remaining features will sync with the Sprint environment to ensure their working environment is up-to-date with the latest code.\n\n\nWhen the objectives of the sprint are complete, the administrator can then make a backup of the live site, then merge the Sprint environment into the live (Master) environment.\n\nThe administrator can then synchronize the next sprint's environment with data from the live (Master) environment to repeat and continue the development process.\nNaming conventions\nPlatform.sh provides great flexibility on the way you can organize and work with your development environments. To improve readability and productivity, it's important to think carefully about how to name and structure those environments.\nThe name should represent the purpose of the environment. Is it a Staging site to show to your client? Is it an implementation of a new feature? Is it a hotfix?\nIf you use Agile, for example, you could create hierarchical environments and name them like this:\nSprint1\n  Feature1\n   Feature2\n   Feature3\nSprint2\n  Feature1\n  Feature2\n  ...\n\nIf you prefer splitting your environments per developer and having a specific environment per task or per ticket, you could use something like this:\nStaging\n  Developer1\n    Ticket-526\n    Ticket-593\n  Developer2\n    Ticket-395\n  ...\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Security", "title": "SCA", "url": "/security/sca.html", "documentId": "aaa5f544ee5ff3e24c9f6584911599d871e7d780", "text": "\n                        \n                            \n                                \n                                \n                                Strong Customer Authentication (SCA)\nIn accordance with Article 14(1) of the Commission Delegated Regulation (EU) 2018/389, Platform.sh has made changes in order to comply with and implement Strong Customer Authentication (SCA) for customers using payment methods from the EU. That article states:\n\nPayment service providers shall apply strong customer authentication when a payer creates, amends, or initiates for the first time, a series of recurring transactions with the same amount and with the same payee.\n\nSCA is part of the second Payment Services Directive (PSD2), acting as a regulatory requirement to reduce fraud and to make online transactions more secure.\nThe law went into affect September 14, 2019, and European card holders will be required to go through an additional re-authentication step within the Platform.sh management console starting October 1, 2019 in order to authenticate recurring payments with your payment institution.\nPrior to October 1, 2019, Platform.sh projects associated with an EU credit card will see a banner at the top of the management console.\n\nThat banner will direct you to authenticate your payment information settings.\n\nThe process after you click \"Authenticate\" will vary according to your payment institution. In most cases, having your phone number registered with that institution will be sufficient to enable 2FA with them from here.\nAfter October 1, 2019, the SCA banner in the management console will become a warning for you to update your payment settings.\n\nThere will be a grace period in the first few weeks following that change, but when that period has ended projects that have not set up payment authentication through the console will be suspended, so it is important for you to do so as soon as possible.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Integrations", "title": "Tideways", "url": "/administration/integrations/tideways.html", "documentId": "b031442f6327e692ec3d457ca8072a3ec2c3c20b", "text": "\n                        \n                            \n                                \n                                \n                                Tideways\nPlatform.sh supports Tideways APM for PHP.  This functionality is only available on PHP 7.0 and later.\nThe upstream now maintains two versions for Tideways, and both plugins are available on Platform.sh:\n\ntideways_xhprof: The open source version, therefore no licensing is required (On the downside, less integration services are available). You can use it in combination with xhprof UI.\ntideways: The bundle proprietary full version of the product and plugins, which the rest of the guide is mostly aimed to cover.\n\nGet Started\n1. Get your license key\nSign up at https://tideways.com and get your license key.\n2. Add your license key\nAdd your Tideways license key as a project level variable:\nplatform variable:create --visible-build false php:tideways.api_key --value '&lt;your-license-key&gt;'\n\n3. Enable the Tideways extension\nEnable the Tideways extension in your .platform.app.yaml as follows:\nruntime:\n    extensions:\n        - tideways\n\nEnabling the extension will also activate the Tideways background process.\nPush the changes to your Platform.sh environment to enable Tideways as follows:\ngit add .platform.app.yaml\ngit commit -m \"Enable Tideways.\"\ngit push\n\nTideways should now be enabled.  Give it a few hours to a day to get a decent set of data before checking your Tideways dashboard.\nDeployment Integration\nTideways integrates with Platform.sh deployment hooks and provides performance comparisons\nbefore and after deployments were released. You can find the Platform.sh CLI command to register\nthis hook for your application in Tideways \"Application Settings\" screen under the section\n\"Exports &amp; Integrations\". Here is an example:\nplatform integration:add --type=webhook --url=\"https://app.tideways.io/api/events/external/1234/abcdefghijklmnopqrstuvwxyz1234567890\"\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Integrations", "title": "HipChat", "url": "/administration/integrations/hipchat.html", "documentId": "80437b43cfa3411ab795c23ec5fae7b2d92adcec", "text": "\n                        \n                            \n                                \n                                \n                                HipChat\nThe HipChat integration allows you to send notifications about your Platform.sh activity directly to HipChat.\n\nFind the HipChat ROOM-ID.\n\nIn the HipChat web administrative UI, go to Admin &gt; Rooms and click on the room to link notifications.\nNote down the \"APP ID\" listed in the Room Details on the Room's 'Summary' page (you can also find the ID from the URL).\n\n\nGenerate a room-specific HIPCHAT-TOKEN.\n\nClick on the Room's 'Tokens' page in the sidebar.\nIn the Create New Token section specify 'PlatformSH' as the token's label and click \"Create\" button.\nNote down the Token value.\n\n\nCreate the HipChat webhook with Platform CLI.\n$ platform integration:add --type=hipchat --room=ROOM-ID --token=HIPCHAT-TOKEN\n\nThere are a number of optional parameters as well who's default values include:\n\n--events=*  (All Events)\n--environments=*  (All Environments)\n--excluded-environments= (Empty)\n--states=complete  (Complete state only)\n\nYou're given a chance to customize these parameters in an interactive shell prompt, or you may override the defaults on the command line:\n\n--states=pending,in_progress,complete (All states)\n\n\n\nValidate the integration\nYou can then verify that your integration is functioning properly using the CLI command\n$ platform integration:validate\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Integrations", "title": "GitLab", "url": "/administration/integrations/gitlab.html", "documentId": "e56c372937bbcd77bde2ccd7c854d04ec656f9c0", "text": "\n                        \n                            \n                                \n                                \n                                GitLab\nThe GitLab integration allows you to manage your Platform.sh environments directly from your GitLab repository.\nFeatures supported:\n\nCreate a new environment when creating a branch or opening a pull request on GitLab.\nRebuild the environment when pushing new code to GitLab.\nDelete the environment when merging a pull request.\n\nSetup\n1. Generate a token\nTo integrate your Platform.sh project with an existing GitLab repository, you first need to generate a token on your GitLab user profile. Simply go to your Settings page on GitLab and click Access Tokens.\nFill the Name field for example with \"Platform.sh Integration\" and optionally set an expiration time.\nGive it a description and then ensure the token has the following scopes:\n\napi  - Access your API\nread_user - Read user information\nread_repository - Read repositories\n\nCopy the token and make a note of it (temporarily).\nNote that for the integration to work, your GitLab user needs to have permission to push code to the repository.\n2. Enable the integration\nNote that only project owner or project admin can manage the integrations.\nOpen a terminal window (you need to have the Platform.sh CLI installed). Enable the GitLab integration as follows:\nplatform integration:add --type=gitlab --token=GITLAB-ACCESS-TOKEN --base-url=https://THE-URL-OF-YOUR-GITLAB --server-project=MY-NAMESPACE/MY-PROJECTNAME --project=PLATFORMSH_PROJECT_ID\n\nwhere\n\nPLATFORMSH_PROJECT_ID is the project ID for your Platform.sh project\nGITLAB-ACCESS-TOKEN is the token you generated in step 1\n--base-url is used as the base to call the Gitlab API; you should point it to https://gitlab.com if your project is hosted on Gitlab, or the URL for your own Gitlab instance otherwise. It should not include your namespace and project name.\nMY-NAMESPACE/MY-PROJECTNAME describes the namespace of your GitLab project, not including the base url.\n\nFor example, if your repository is located at https://gitlab.com/sandbox/my_application, the integration command would be\nplatform integration:add --type=gitlab --token=GITLAB-ACCESS-TOKEN --base-url=https://gitlab.com --server-project=sandbox/my_application --project=PLATFORMSH_PROJECT_ID\n\nOptional parameters:\n\n--build-merge-requests: Track and deploy merge-requests (true by default)\n--merge-requests-clone-parent-data : should merge requests clone the data from the parent environment (true by default)\n--fetch-branches: Track and deploy branches (true by default)\n--prune-branches: Delete branches that do not exist in the remote GitLab repository (true by default)\n--base-url: Only set if using self-hosted GitLab on your own server.  If so, set this to the base URL of your private server (the part before the user and repository name).\n\nNote that the --prune-branches option depends on --fetch-branches being enabled.  If --fetch-branches is disabled, --prune-branches will automatically be set to false, even if specifically set to true.\n3. Add the webhook\nThe previous command, if successful should output the configuration of the integration. The last element would look like:\n| hook_url | https://{region}.platform.sh/api/projects/{projectid}/integrations/{hook_id}/hook |\nThe CLI will create the necessary webhook using the above URL for you when there's correct permission set in the given token. If you see the message Failed to read or write webhooks, you will need to add a webhook manually:\n\nCopy the hook URL shown in the message.\nGo to your GitLab repository and click Settings &gt; Integrations.\nPaste the hook URL. In the Triggers section choose Push events, Tag push events and Merge Request events. Click on Add webhook.\n\nYou can now start pushing code, creating new branches or opening merge requests directly on your GitLab repository. You will see environments get automatically created and updated on the Platform.sh side.\n4. Validate the integration\nYou can then verify that your integration is functioning properly using the CLI command\n$ platform integration:validate\nTypes of environments\nEnvironments based on GitLab merge requests will have the correct 'parent' environment on Platform.sh; they will be activated automatically with a copy of the parent's data (unless you have set the option merge-requests-clone-parent-data to false).\nHowever, environments based on (non-merge-request) branches cannot have parents; they will inherit directly from master and start inactive by default.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Web", "title": "Project configuration", "url": "/administration/web/configure-project.html", "documentId": "680cd7dff148ee24a9a3077cbcdde9e6f2f89bc7", "text": "\n                        \n                            \n                                \n                                \n                                Project configuration\nYou can access the project-wide configuration settings by selecting the project from your list of projects, then click the Settings tab at the top of the screen.\nGeneral\nFrom the first page of the project settings, General, you can update the project name, or navigate to other project settings options on the left side of the screen.\n\nAccess\nThe Access screen allows you to manage users' access on your project.\nYou can invite new users to your project by clicking the Add button and entering their email address, or modify permissions of existing users by clicking the Edit link when hovering over the user.\n\n\nnote\nCurrently, permissions changes that grant or revoke SSH access to an environment take effect only after the next time that environment is deployed.\n\nSelecting a user will allow you to either edit that user's permissions or delete the user's access to the project entirely.\n\nIf you check the Project admin box, this user will be an administrator of the project and will have fulll access on all environments. If you uncheck the box, you'll have the option of adjusting the user's permissions on each environment.\n\nnote\nThe Account owner is locked and you can't change its permissions.\n\nDomains\nThe Domains screen allows you to manage your domains that your project will be accessible at.\n\nMore information on how to setup your domain.\n\nnote\nPlatform.sh expects an ASCII representation of the domain here. In case you want to use an internationalized domain name you can use the conversion tool provided by Verisign to convert your IDN domain to ASCII.\n\nCertificates\nThe Certificates screen allows you to manage your project's TLS certificates that enable HTTPS.\n\nYou can view current certificates by hovering over one on the list and clicking the View link that appears, or you can add a new certificate by clicking the Add button a the top of the page.\n\nAll projects get TLS certificates provided by Let's Encrypt automatically. In most cases no user action is required. You will only need to add certificates on this page if you are using TLS certificates provided by a third party.\nDeploy Key\nThe Deploy Key page provides the SSH key that Platform.sh will use when trying to access external private Git repository during the build process.\nThis is useful if you want to reuse some code components across multiple projects and manage those components as dependencies of your project.\n\nVariables\nThe Variables screen allows you to define the variables that will be available project-wide - that is, in each environment. It also allows you define variables that will be available during the build process.\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Integrations", "title": "GitHub", "url": "/administration/integrations/github.html", "documentId": "0aad82a3ab9c854febdf7817977bc94d80083bad", "text": "\n                        \n                            \n                                \n                                \n                                GitHub\nThe GitHub integration allows you to manage your Platform.sh environments directly from your GitHub repository.\nFeatures supported:\n\nCreate a new environment when creating a branch or opening a pull request on GitHub.\nRebuild the environment when pushing new code to GitHub.\nDelete the environment when merging a pull request.\n\nSetup\n1. Generate a token\nTo integrate your Platform.sh project with an existing GitHub repository, you first need to generate a token on your GitHub user profile. Simply go to your Settings, then select Developer settings and click Personal access tokens. Here you can Generate a new token.\nGive it a description and then ensure the token has the following scopes:\n\nTo integrate with public repositories: public_repo\nTo integrate with your own private repositories: repo\nTo integrate with your organization's private repositories: repo\n  and read:org\n\nCopy the token and make a note of it (temporarily).\nNote that for the integration to work, your GitHub user needs to have permission to push code to the repository.\n2. Enable the integration\nNote that only the project owner can manage integrations.\nOpen a terminal window (you need to have the Platform.sh CLI installed). Enable the GitHub integration as follows:\nplatform integration:add --type=github --project=PLATFORMSH_PROJECT_ID --token=GITHUB-USER-TOKEN --repository=USER/REPOSITORY\n\nwhere\n\nPLATFORMSH_PROJECT_ID is the project ID for your Platform.sh project\nGITHUB-USER-TOKEN is the token you generated in step 1\nUSER is your github user name\nREPOSITORY is the name of the repository in github (not the git address)\n\nNote that if your repository belongs to an organization, use --repository=ORGANIZATION/REPOSITORY.\ne.g.\nplatform integration:add --type=github --project=abcde12345 --token=xxx --repository=platformsh/platformsh-docs\n\nOptional parameters:\n\n--fetch-branches: Track and deploy branches (true by default)\n--prune-branches: Delete branches that do not exist in the remote GitHub repository (true by default)\n--build-pull-requests: Track and deploy pull-requests (true by default)\n--build-pull-requests-post-merge: false to have Platform.sh build the branch specified in a PR. true to build the result of merging the PR.  (false by default)\n--pull-requests-clone-parent-data: Set to false to disable cloning of parent environment data when creating a PR environment, so each PR environment starts with no data. (true by default)\n--base-url: Only set if using GitHub Enterprise, hosted on your own server.  If so, set this to the base URL of your private server (the part before the user and repository name).\n\nThe CLI will create the necessary webhook for you when there's correct permission set in the given token.\nNote that the --prune-branches option depends on --fetch-branches being enabled.  If --fetch-branches is disabled, --prune-branches will automatically be set to false, even if specifically set to true.\n3. Add the webhook\nIf you see the message Failed to read or write webhooks, you will need to add a webhook manually:\n\nCopy the hook URL shown in the message.\nGo to your GitHub repository and click Settings, select the Webhooks and Services tab, and click Add webhook.\nPaste the hook URL, choose application/json for the content type, choose \"Send me everything\" for the events you want to receive, and click Add webhook.\n\nYou can now start pushing code, creating new branches or opening pull requests directly on your GitHub repository.\nNote that if you have created your account using the GitHub oAuth Login then in order to use the Platform CLI, you will need to setup a password.\n4. Validate the integration\nYou can then verify that your integration is functioning properly using the CLI command\n$ platform integration:validate\nTypes of environments\nEnvironments based on GitHub pull requests will have the correct 'parent' environment on Platform.sh; they will be activated automatically with a copy of the parent's data.\nHowever, environments based on (non-pull-request) branches cannot have parents; they will inherit directly from master and start inactive by default.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Administration", "title": "Management console", "url": "/administration/web.html", "documentId": "bc38b33402907a53fcc42efaf69f389b9420624d", "text": "\n                        \n                            \n                                \n                                \n                                Management Console\nPlatform.sh provides a responsive management console which allows you to interact with your projects and manage your environments.\n\nEverything you can do with the management console you can also achieve with the  CLI (Command Line Interface).\nEnvironment List\nFrom your project's main page, each of the environments are available from the pull-down menu ENVIRONMENT at the top of the page.\n\nThere is also a graphic view of your environments on the right hand side, where you can view your environments as a list or as a project tree.\n\nThe name of the environment is struck out if it's been disabled. If it has an arrow next to it, this means the environment has children.\nEnvironments\nOnce you select an environment, the management console can give you a great deal of information about it.\nActivity Feed\nThe management console displays all the activity happening on your environments. You can filter messages per type.\n\nHeader\nWithin a project's environment, the management console exposes 4 main actions and 4 drop-down command options that you can use to interface with your environments.\n\nBranch\n\nBranching an environment means creating a new branch in the Git repository, as well as an exact copy of that environment.\nThe new branch includes code, all of the data that is stored on disk (database, Solr indexes, uploaded files, etc.), and also a new copy of the running services (and their configuration) that the application needs. This means that when you branch an environment, you also branch the complete infrastructure.\nDuring a branch, three things happen:\n\nA new branch is created in Git.\nThe application is rebuilt on the new branch, if necessary.\nThe new branch is deployed.\n\nAfter clicking Branch a dialog box will appear that will provide commands to execute future merges from the command line using the Platform.sh CLI.\n\nMerge\n\nMerging an environment means introducing the code changes from a branch to its parent branch and redeploying the parent.\nDuring a merge:\n\nThe code changes are merged via Git to the parent branch.\nThe application is rebuilt on the parent branch, if necessary.\nThe parent branch is deployed.\n\nRebuilding the application is not necessary if the same code was already built (for any environment): in this case you will see the message Slug already built for this tree id, skipping.\nAfter clicking Merge a dialog box will appear that will provide commands to execute future merges from the command line using the Platform.sh CLI.\n\nSync\n\nSynchronization performs a merge from a parent into a child environment, and then redeploys that environment.\nYou have the option of performing a Sync on only the code, replacing the data (i.e. databases) of that environment from its parent, or both.\nThese options are provided in a separate dialog box that will appear when you click the Sync button, along with the Platform.sh CLI commands that perform the same action.\nBe aware that sync uses the Snapshot mechanism and will have the same caveats.\n\nBe aware that sync uses the Backup mechanism and will have the same caveats.\nNote that Sync is only available if your branch has no unmerged commits, and can be fast-forwarded.\nIt is good practice to take a backup of your environment before performing a synchronization.\nBackup\n\nCreating a backup for an environment means saving a copy of the database so that it can be restored. You will see the backup in the activity feed of you environment in the Platform.sh management console where you can trigger the restore by clicking on the restore link.\nAfter clicking Backup a dialog box will appear that will provide commands to execute future merges from the command line using the Platform.sh CLI.\n\nYou can also use the CLI with:\n$ platform environment:backup\n\nto create a backup, and\n$ platform environment:restore\n\nto restore an existing backup.\nURLs\n\nThe URLs pull-down exposes the domains that can be used to access application environments from the web.\nGIT\n\nThe Git pull-down displays the commands to use to clone the codebase via Git.\nCLI\n\nThe CLI pull-down displays the commands to get your project set up locally with the Platform.sh CLI.\nSSH\n\nThe SSH pull-down display the commands to access your project over SSH.\nConfiguration settings\nFrom the management console you can also view information about how your routes, services, and applications are currently configured for the environment.\nAt the top of the page, click the \"Services\" tab.\nApplications\nSelect the application container on the left to show more detailed information for it on the right.\n\nThe \"Overview\" tab gives you metadata information regarding the application. It tells you what size container it has been configured for, the amount of persistent disk, the number of active workers and cron jobs, as well as the command to ssh into that container.\n\nEach cron job associated with the application is listed with its frequency, the last time it was run, it's status, and its command.\n\nThe \"Configuration\" tab provides an overview of the application's configuration pulled from its .platform.app.yaml file.\nServices\nEach service has a tab on the left, so select the one you are interested in.\n\nThe overview tab gives you metadata information regarding the service. It tells you what size container it has been configured for and the amount of persistent disk given to it in your services.yaml file.\n\nThe \"Configuration\" tab provides an overview of the service configuration that has been pulled from the services.yaml file.\nRoutes\n\nEach route will appear when you select the Routes tab on the left and describe its type and whether caching and SSI have been enabled for it.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Integrations", "title": "Webhooks", "url": "/administration/integrations/webhooks.html", "documentId": "cebd5c233389edb53ba5b8f7a8ba41bbf84a0758", "text": "\n                        \n                            \n                                \n                                \n                                Generic Webhook\nThis hook allows you to capture any push events on platform and POST a JSON message describing the activity to the url of your choice. You can use this to further automate your Platform.sh workflow.\n$ platform integration:add --type=webhook --url=A-URL-THAT-CAN-RECEIVE-THE-POSTED-JSON\n\nThe webhook URL will receive a POST message for every \"Activity\" that is triggered, and the message will contain complete information about the entire state of the project at that time.  In practice most of the message can be ignored but is available if needed.  The most commonly used values are documented below.\nIt's also possible to set the integration to only send certain activity types, or only activities on certain branches.  The CLI will prompt you to specify which to include or exclude.  Leave at the default values to get all events on all environments in a project.\nWebhook schema\nid\nA unique opaque value to identify the activity.\nproject\nThe Project ID for which the activity was triggered.  Use this value if you want to have multiple projects POST to the same URL.\ntype\nThe type property specifies the event that happened.  Its value is one of:\n\nproject.modify.title: The human-friendly title of the project has been changed.\nproject.create: A project has been created.  Although it will appear in the activity feed exactly once, it will not be sent via a webhook as it will always happen before a webhook can be configured.\nproject.domain.create: A new domain has been added to the project.\nproject.domain.delete: A domain associated with the project has been removed.\nproject.domain.update: A domain associated with the project has been updated, including modifying it's SSL certificate.\n\nenvironment.access.add: A new user has been given access to the environment.\n\nenvironment.access.remove: A user has been removed from the environment.\n\nenvironment.backup: A user triggered a backup\n\nenvironment.restore: A user restored a backup\n\nenvironment.push: A user has pushed code to a branch, either existing or new.\n\nenvironment.branch: A new branch has been created via the management console. (A branch created via a push will show up only as an environment.push.)\nenvironment.activate: A branch has been \"activated\", and an environment created for it.\nenvironment.initialize: The master branch of the project has just been initialized with its first commit.\nenvironment.deactivate: A branch has been \"deactivated\". The code is still there but the environment was destroyed.\nenvironment.synchronize: An environment has had its data and/or code re-copied from its parent environment.\nenvironment.merge: A branch was merged through the management console or Platform.sh API. A basic Git merge will not trigger this event.\nenvironment.delete: A branch was deleted.\n\nenvironment.route.create: A new route has been created through the management console. This will not fire for route edits made to the routes.yaml file directly.\n\nenvironment.route.delete: A route has been deleted through the management console. This will not fire for route edits made to the routes.yaml file directly.\nenvironment.route.update: A route has been modified through the management console. This will not fire for route edits made to the routes.yaml file directly.\n\nenvironment.variable.create: A new variable has been created.\n\nenvironment.variable.delete: A variable has been deleted.\nenvironment.variable.update: A variable has been modified.\n\nenvironment.update.http_access: HTTP access rules for an environment have been modified.\n\nenvironment.update.smtp: Sending of emails has been enabled/disabled for an environment.\nenvironment.update.restrict_robots: The block-all-robots feature has been enabled/disabled.\nenvironment.subscription.update: The master environment has been resized because the subscription has changed.  There are no content changes.\n\nenvironments\nAn array listing the environments that were involved in the activity. This is usually single-value.\nresult\nWhether the activity was completed successfully or not. It should be success if all went as planned.\ncreated_at, started_at, completed_at\nThese values are all timestamps in UTC.  If you need only a point in time when the action happened, use completed_at.  You can also combine it with started_at to see how long the activity took.\nlog\nA text description of the action that happened.  This is a human-friendly string that may be displayed to a user but should not be parsed for data as its structure is not guaranteed.\npayload.environment\nThis block contains information about the environment itself, after the action has taken place.  The most notable properties of this key are\n\nname (the name of the branch)\nmachine_name (the name of the environment)\nhead_commit (the Git commit ID that triggered the event)\n\npayload.user\nThe Platform.sh user that triggered the activity.\ndeployment\nThis large block details all information about all services in the environment.  That includes the resulting configuration objects derived from routes.yaml, services.yaml, and .platform.app.yaml.\nMost notably, the deployment.routes object's keys are all of the URLs made available by the environment.  Note that some will be redirects.  To find those that are live URLs filter to those objects whose type property is upstream.\nExample webhook payload\nThe following is an example of a webhook message.  Specifically, this one was created by a \"push\" event.\n{\n  \"id\": \"774-this-is-an-example-valuexzs4no\",\n  \"_links\": {\n    \"self\": {\n      \"href\": \"https://eu.platform.sh/api/projects/sx-this-is-an-example-value-hu/activities/774-this-is-an-example-valuexzs4no\",\n      \"meta\": {\n        \"get\": {\n          \"responses\": {\n            \"default\": {\n              \"schema\": {\n                \"properties\": {\n                  \"created_at\": {\n                    \"type\": \"string\",\n                    \"format\": \"date-time\"\n                  },\n                  \"updated_at\": {\n                    \"type\": \"string\",\n                    \"format\": \"date-time\"\n                  },\n                  \"type\": {\n                    \"type\": \"string\"\n                  },\n                  \"parameters\": {\n                    \"properties\": {\n                    },\n                    \"required\": [\n\n                    ]\n                  },\n                  \"project\": {\n                    \"type\": \"string\"\n                  },\n                  \"environments\": {\n                    \"items\": {\n                      \"type\": \"string\"\n                    },\n                    \"type\": \"array\"\n                  },\n                  \"state\": {\n                    \"type\": \"string\"\n                  },\n                  \"result\": {\n                    \"type\": \"string\"\n                  },\n                  \"started_at\": {\n                    \"type\": \"string\",\n                    \"format\": \"date-time\"\n                  },\n                  \"completed_at\": {\n                    \"type\": \"string\",\n                    \"format\": \"date-time\"\n                  },\n                  \"completion_percent\": {\n                    \"type\": \"integer\"\n                  },\n                  \"log\": {\n                    \"type\": \"string\"\n                  },\n                  \"payload\": {\n                    \"properties\": {\n                    },\n                    \"required\": [\n\n                    ]\n                  }\n                },\n                \"required\": [\n                  \"created_at\",\n                  \"updated_at\",\n                  \"type\",\n                  \"parameters\",\n                  \"project\",\n                  \"environments\",\n                  \"state\",\n                  \"result\",\n                  \"started_at\",\n                  \"completed_at\",\n                  \"completion_percent\",\n                  \"log\",\n                  \"payload\"\n                ]\n              }\n            }\n          },\n          \"parameters\": [\n\n          ]\n        }\n      }\n    }\n  },\n  \"created_at\": \"2017-12-07T10:45:30.870660+00:00\",\n  \"updated_at\": \"2017-12-07T10:47:39.369761+00:00\",\n  \"type\": \"environment.push\",\n  \"parameters\": {\n    \"environment\": \"master\",\n    \"old_commit\": \"34be31cbabcc0f65d7fd8ec29769947396d0cabd\",\n    \"new_commit\": \"8d2e6003d50136c750fb8b65ec506ee3aa4a5b15\",\n    \"user\": \"384491da-031e-4c23-b264-9f96040a6e36\"\n  },\n  \"project\": \"sx-this-is-an-example-value-hu\",\n  \"environments\": [\n    \"master\"\n  ],\n  \"state\": \"complete\",\n  \"result\": \"success\",\n  \"started_at\": \"2017-12-07T10:45:30.996898+00:00\",\n  \"completed_at\": \"2017-12-07T10:47:39.369741+00:00\",\n  \"completion_percent\": 100,\n  \"log\": \"Found 1 new commit.\\n\\nBuilding application 'myrubyapp' (runtime type: ruby:2.4-rc, tree: 2d24228)\\n  Generating runtime configuration.\\n  \\n  Executing build hook...\\n    2.4.2\\n    Fetching gem metadata from https://rubygems.org/..........\\n    Resolving dependencies...\\n    Installing concurrent-ruby 1.0.5\\n    Installing i18n 0.9.1\\n    Installing minitest 5.10.3\\n    Installing thread_safe 0.3.6\\n   ...more lines...\\n   Installing unicorn 5.3.1\\n    Using bundler 1.7.4\\n    Your bundle is complete!\\n    Use `bundle show [gemname]` to see where a bundled gem is installed.\\n  \\n  Executing pre-flight checks...\\n\\n  Compressing application.\\n  Beaming package to its final destination.\\n\\nProvisioning certificates:\\n  Reusing existing certificates.\\n\\n\\nRe-deploying environment sx-this-is-an-example-value-hu-master-7rqtwti.\\n  Environment configuration:\\n    myrubyapp (type: ruby:2.4-rc, size: S, disk: 2048)\\n    postgresql (type: postgresql:9.3, size: S, disk: 256)\\n    mongodb (type: mongodb:3.0, size: S, disk: 500)\\n    redis (type: redis:3.2-rc, size: S)\\n    influxdb (type: influxdb:1.2, size: S, disk: 256)\\n    elasticsearch (type: elasticsearch, size: S, disk: 256)\\n    rabbitmq (type: rabbitmq:3.5, size: S, disk: 256)\\n    mysql (type: mysql:10.0, size: S, disk: 256)\\n    solr (type: solr:4.10, size: S, disk: 256)\\n\\n  Environment routes:\\n    http://master-7rqtwti-sx-this-is-an-example-value-hu.eu.platform.sh/ redirects to https://master-7rqtwti-sx-this-is-an-example-value-hu.eu.platform.sh/\\n    https://master-7rqtwti-sx-this-is-an-example-value-hu.eu.platform.sh/ is served by application `myrubyapp`\\n\\n\",\n  \"payload\": {\n    \"environment\": {\n      \"status\": \"active\",\n      \"head_commit\": \"8d2e6003d50136c750fb8b65ec506ee3aa4a5b15\",\n      \"machine_name\": \"master-7rqtwti\",\n      \"name\": \"master\",\n      \"parent\": null,\n      \"title\": \"Master\",\n      \"created_at\": \"2017-11-22T14:43:17.154870+00:00\",\n      \"updated_at\": \"2017-11-22T14:43:17.155103+00:00\",\n      \"clone_parent_on_create\": true,\n      \"project\": \"sx-this-is-an-example-value-hu\",\n      \"is_dirty\": false,\n      \"restrict_robots\": true,\n      \"has_code\": true,\n      \"enable_smtp\": true,\n      \"id\": \"master\",\n      \"deployment_target\": \"local\",\n      \"http_access\": {\n        \"is_enabled\": true,\n        \"addresses\": [\n\n        ],\n        \"basic_auth\": {\n        }\n      },\n      \"is_main\": true\n    },\n    \"commits\": [\n      {\n        \"sha\": \"8d2e6003d50136c750fb8b65ec506ee3aa4a5b15\",\n        \"message\": \"deploy with release candidates\",\n        \"parents\": [\n          \"34be31cbabcc0f65d7fd8ec29769947396d0cabd\"\n        ],\n        \"author\": {\n          \"email\": \"ori@example.com\",\n          \"name\": \"Ori Pekelman\"\n        }\n      }\n    ],\n    \"commits_count\": 1,\n    \"user\": {\n      \"created_at\": \"2017-12-07T10:45:13.491553+00:00\",\n      \"display_name\": \"Ori Pekelman\",\n      \"id\": \"384491da-031e-4c23-b264-9f96040a6e36\",\n      \"updated_at\": null\n    },\n    \"deployment\": {\n      \"id\": \"current\",\n      \"services\": {\n        \"mongodb\": {\n          \"type\": \"mongodb:3.0\",\n          \"size\": \"AUTO\",\n          \"disk\": 500,\n          \"access\": {\n          },\n          \"configuration\": {\n          },\n          \"relationships\": {\n          }\n        },\n        \"redis\": {\n          \"type\": \"redis:3.2-rc\",\n          \"size\": \"AUTO\",\n          \"disk\": null,\n          \"access\": {\n          },\n          \"configuration\": {\n          },\n          \"relationships\": {\n          }\n        },\n        \"solr\": {\n          \"type\": \"solr:4.10\",\n          \"size\": \"AUTO\",\n          \"disk\": 256,\n          \"access\": {\n          },\n          \"configuration\": {\n          },\n          \"relationships\": {\n          }\n        }\n      },\n      \"routes\": {\n        \"https://master-7rqtwti-sx-this-is-an-example-value-hu.eu.platform.sh/\": {\n          \"type\": \"upstream\",\n          \"redirects\": {\n            \"expires\": \"-1s\",\n            \"paths\": {\n            }\n          },\n          \"original_url\": \"https://{default}/\",\n          \"cache\": {\n            \"enabled\": true,\n            \"default_ttl\": 0,\n            \"cookies\": [\n              \"*\"\n            ],\n            \"headers\": [\n              \"Accept\",\n              \"Accept-Language\"\n            ]\n          },\n          \"ssi\": {\n            \"enabled\": false\n          },\n          \"upstream\": \"myrubyapp\"\n        },\n        \"http://master-7rqtwti-sx-this-is-an-example-value-hu.eu.platform.sh/\": {\n          \"type\": \"redirect\",\n          \"redirects\": {\n            \"expires\": \"-1s\",\n            \"paths\": {\n            }\n          },\n          \"original_url\": \"http://{default}/\",\n          \"to\": \"https://master-7rqtwti-sx-this-is-an-example-value-hu.eu.platform.sh/\"\n        }\n      },\n      \"webapps\": {\n        \"myrubyapp\": {\n          \"size\": \"AUTO\",\n          \"disk\": 2048,\n          \"access\": {\n            \"ssh\": \"contributor\"\n          },\n          \"relationships\": {\n            \"mongodb\": \"mongodb:mongodb\",\n            \"redis\": \"redis:redis\",\n            \"solr\": \"solr:solr\"\n          },\n          \"mounts\": {\n            \"/public\": \"shared:files/files\"\n          },\n          \"timezone\": null,\n          \"variables\": {\n          },\n          \"name\": \"myrubyapp\",\n          \"type\": \"ruby:2.4-rc\",\n          \"runtime\": {\n          },\n          \"preflight\": {\n            \"enabled\": true,\n            \"ignored_rules\": [\n\n            ]\n          },\n          \"web\": {\n            \"locations\": {\n              \"/\": {\n                \"root\": \"public\",\n                \"expires\": \"1h\",\n                \"passthru\": true,\n                \"scripts\": true,\n                \"allow\": true,\n                \"headers\": {\n                },\n                \"rules\": {\n                }\n              }\n            },\n            \"commands\": {\n              \"start\": \"unicorn -l $SOCKET -E production config.ru\",\n              \"stop\": null\n            },\n            \"upstream\": {\n              \"socket_family\": \"unix\",\n              \"protocol\": null\n            },\n            \"move_to_root\": false\n          },\n          \"hooks\": {\n            \"build\": \"ruby -e 'bundle install\\n\",\n            \"deploy\": null\n          }\n        }\n      },\n      \"workers\": {\n      }\n    }\n  }\n}\n\nValidate the integration\nYou can then verify that your integration is functioning properly using the CLI command\n$ platform integration:validate\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Administration", "title": "Backup and restore", "url": "/administration/backup-and-restore.html", "documentId": "376daa9bcad5656f29ca4b78a681163719662e17", "text": "\n                        \n                            \n                                \n                                \n                                Backup and Restore\nBackups\nBackups are triggered directly via the management console or via the CLI. The backup creates a complete snapshot of the environment. It includes all persistent data from all running services (MySQL, SOLR,...) and any files stored on the mounted volumes.\nYou need to have the \"admin\" role in order to create a backup of an environment.\nBackups on Platform.sh Professional are retained for at least 7 days. They will be purged between 7 days and 6 months, at Platform.sh's discretion. Please see the data retention page for more information.\n\nnote\nWe advise you to make backups of your live environment before merging an environment to the live environment, or each time you increase the storage space of your services.\n\nUsing the CLI:\n$ platform backup:create\n\nPlease be aware that triggering a backup will cause a momentary pause in site availability so that all requests can complete, allowing the backup to be taken against a known consistent state.  The total interruption is usually only 15 to 30 seconds and any requests during that time are held temporarily, not dropped.\nRestore\nYou will see the backup in the activity feed of your environment in the Platform.sh management console. You can trigger the restore by clicking on the restore link. You can also restore the backup to a different environment using the CLI.\nYou can list existing backups with the CLI as follows:\n$ platform backups\n\nFinding backups for the environment master\n+---------------------+------------+----------------------+\n| Created             | % Complete | Backup name          |\n+---------------------+------------+----------------------+\n| 2015-06-19 17:11:42 | 100        | 2ca4d90639f706283fee |\n| 2015-05-28 15:05:42 | 100        | 1a1fbcb9943849706ee6 |\n| 2015-05-21 14:38:40 | 100        | 7dbdcdb16f41f9e1c061 |\n| 2015-05-20 15:29:58 | 100        | 4997900d2804d5b2fc39 |\n| 2015-05-20 13:31:57 | 100        | c1f2c976263bec03a10e |\n| 2015-05-19 14:51:18 | 100        | 71051a8fe6ef78bca0eb |\n\nYou can then restore a specific backup with the CLI as follows:\n$ platform backup:restore 2ca4d90639f706283fee\n\nOr even restore the backup to a different branch with the CLI as follows:\n$ platform backup:restore --target=RESTORE_BRANCH 2ca4d90639f706283fee\n\nFor this to work, it's important to act on the active branch on which the backup was taken. Restoring a backup from develop when working on the staging branch is impossible. Switch to the acting branch and set your --target as above snippet mentions.\nIf no branch already exists, you can specify the parent of the branch that will be created to restore your backup to as follows:\n$ platform backup:restore --branch-from=PARENT_BRANCH 2ca4d90639f706283fee\n\n\nnote\nYou need \"admin\" role to restore your environment from a backup.\n\nBe aware that the older US and EU regions do not support restoring backups to different environments.  If your project is on one of the older regions you may file a support ticket to ask that a backup be restored to a different environment for you, or migrate your project to one of the new regions that supports this feature.\nBackups and downtime\nA backup does cause a momentary pause in service. We recommend running during non-peak hours for your site.\nAutomated backups\nBackups are not triggered automatically on Platform.sh Professional.\nBackups may be triggered by calling the CLI from an automated system such as Jenkins or another CI service, or by installing the CLI tool into your application container and triggering the backup via cron.\nAutomated backups using Cron\n\nnote\nAutomated backups using cron requires you to get an API token and install the CLI in your application container.\n\nWe ask that you not schedule a backup task more than once a day to minimize data usage.\nOnce the CLI is installed in your application container and an API token configured you can add a cron task to run once a day and trigger a backup.  The CLI will read the existing environment variables in the container and default to the project and environment it is running on. In most cases such backups are only useful on the master production environment.\nA common cron specification for a daily backup on the master environment looks like this:\ncrons:\n    backup:\n        spec: '0 5 * * *'\n        cmd: |\n            if [ \"$PLATFORM_BRANCH\" = master ]; then\n                platform backup:create --yes --no-wait\n            fi\n\nThe above cron task will run once a day at 5 am (UTC), and, if the current environment is the master branch, it will run platform backup:create on the current project and environment.  The --yes flag will skip any user-interaction.  The --no-wait flag will cause the command to complete immediately rather than waiting for the backup to complete.\n\nnote\nIt is very important to include the --no-wait flag.  If you do not, the cron process will block and you will be unable to deploy new versions of the site until the backup creation process is complete.\n\nRetention\nPlease see our Data Retention Page.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Integrations", "title": "New Relic", "url": "/administration/integrations/new-relic.html", "documentId": "72ac293e52744f525ecdf270b76d132d3e48ef56", "text": "\n                        \n                            \n                                \n                                \n                                New Relic\nPlatform.sh supports New Relic APM.\nOn a Grid plan\n1. Get your license key\nSign up at https://newrelic.com and get your license key.\n2. Add your license key\nAdd your New Relic license key as a project level variable:\nplatform variable:create --visible-build false php:newrelic.license --value '&lt;your-new-relic-license-key&gt;'\n\n3. Enable the New Relic extension\nEnable the New Relic extension in your .platform.app.yaml as follows:\nruntime:\n    extensions:\n        - newrelic\n\nPush the changes to your Platform.sh environment to enable New Relic as follows:\ngit add .platform.app.yaml\ngit commit -m \"Enable New Relic.\"\ngit push\n\nThat's it! You need to wait a little bit for your New Relic dashboard to be generated.\nOn a Dedicated cluster\nSign up at https://newrelic.com and get your license key.  Then open a support ticket and let us know what your key is.  Our support team will install it and let you know when it is complete.\nTroubleshoot\nAdditionally, you can check that your application is properly connected to New Relic by looking at the /var/log/app.log file:\nplatform log app\n\n2017/04/19 14:00:16.706450 (93) Info: Reporting to: https://rpm.newrelic.com/accounts/xxx/applications/xxx\n2017/04/19 14:00:16.706668 (93) Info: app 'xxx-master-xxx.app' connected with run id 'xxx'\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Integrations", "title": "Blackfire", "url": "/administration/integrations/blackfire.html", "documentId": "35b11c0a52f66882d3bf2ac446fb856636e81cfd", "text": "\n                        \n                            \n                                \n                                \n                                Blackfire\nPlatform.sh supports Blackfire.io.\nBlackfire is a PHP profiler and automated performance testing tool that can be used in the development Integration, Staging, and Production environments.\nIt grants details information on your PHP code's resources consumption across Wall-Time, CPU, I/O, Memory, Network Calls, HTTP requests and SQL queries.\nIn addition, it can profile your code automatically and notify you whenever your code does not comply with best practices for PHP, Symfony, Drupal, eZPlatform, Typo3 &amp; Magento code performance management.\nFor a high level overview and demo of Blackfire, check out the full video tutorial.\nVersion\nCheck the latest versions of the probe and CLI tool on Blackfire's documentation.\nOn a Grid plan\n1. Get your credentials\nSign up for the free 15 days Premium trial at blackfire.io and install the Blackfire Companion web browser extension (Chrome or Firefox).\n\nnote\nBlackfire also offers a perpetually-free edition but it is for local development only and will not run on Platform.sh.\n\nGo to your Dashboard and create a new environment under the Environments tab.\n\nYou will need to store the server credentials for further configuration. You can find them any time under the \"Settings\" tab of your environment in Blackfire.\n\n2. Enable the Blackfire extension\nConfigure the extension in your .platform.app.yaml as follows:\nruntime:\n    extensions:\n        - blackfire\n\nPush the changes to your Platform environment to enable Blackfire as follows:\ngit add .platform.app.yaml\ngit commit -m \"Enable Blackfire.\"\ngit push\n\n3. Configure your server credentials\nBlackfire enables to have a fine grained configuration of server credentials across branches and environments on Platform.sh.\nConfiguring global server credentials\nConfiguring server credentials on your master branch will enable you to make sure you can profile any other branch:\nplatform variable:create -e master env:BLACKFIRE_SERVER_ID --value &lt;insert your Server ID&gt;\nplatform variable:create -e master env:BLACKFIRE_SERVER_TOKEN --value &lt;insert your Server Token&gt;\n\nConfiguring server credentials per branch\nA recommendation is to have a Blackfire environment for production, another one for staging, and another one for development/integration. That can be mapped in Platform.sh to one Blackfire environment for the production branch, one for the staging branch, and one for all feature branches.\nplatform variable:create -e=&lt;insert your branch name&gt; env:BLACKFIRE_SERVER_ID &lt;insert your Server ID&gt;\nplatform variable:create -e=&lt;insert your branch name&gt; env:BLACKFIRE_SERVER_TOKEN &lt;insert your Server Token&gt;\n\n4. Confirm it's running\nLogin via SSH to your container and confirm that Blackfire is running as follows:\nphp --ri blackfire\n\nblackfire\n\nblackfire =&gt; enabled\nblackfire =&gt; 1.16.1\nTiming measurement =&gt; gtod\nNum of CPU =&gt; 8\n...\n\nOn a Dedicated cluster\nSign up for the free 15 days Premium trial at blackfire.io and install the Blackfire Companion web browser extension (Chrome or Firefox).\nThen open a support ticket with the Backfire server ID and token.  The client ID and token is optional.  Our support team will install it for you.\nNote, Blackfire integration works only on profiling your cluster via the URL to the origin. Do not profile your site going through the CDN.\nProfiling web requests\nAccess your site via your browser and click Profile in the Blackfire Companion.\n\nThat's it! Your site will be profiled and you should get all the results in your Blackfire account.\nProfiling CLI commands\nTo profile your PHP CLI scripts, use the following command line:\nblackfire --config /etc/platform/$USER/blackfire.ini &lt;command&gt;\nGoing further with Blackfire\nBlackfire also enables to:\n\ncollaborate with the rest of your team\nwrite performance tests\nautomate profiling with periodic builds\nintegrate further with Platform.sh by enabling to automate profiling as each code commit\nintegrate with New Relic for combined benefits of monitoring and profiling\nintegrate with GitHub, Bitbucket and GitLab to show the results of Blackfire builds at the commit status level\n\nCheck Blackfire's documentation for more information.\n\nnote\nThose features may require a Premium or an Enterprise subscription.\nWe offer attractive bundles of Platform.sh and Blackfire.io subscriptions.\nPlease contact our sales department to discuss how we can help you.\n\nTroubleshooting\nBypassing Reverse Proxy, Cache, and Content Delivery Networks (CDN)\nIf you are using one of those, you will need them to let Blackfire access your servers.\nMore information on how to configure a bypass.\nHTTP Cache configuration\nIf you are using the HTTP cache with cookies , please update in your .platform.app.yaml the cookies that are allowed to go through the cache. You need to allow the __blackfire cookie name.\nSomething like:\ncache:\n    enabled: true\n    cookies: [\"/SESS.*/\", \"__blackfire\"]\n\nReaching out to the Blackfire support\nIf the above didn't help, collect the following and send it to the Blackfire support:\n\nThe output of platform ssh -- php -d display_startup_errors=on --ri blackfire command\nThe Blackfire logs\n\nGetting the Blackfire logs\nPlease execute the following in the environment where you're facing the issue:\n\nplatform variable:create php:blackfire.log_file --value /tmp/blackfire.log\nplatform variable:create php:blackfire.log_level --value 4\nstart a profile/build again\n\nYou will get the logs with platform ssh -- cat /tmp/blackfire.log &gt; blackfire.log.\nDisabling the Blackfire logs\nOnce you are done, please disable logging with:\n\nplatform variable:delete php:blackfire.log_file\nplatform variable:delete php:blackfire.log_level\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Integrations", "title": "Bitbucket", "url": "/administration/integrations/bitbucket.html", "documentId": "31a322f71ec0aa72b4b6f474bacd9a84c7e2598d", "text": "\n                        \n                            \n                                \n                                \n                                Bitbucket\nThere are two options for setting up Bitbucket integrations with Platform.sh.\n\nUsing the Platform.sh add-on\nSetting up an OAuth consumer manually\n\nPlatform.sh Bitbucket add-on\nThe Bitbucket add-on allows you to manage your Platform.sh environments directly from your Bitbucket repository.\nSupported:\n\nCreate a new environment when creating a branch or opening a pull request on Bitbucket.\nRebuild the environment when pushing new code to Bitbucket.\nDelete the environment when merging a pull request.\n\nInstall the add-on\nOn your Bitbucket account, click on your avatar and select App Marketplace. Filter the list of available add-ons for the \"Deployment\" category. Add the add-on titled \"Platform.sh PHP PaaS\" and grant its access to the account.\n\n\nNotes\n\nThe Bitbucket Integration can only be connected by the Platform.sh account owner.\nPlatform.sh recommends you install the add-on at the team level (select Manage Team instead) so that every repository that belongs to the team can use the add-on.\nIf you have created your account using the bitbucket oAuth Login in order to use the Platform CLI you will need to setup a password which you can do by visiting this page https://accounts.platform.sh/user/password\n\n\nSetup the integration\nTo connect your Bitbucket repository to Platform.sh, go to the repository page as an administrator  and click on the Settings icon. Then click Platform.sh integration under the PLATFORM.SH section.\nThe add-on will give you two options when you grant Platform.sh access to the repository:\n\nCreate a new project: this will allow you to choose a region and create a project for the repository.\nConnect to an existing project (bottom of the page): this will allow you to connect the repository to a pre-initialized empty project on Platform.sh.\n\nSetting up an OAuth consumer\nIf that Platform.sh add-on does not configure the integration automatically, you can also do so manually by creating an OAuth consumer for your account.\nOn Bitbucket\n\nGo to your user account and click \"Settings\".\nUnder \"ACCESS MANAGEMENT\" click OAuth.\nAt the bottom of that page under \"OAuth consumers\", click the \"Add consumer\" button.\nFill out the information for the consumer. In order for OAuth2 to work correctly, it's recommended that you include:\nName: Give the consumer a recognizable name, like Platform.sh consumer or Platform.sh integration.\nCallback URL: The URL users will be redirected to after access authorization. It is sufficient to set this value to http://localhost.\nSet as a private consumer: At the bottom of the \"Details\" section, select the \"This is a private consumer\" checkbox.\nPermissions: Sets the integration permissions for Platform.sh. These permissions will create the webhooks that will enable Platform.sh to mirror actions from the Bitbucket repository.\nAccount - Email, Read\nRepositories - Read, Write\nPull requests - Read\nWebhooks - Read and write\n\n\n\n\nAfter you have completed the form, Save the consumer.\nAfter you have saved, you will see Platform.sh consumer listed in the \"OAuth consumers\" section. If you open that item, it will expose two variables that you will need to complete the integration using the Platform.sh CLI: Key and Secret.\n\nLocal\nInstall the Platform.sh CLI if you have not already done so.\nRetrieve a PROJECT_ID for an existing project with platform project:list or create a new project with platform project:create.\nThen run the integration command:\nplatform integration:add --type=bitbucket --project &lt;PLATFORMSH_PROJECT_ID&gt; --key &lt;CONSUMER_KEY&gt; --secret &lt;CONSUMER_SECRET&gt; --repository &lt;USER&gt;/&lt;REPOSITORY&gt;\nwhere\n\nPLATFORMSH_PROJECT_ID is the project ID for your Platform.sh project.\nCONSUMER_KEY is the Key variable of the consumer you created.\nCONSUMER_SECRET is the Secret variable of the consumer you created.\nUSER/REPOSITORY is the location of the repository.\n\nValidate the integration\nIn both cases, you can verify that your integration is functioning properly using the CLI command\n$ platform integration:validate\nOptional parameters\nBy default several parameters will be set for the Bitbucket integration. They can be changed using the platform integration:update command.\n\n--fetch-branches: Track and deploy branches (true by default)\n--prune-branches: Delete branches that do not exist in the remote Bitbucket repository (true by default)\n--build-pull-requests: Track and deploy pull-requests (true by default)\n--build-pull-requests-post-merge: false to have Platform.sh build the branch specified in a PR. true to build the result of merging the PR.  (false by default)\n--pull-requests-clone-parent-data: Set to false to disable cloning of parent environment data when creating a PR environment, so each PR environment starts with no data. (true by default)\n\nFor more information see:\nplatform help integration:update\n\nnote\nThe --prune-branches option depends on --fetch-branches being enabled. If --fetch-branches is disabled, --prune-branches will automatically be set to false, even if specifically set to true.\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Administration", "title": "External integrations", "url": "/administration/integrations.html", "documentId": "07fd2e4d065eb3b9e3889077cfe0672c43a5ca89", "text": "\n                        \n                            \n                                \n                                \n                                Integrations\nPlatform.sh can be integrated with external services.\nWe support native integrations with multiple services, first and foremost Git hosting services such as GitHub, GitLab, or Bitbucket.  You can continue to use those tools for your development workflow, and have Platform.sh environments created automatically for your pull requests and branches.\nYou can also add our native integrations with performance monitoring tools such as Blackfire, New Relic, or Tideways, as well as setting up health notifications.  Or create your own integration using our webhooks.\nBe aware that only a project administrator (someone with admin level access to the project) can add or remove integrations.  See User administration for more details.\nListing active integrations\nWith the CLI, you can list all your active integrations:\nplatform integrations\n\n\n\nnote\nIf you have created your account using the Bitbucket or GitHub oAuth Login, then in order to use the Platform.sh CLI you will need to set up a password by visiting https://accounts.platform.sh/user/password.\n\nValidating integrations\nOnce your integration has been configured, you can validate that it is functioning properly with the command:\n$ platform integration:validate\n\nEnter a number to choose an integration:\n  [0] 5aut2djgt6kdd (health.slack)\n  [1] a6535j9qp4sl8 (github)\n &gt; 1\n\nValidating the integration a6535j9qp4sl8 (type: github)...\nThe integration is valid.\nDebugging integrations\nWhen integrations run, they trigger \"activities.\"  Activities are actions that happen on Platform.sh, and they get logged.\nThose logs are available via the CLI.  In most cases they are not necessary but may be useful for debugging an integration if it is misbehaving for some reason.\nThere are a handful of commands available, all under the integrations section.\nList all activities\nThe commands platform integration:activity:list or its alias platform integration:activities will list all activities on a given project and integration.\nFor example, for the project for this site, the command platform integration:activity:list outputs:\n$ platform integration:activities\n\nEnter a number to choose an integration:\n  [0] dxr45hfldrkoe (webhook)\n  [1] n2ukd4p7qofg4 (health.email)\n  [2] c4opi5tjv3yfd (github)\n &gt; 2\n\nActivities on the project Platform.sh | Docs (6b2eocegfkwwg), integration c4opi5tjv3yfd (github):\n+---------------+---------------------------+-------------------------------------------------------------+----------+---------+\n| ID            | Created                   | Description                                                 | State    | Result  |\n+---------------+---------------------------+-------------------------------------------------------------+----------+---------+\n| 6456zmdtoykxa | 2020-04-14T16:38:09-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success |\n| wcwp34yjvydgk | 2020-04-14T16:35:22-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success |\n| w2bp3oa5xbfoe | 2020-04-14T16:33:13-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success |\n| uqqvdyxmcdmsa | 2020-04-14T16:31:45-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success |\n| 7x3wefhh4fwqc | 2020-04-14T16:30:36-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success |\n| a46aah3ga65gc | 2020-04-14T16:29:46-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success |\n| r7erid2jlixgi | 2020-04-14T16:24:50-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success |\n| ieufk3vvde5oc | 2020-04-14T16:24:49-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success |\n| bc7ghg36ty4ea | 2020-04-14T15:30:17-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success |\n| 4qojtv7a6rk2w | 2020-04-14T15:27:26-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success |\n+---------------+---------------------------+-------------------------------------------------------------+----------+---------+\n\nYou may also specify which integration to display in the command line directly: platform integration:activities c4opi5tjv3yfd.\nThe ID is an internal identifier for the activity event.  The Description field is an arbitrary string of text produced by the integration code.  The State and Result fields indicate if the activity completed successfully, failed for some reason, or is currently in progress.\nSee the --help output of the command for more options.\nShowing detailed information on an activity\nYou can call up more detailed information on a specific activity by its ID, using the platform integration:activity:log command.  It requires both the integration ID and an activity ID from the list above.  It also works best with the -t option to include timestamps.\n$ platform integration:activity:log c4opi5tjv3yfd 6456zmdtoykxa -t\n\nIntegration ID: ceopz5tgj3yfc\nActivity ID: 6456zmdtoykxa\nType: integration.github.fetch\nDescription: Fetching from https://github.com/platformsh/platformsh-docs\nCreated: 2020-04-15T08:44:07-05:00\nState: complete\nLog:\n[2020-04-15T13:44:17-05:00] Waiting for other activities to complete\n[2020-04-15T13:46:07-05:00] Fetching from GitHub repository platformsh/platformsh-docs\n[2020-04-15T13:46:09-05:00]   No changes since last fetch\n[2020-04-15T13:46:09-05:00]\n[2020-04-15T13:46:09-05:00] Synchronizing branches\n[2020-04-15T13:46:09-05:00]\n[2020-04-15T13:46:09-05:00] Synchronizing pull requests\n[2020-04-15T13:46:59-05:00]\n[2020-04-15T13:46:59-05:00] W: No changes found, scheduling a retry..\n\nThat will show the full output of the activity, including timestamps.  That can be especially helpful if trying to determine why an integration is not behaving as expected.\nSee the --help output of the command for more options.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Administration", "title": "Users", "url": "/administration/users.html", "documentId": "207731d69ada05ddfc7da55f651240a8b59bedcc", "text": "\n                        \n                            \n                                \n                                \n                                User administration\nUser roles\nEvery Platform.sh user has a role which controls access and improves security on your project. Different roles are authorized to do different things with your applications, environments and users. You can use your collection of Roles to manage how users interact with Platform.sh.\nAt the project level:\n\nProject Administrator - A project administrator can change settings and execute actions on any environment.\nProject Viewer - A project reader can view all environments within a project but cannot execute any actions on them.\n\nA Project Reader can have a specific role on different environments. At the environment level:\n\nEnvironment Administrator - An environment administrator can change settings and execute actions on this environment.\nEnvironment Contributor - An environment contributor can push code to this environment and branch the environment.\nEnvironment Viewer - An environment reader can only view this environment.\n\n\nImportant!\nAfter a user is added to (or deleted from) an environment, it will be automatically redeployed, after which the new permissions will be fully updated.\nWhen adding users at the project level, however, redeployments do not occur automatically, and you will need to trigger a redeployments to update those settings for each environment using the CLI command platform redeploy. Otherwise, user access will not be updated on those environments until after the next build and deploy commit. \n\n\nWhen a development team works on a project, the team leader can be the project administrator and decide which roles to give his team members.  One team member can contribute to one environment, another member can administer a different environment and the customer can be a reader of the master environment.\nIf you want your users to be able to see everything (Reader), but only commit to a specific branch, change their permission level on that environment to \"Contributor\".\n\nSSH Access Control\nAn environment contributor can push code to the environment and has SSH access to the environment. You can change this by specifying user types with SSH access.\nnote\nThe project owner - the person licensed to use Platform.sh - doesn't have special powers. A project owner usually has a project administrator role.\n\n\nManage user permissions at the project level\nFrom your list of projects, select the project where you want to view or edit user permissions. At this point, you will not have selected a particular environment. Click the Settings tab at the top of the page, then click the Access tab on the left to show the project-level users and their roles.\n\nThe Access tab shows project-level users and their roles.\nSelecting a user will allow you either to edit that user's permissions or delete the user's access to the project entirely.\nAdd a new user by clicking on the Add button.\nIf you select the \"Viewer\" role for the user, you'll have the option of adjusting the user's permissions at the environment level.\nFrom this view, you can assign the user's access. Selecting them to become a \"Project admin\" will give them \"Admin\" access to every environment in the project. Alternatively, you can give the user \"Admin\", \"Viewer\", \"Contributor\", or \"No Access\" to each environment separately.\nIf you select the \"Viewer\" role for the user, you'll have the option of adjusting the user's permissions at the environment level.\nOnce this has been done, if the user does not have a Platform.sh account, they will receive an email asking to confirm their details and register an account name and a password.\nIn order to push and pull code (or to SSH to one of the project's environments) the user will need to add an SSH key.\nIf the user already has an account, they will receive an email with a link to the project.\n\nManage user permissions at the environment level\nFrom within a project select an environment from the ENVIRONMENT pull-down menu.\nClick the Settings tab at the top of the screen and then click the Access tab on the left hand side.\n\nThe Access tab shows environment-level users and their roles.\nSelecting a user will allow you either to edit that user's permissions or delete the user's access to the environment entirely.\nAdd a new user by clicking on the Add button.\n\nnote\nRemember the user will only be able to access the environment once it has been rebuilt (after a git push)\n\n\nManage users with the CLI\nYou can user the Platform.sh command line client to fully manage your users and integrate this with any other automated system.\nAvailable commands:\n\nuser:add\nAdd a user to the project\n\n\nuser:delete\nDelete a user\n\n\nuser:list (users)\nList project users\n\n\nuser:role\nView or change a user's role\n\n\n\nFor example, the following command would add the 'admin' role to alice@example.com in the current project.\nplatform user:add\n\nThis will present you with an interactive wizard that will allow you to choose precisely what rights you want to give the new user.\n$ platform user:add\n\nEmail address: alice@example.com\nThe user's project role can be 'viewer' ('v') or 'admin' ('a').\nProject role [V/a]:\nThe user's environment-level roles can be 'viewer', 'contributor', or 'admin'.\ndevelopment environment role [V/c/a]:\nsprint1 environment role [V/c/a]:\nhot-fix environment role [V/c/a]:\nmaster environment role [V/c/a]:\npr-2 environment role [V/c/a]:\npr-3 environment role [V/c/a]:\nSummary:\n    Email address: alice@example.com\n    Project role: viewer\n    development: viewer\n    sprint1: viewer\n    hot-fix: viewer\n    pr-2: viewer\n    pr-3: viewer\nAdding users can result in additional charges.\nAre you sure you want to add this user? [Y/n]\nUser alice@example.com created\n\nOnce this has been done, the user will receive an email asking her to confirm her details and register an account name and a password.\nTo give Alice the 'contributor' role on the environment 'development', you could run:\nplatform user:role alice@example.com --level environment --environment development --role contributor\n\nUse platform list to get the full list of commands.\nTransfer ownership\nIf you want to transfer ownership of a project to a different user, first invite that user as a project administrator and then submit a support ticket from the current project owner to ask for the transfer.\nThis action will automatically transfer the subscription charges to the new owner.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Go live", "title": "Example", "url": "/golive/example.html", "documentId": "c89251aaf75d77e7dc0032243425bacb5fed635e", "text": "\n                        \n                            \n                                \n                                \n                                Going Live - Example\nIn this short section we will give you a  very simple, typical example. More involved use-cases (such as site with many domains or multiple applications are simply variations on this).\nSuppose your project ID is abc123 in the US region, and you've registered mysite.com.  You want www.mysite.com to be the \"real\" site and mysite.com to redirect to it.\nConfigure routes.yaml\nFirst, configure your routes.yaml file like so:\n\"https://www.{default}/\":\n  type: upstream\n  upstream: \"app:http\"\n\n\"https://{default}/\":\n  type: redirect\n  to: \"https://www.{default}/\"\n\nThat will result in two domains being created on Platform.sh: master-def456-abc123.eu-2.platformsh.site and www---master-def456-abc123.eu-2.platformsh.site.  The former will automatically redirect to the latter.  In the routes.yaml file, {default} will automatically be replaced with master-def456-abc123.eu-2.platformsh.site.  In domain prefixes (like www), the . will be replaced with ---.\nSet your domain\nNow, add a single domain to your Platform.sh project for mysite.com.  \nUsing the CLI type:\nplatform domain:add mysite.com\nYou can also use the management console for that.\nAs soon as you do, Platform.sh will no longer serve master-def456-abc123.eu-2.platformsh.site at all.  Instead, {default} in routes.yaml will be replaced with mysite.com anywhere it appears when generating routes to respond to.\nYou can still access the original internal domain by running platform environment:info edge_hostname -e master.\nConfigure your DNS provider\nOn your DNS provider, you would create two CNAMEs:\nmysite.com should be an ALIAS/CNAME/ANAME  to master-def456-abc123.eu-2.platformsh.site.\nwww.mysite.com should be a CNAME to master-def456-abc123.eu-2.platformsh.site.\n\n Both point to the same name. See the note above regarding how different registrars handle dynamic apex domains.\n\nResult\nHere's what will now happen under the hood.  Assume for a moment that all caches everywhere are empty.  An incoming request for mysite.com will result in the following:\n\nYour browser asks the DNS network for mysite.com's DNS A record (the IP address of this host).  It responds with \"it's an alias for www.master-def456-abc123.eu-2.platformsh.site\" (the CNAME) which itself resolves to the A record with IP address 1.2.3.4  (Or whatever the actual address is). By default DNS requests by browsers are recursive, so there is no performance penalty for using CNAMEs.\nYour browser sends a request to 1.2.3.4 for domain mysite.com.\nYour router responds with an HTTP 301 redirect to www.mysite.com (because that's what routes.yaml specified).\nYour browser looks up www.mysite.com and, as above, gets an alias for www.master-def456-abc123.eu-2.platformsh.site, which is IP 1.2.3.4.\nYour browser sends a request to 1.2.3.4 for domain www.mysite.com.  Your router passes the request through to your application which in turn responds with whatever it's supposed to do.\n\nOn subsequent requests, your browser will know to simply connect to 1.2.3.4 for domain www.mysite.com and skip the rest.  The entire process takes only a few milliseconds.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Integrations", "title": "Health notifications", "url": "/administration/integrations/notifications.html", "documentId": "0fce09a72ff5ec543c9c6d1afb8336246df5e5ac", "text": "\n                        \n                            \n                                \n                                \n                                Health Notifications\nPlatform.sh can notify you when various events happen on your project, in any environment.  At this time the only notification provided is a low disk space warning, but others may be added in the future.\n\nNote\nRemember that you must have admin access to a project in order to add or modify an integration.  See User administration roles for more details.\n\nDefault low-disk email notifications\nWhen you create a new project, Platform.sh creates a default low-disk email notification for all Project Admins.\n\nNote\nAll projects created prior to 6 April 2020 that did not have any health notifications enabled had an email notification added for admin users.\n\nAvailable notifications\nLow-disk warning\nPlatform.sh monitors disk space usage on all applications and services in your cluster.\n\nIf and when available disk space drops below 20%, a warning notification is generated.\nIf and when available disk space drops below 10%, a critical notification is generated.\nIf and when available disk space goes back above 20% after previously having been lower, an all-clear notification is generated.\n\nNotifications are generated every 5 minutes, so there may be a brief delay between when the threshold is crossed and when the notification is triggered.\nConfiguring notifications\nHealth notifications can be set up via the Platform.sh CLI, through a number of different channels.\nEmail notifications\nA notification can trigger an email to be sent, from an address of your choosing to one or more addresses of your choosing.\nYou can view an email notification by running platform integration:get.\nplatform integration:get\n+--------------+---------------+\n| Property     | Value         |\n+--------------+---------------+\n| id           | abcdefghijklm |\n| type         | health.email  |\n| role         |               |\n| from_address |               |\n| recipients   | - '#admins'   |\n+--------------+---------------+\n\nTo edit the recipients that receive the default email notification, use the integration:update command.\nplatform integration:update abcdefghijklm --recipients you@example.com\n\nThe recipients field may be any valid email address, or one of the following special values.\n\n#admins maps to all project admins and up.\n#viewers maps to everyone with access to the project.\n\nTo add a new email notification, register a health.email integration as follows:\nplatform integration:add --type health.email --from-address you@example.com --recipients them@example.com --recipients others@example.com\n\nThe from-address is whatever address you want the email to appear to be from.  You must specify one or more recipients, each as its own switch.  It is completely fine to use the same email address for both from-address and recipients.\nSlack notifications\nA notification can trigger a message to be sent to a Slack bot.  First, create a new custom \"bot user\" for your Slack group and configure the channels you wish it to live in.  Note the API token is the \"Bot User OAuth Access Token\" provided by Slack.\nThen register that Slack bot with Platform.sh using a health.slack integration:\nplatform integration:add --type health.slack --token YOUR_API_TOKEN --channel '#channelname'\n\nThat will trigger the corresponding bot to post a notification to the #channelname channel in your Slack group.\nPagerDuty notifications\nA notification can trigger a message to be sent via PagerDuty, if you are using that service.  First, create a new PagerDuty \"integration\" that uses the Events API v2.  Copy the \"Integration Key\" as known as the \"routing key\" for the integration.\nNow register a health.pagerduty integration as follows:\nplatform integration:add --type health.pagerduty --routing-key YOUR_ROUTING_KEY\n\nAny notification will now trigger an alert in PagerDuty.\nWebhooks notifications\nA notification can trigger a message to be sent to a web endpoint.\nTo do so, register a health.webhook integration as follows:\nplatform integration:add --type health.webhook --url=A-URL-THAT-CAN-RECEIVE-THE-POSTED-JSON\n\nAny notification will now be posted to the health.webhook URL.\nIn order to let you verify that requests are coming from the integration, you can use the optional shared-key parameter which will add a X-JWS-Signature request header containing the JSON Web Token Signature in JWS Compact Serialization with Unencoded Detached Payload (RFC7797).\nplatform integration:add --type health.webhook --url=A-URL-THAT-CAN-RECEIVE-THE-POSTED-JSON --shared-key JWS-SYMMETRIC-KEY\n\nThe signature is calculated using the given shared-key and the fixed header:\n{\"alg\":\"HS256\",\"b64\":false,\"crit\":[\"b64\"]}\n\nA simplified example payload with the corresponding signature might look like the following snippet:\nPOST /health/notifications HTTP/1.0\nHost: www.example.com\nContent-Length: 1495\nContent-Type: application/json\nX-JWS-Signature: eyJhbGciOiJIUzI1NiIsImI2NCI6ZmFsc2UsImNyaXQiOlsiYjY0Il19..fYW9qrjShmEArV17Z1kH6yudoXzpBE3PzJXq_OqrIfM\n\n{...request body...}\n\nSignature verification is a simple 2 step process:\n# 1. Compute JWS Compact Serialization with Unencoded Detached Payload\n\nfrom jwcrypto import jws, jwk\n\nrfc7797_u_header = '{\"alg\":\"HS256\",\"b64\":false,\"crit\":[\"b64\"]}'\njson_web_key = jwk.JWK(kty=\"oct\", k=\"JWS-SYMMETRIC-KEY\")\n\nsig = jws.JWS(request.body())\nsig.add_signature(json_web_key, protected=rfc7797_u_header)\nsig.detach_payload()\n\n# 2. Verify the signature\n\nassert sig.serialize(compact=True) == request.headers[\"X-JWS-Signature\"]\n\nPlease refer to the JOSE Cookbook for examples about protecting content using JavaScript Object Signing and Encryption (JOSE).\nValidate the integration\nYou can then verify that your integration is functioning properly using the CLI command\n$ platform integration:validate\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Go live", "title": "Troubleshooting", "url": "/golive/troubleshoot.html", "documentId": "a4da489922c28a1ba8c6e6782a38a5a6a63a7ba6", "text": "\n                        \n                            \n                                \n                                \n                                Going Live - Troubleshooting\nIf all steps above have been followed and the site still does not resolve (after waiting for the DNS update to propagate), here are a few simple self-help steps to take before contacting support.\nVerify DNS\nOn the command line with OS X or Linux (or using the Linux subsystem for Windows) type host www.example.com:\nThe response should be something like:\nwww.example.com is an alias for master-t2xxqeifuhpzg.eu.platform.sh.\nmaster-t2xxqeifuhpzg.eu.platform.sh has address 54.76.136.188\n\nIf it is not either you have not configured correctly your DNS server, or the DNS configuration did not propagate yet. As a first step you can try and remove your local DNS cache. \nYou can also try to set your DNS server to the Google public DNS server (8.8.8.8/8.8.4.4) to see if the issue is with the DNS server you are using.\nTry to run ping www.example.com (with you own domain name) if the result is different from what you got form the host www.example.com you might want to verify your /etc/hosts file (or its windows equivalent), you might have left there an entry from testing.\n\nVerify SSL\nOn the command line with OS X or Linux (or using the Linux subsystem for Windows) type curl -I -v  https://example.com (again using your own domain):\nThe response should be long. Look for error messages. They are usually explicit enough. Often the problem will be with a mismatch between the certificate and the domain name.\nVerify your application\nOn the command line type platform logs app and see there are no clear anomalies there. Do the same with platform logs error\nSomething still wrong ? Contact support\nWe are here to help. Please include as much detail as possible (we will be able to provide quicker help).\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "CDN", "title": "Cloudflare", "url": "/golive/cdn/cloudflare.html", "documentId": "c3aa1f579ab616d6df1f68ec43edf6459bed9619", "text": "\n                        \n                            \n                                \n                                \n                                Cloudflare configuration\nOne of the main features that a modern DNS provider needs to have in order to work well with Platform.sh is somethat that's colloquially known as \"Cname Flattening\".  This solves the problem of being able to point your \"root domain\" (example.com) to a domain name (CNAME) rather than an IP address (A record).  This post explains it well.\nIn order to correctly point DNS to your Platform.sh project, you'll need at the very least the master environment CNAME, in other words the domain of your site before you add a custom domain on the management console for that project (or otherwise in the CLI).  This is the value you would get from Step 2 of the pre-launch checklist.\nAssuming that you are using both a www. subdomain as well as the bare domain, you'll want to point both of those DNS entries to the same place. Whether you choose the bare domain version or the www subdomain doesn't make any practical difference, as they both will reach Platform.sh and be handled correctly.\nEnable \"Full SSL\" option in the Cloudflare admin\nCloudflare also makes it very simple to use their free TLS/SSL service to secure your site via HTTPS, while also being behind their CDN if you so choose.  If you decide to use Cloudflare's CDN functionality in addition to their DNS service, you should be sure to choose the \"Full SSL\" option in the Cloudflare admin.\nThis means that traffic to your site is encrypted from the client (browser) to Cloudflare's servers using their certificate, and also between Cloudflare's servers and your project hosting here at Platform.sh, mostly like using your project's Let's Encrypt certificate.\n# Cloudflare's Full SSL option\n           https                       https\nUser &lt;---------------&gt; Cloudflare &lt;-------------&gt; Platform.sh\nThe other option known as \"Flexible SSL\" will cause issues if you intend to redirect all traffic to HTTPS.  The \"Flexible SSL\" option will use Cloudflare's TLS/SSL certificate to encrypt traffic between your users and the CDN, but will pass requests from the CDN back to your project at Platform.sh via HTTP.  This can make it easy for sites that don't have a TLS/SSL certificate to begin ofering their users a more secure experience, by at the least eliminating the unencrypted attack vector on the the \"last mile\" to the user's browser.\n# Cloudflare's Flexible SSL option\n           https                       http\nUser &lt;---------------&gt; Cloudflare &lt;-------------&gt; Platform.sh\nThis will cause all traffic from Cloudflare to your project to be redirected to HTTPS, which will set off an endless loop as HTTPS traffic will be presented as HTTP to your project no matter what.\nIn short: Always use \"Full SSL\" unless you have a very clear reason to do otherwise\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "CDN", "title": "Fastly", "url": "/golive/cdn/fastly.html", "documentId": "0f86e3f96767de51635d99579ed76635108d1774", "text": "\n                        \n                            \n                                \n                                \n                                Fastly configuration\nIn some cases you may want to opt to use a CDN such as Fastly rather than the Platform.sh router's cache.  Using a CDN can offer a better time-to-first-byte for cached content across a wider geographic region at the cost of the CDN service.\nA Fastly CDN is included for Platform.sh Dedicated instances.  Platform.sh does not offer an integrated CDN on self-service Grid projects at this time, but it is a common choice for customers to self-configure.\nLaunching a Platform.sh site with Fastly in front of it is nearly the same as launching normally.  There are only two notable differences.\nNote that individual applications may have their own Fastly setup instructions or additional modules.  Consult the documentation for your application for specific details.\nSet the Platform.sh domain on Fastly\nRather than create a DNS CNAME for your Platform.sh master branch (for instance master-7rqtwti-qwertyqwerty.eu.platform.sh), configure Fastly to respond to requests for your domain name and to treat the Platform.sh master branch as its backend server.  Be sure to enable TLS for the backend connection to Platform.sh.  Then configure your DNS to point your domain at Fastly instead.  See the Fastly documentation for further details.\nDNS TXT records\nIf using the Fastly CDN that is included with a Platform.sh Enterprise subscription, You will need to obtain a DNS TXT record from your Customer Support Engineer prior to going live.  You will need to enter that as a DNS TXT record with your domain registrar.  This step should be done well in advance of the actual go-live.\nAnycast\nYou have the option of using either a CNAME or a set of Anycast IP addresses.  Fastly prefers that you use the CNAME but either work.  If using the Anycast IP addresses on a Dedicated production environment, open a support ticket with the new A records to provide to our support team.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Go live", "title": "Content Delivery Networks", "url": "/golive/cdn.html", "documentId": "a070e59c98996bbcb2d9a1a46d7c0678a711562d", "text": "\n                        \n                            \n                                \n                                \n                                Content Delivery Networks\nPlatform.sh Enterprise plans (both Grid and Dedicated) include a Fastly CDN account by default, which will be managed by Platform.sh.  Our experience has shown that effective caching can mean a huge difference in the perceived performance of an application by its users, and that placing the caches closer to your users (wherever they may be) is the best solution currently available.\nSelf-Service Grid plans do not include a CDN by default, but you are welcome to configure one yourself.  See our guidelines for when and if to use a CDN for HTTP caching.\nWe have partnerships with a variety of CDN vendors depending on your application\u2019s needs.  Our recommended CDN provider is Fastly.\nDNS management\nThe distributed nature of most CDNs means that for proper functioning, any domains that you intend to make use of the CDN will be required to use CNAME records for pointing the DNS entries.  Pointing the root domain (example.com) at a CNAME record is not possible for all DNS hosts, so you will need to confirm this functionality or migrate to a new DNS host.  CloudFlare has a more detailed writeup of the challenges of root CNAMEs.\nIn the event that you and your team choose a pure Fastly solution, this is negated by their providing a set of Anycast IP addresses for you.  This allows you to create A records for your root domain that will point to Fastly\u2019s CDN.\nInitial setup\nFor Enterprise-Dedicated plans, CDN setup is handled by Platform.sh as part of your onboarding.  After the application is stood up on its Dedicated VMs we can begin the collaborative process of provisioning the CDN and configuring DNS and caching setup. We provide CDN services for both staging and production.\nFor self-service Grid plans, the setup can be done at any time by the customer.\nCache configuration\nDepending on which CDN is decided as part of the pre-sales analysis, there may be varying levels of flexibility with regard to caching and ongoing cache invalidation.  This should be discussed between your sales representative and senior technical members of your team if there are concerns with CDN configuration and functionality.\nIf using Fastly as a CDN, it is possible to provide either custom VCL snippets or a full custom VCL file.  Platform.sh will grant customers access to do so upon request.  However, be aware that downtime caused by custom VCL configuration will not be covered by the SLA, just as application code in your repository is not covered by the SLA.\nTLS encryption\nSecurity and the related topic of encryption of data are fundamental principles here at Platform.sh, and as such we provide TLS certificates in the default Enterprise-Dedicated package.  This allows for encryption of all traffic between your users and your application.  By default we will provision a shared certificate with the chosen CDN vendor.  If you opt for the Global Application Cache, we will provision certificates for both the site subdomain (www) and the asset/CDN subdomain.  We use wildcard certificates to secure production, staging, and any other subdomains simultaneously.  If you need Extended Validation TLS certificates you will need to provide your own from an issuer of your choice that we can install for you.\nIf you need to provide your own TLS certificate, place the certificate, the unencrypted private key, and the necessary certificate chain supplied by your TLS provider in your application's private directory (not web accessible), and then open a ticket to let our team know to install it.\nPlatform.sh Enterprise-Dedicated supports a single TLS certificate on the origin. Support for multiple certificates is offered only through a CDN such as CloudFront or Fastly. Self-signed certificates can optionally be used on the origin for development purposes or for enabling TLS between the CDN and origin.\nAll TLS certificates used with CloudFront MUST be 2048 bit certificates.  Larger sizes will not work.\nWeb Application Firewall &amp; Anti-DDoS\nAll Platform.sh-hosted sites, either Grid or Dedicated, live on infrastructure provided by major cloud vendors.  These vendors include their own Level 3 DDoS protection that is sufficient for the vast majority of cases.\nCustomers are welcome to put their own WAF in front of a Dedicated cluster or add other security measures not included in the offering.\nThe router cache\nWhen using a CDN the Platform.sh router's HTTP cache becomes redundant.  In most cases it's best to disable it outright.  Modify your route in .platform/routes.yaml like so to disable the cache:\n\"https://{default}/\":\n    type: upstream\n    upstream: \"app:http\"\n    cache:\n        # Disable the HTTP cache on this route. It will be handled by the CDN instead.\n        enabled: false\n\nPreventing direct access\nWhen using a CDN, you might not want users to access your Platform.sh origin directly. There are three ways to secure your origin.\nPassword protected HTTP Authentication\nYou can password protect your project using HTTP access control.\nMake sure that you generate a password of sufficient strength. You can then share the password with your CDN provider. Make sure the CDN adds a header to authenticate correctly to your origin.\nAdd a custom header to the origin request with the base64 encoded username:password.\nFor example: Aladdin:OpenSesame would become Authorization: Basic QWxhZGRpbjpPcGVuU2VzYW1l.\nBe aware that this approach will apply the same user and password to all development environments, too.  You can have developers enter credentials through their browser, or override the access control setting for each child environment.\n\nnote\nThis is the recommended approach for CloudFlare.\n\nIP whitelisting\nIf your CDN does not support adding headers to the request to origin, you can allow the IP addresses of your CDN.\n\nnote\nYou WILL have to update your configuration when your CDN updates their IP addresses.\n\nList of IP ranges for:\n\nCloudFlare\nFastly\n\nBe aware that this approach will apply the same IP restrictions to all development environments, too.  To remove it from development environments, you will need to disable it on each environment or else create a single child of master where it is disabled, and them make all development branches off of that environment.\nClient authenticated TLS\nIf your CDN offers this option, an alternative way of securing the connection is client authenticated TLS.\nnote: Please remember to permit your developers to access the origin by creating your own certificate or else they won't be able to access the project url directly. (see below)\nCloudFlare has a very good article on what client authenticated TLS is, and how to set this up.\nTo activate authenticated TLS follow the following steps:\n\nDownload the correct certificate from your CDN provider.\nCloudFlare\nCaveat! an attacker could make a Cloudflare account to bypass your origin restriction. For CloudFlare, using the HTTP access control described above is the recommended way of securing your origin.\n\n\nFastly\n\n\nMake sure you have a .crt file. If you have have .pem file, simply rename it to cdn.crt\nAdd the cdn.crt to your git repository\nAdd the relevant configuration to your .platform.app.yaml filetls:\n  client_authentication: \"require\"\n  client_certificate_authorities:\n      - !include\n          type: string\n          path: cdn.crt\n\n\n\nnote\nThe steps above are generally similar but can vary for different CDN providers. Contact your CDN provider's support department for specific assistance.\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Go live steps", "title": "Custom TLS", "url": "/golive/steps/tls.html", "documentId": "4e211357824100887c638be796308acb65a662c5", "text": "\n                        \n                            \n                                \n                                \n                                (Optional) Configure a third-party TLS certificate\nPlatform.sh automatically provides standard TLS certificates issued by Let's Encrypt to all production instances. No further action is required to use TLS-encrypted connections beyond specifying HTTPS routes in your routes.yaml file.\nAlternatively, you may provide your own third party TLS certificate from the TLS issuer of your choice at no charge from us.  Please consult your TLS issuer for instructions on how to generate an TLS certificate.\nA custom certificate is not necessary for development environments.  Platform.sh automatically provides wildcard certificates that cover all *.platform.sh domains, including development environments.\n\nnote\nThe private key should be in the old style, which means it should start with BEGIN RSA PRIVATE KEY. If it starts with BEGIN PRIVATE KEY that means it is bundled with the identifier for key type. To convert it to the old-style RSA key:\nopenTLS rsa -in private.key -out private.rsa.key\n\nAdding a custom certificate through the management console\nYou can add a custom certificate via the Platform.sh management console. In the management console for the project go to Settings and click Certificates on the left hand side. You can add a certificate with the Add button at the top of the page. You can then add your private key, public key certificate and optional certificate chain.\n\nAdding a custom certificate through the CLI\nExample:\nplatform domain:add secure.example.com --cert=/etc/TLS/private/secure-example-com.crt --key=/etc/TLS/private/secure-example-com.key\n\nSee platform help domain:add for more information.\n\n\nSuccess!\nYour site should now be live, and accessible to the world (as soon as the DNS propagates).\n\nIf something is not working see the troubleshooting guide for common issues.  If that doesn't help feel free to contact support.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Go live steps", "title": "DNS and CNAMEs", "url": "/golive/steps/dns.html", "documentId": "01499532a6daa9fd440b3f6fa10032494bddad51", "text": "\n                        \n                            \n                                \n                                \n                                DNS management and Apex domains\nPlatform.sh expects you to use a CNAME for all DNS records.  However, that is problematic with some DNS registrars.\nWhy CNAMEs?\nPlatform.sh is a cloud hosting provider.  That means each individual \"site\" is not its own computer but a set of containers running on one or more virtual machines, which are themselves running on any number of physical computers, all of which are shared with other customers running the same configuration.  An entire region of projects runs behind our dedicated, high-performance edge routers, which are responsible for mapping incoming requests to the particular container on a particular host that is appropriate.\nAll of that logic is quite robust and fast, but it does require that incoming requests all get sent first to the edge routers.  While the IP addresses of the edge routers are fairly stable, they are not guaranteed to never change.  We also may add or remove routers to help scale the region, or take them offline one at a time for upgrades and maintenance.  It is therefore critical that inbound requests always know what the IPs are of the edge routers at the time of the request.\nAll of Platform.sh's \"edge hostnames\" (the auto-generated URLs in the form &lt;branch&gt;-&lt;hash&gt;-&lt;project_id&gt;.&lt;region&gt;.platformsh.site) are DNS records we control that resolve to the IP addresses of the edge routers for that region.  If an edge router is updated, taken out of rotation, etc. then those domains will update quickly and automatically with no further action required.\nAn A record pointed at the same IP addresses would need to be updated manually every time an edge router changes or is temporarily offline.  That means every time Platform.sh is doing routine maintenance or upgrades on the edge routers there's a significant potential for a site to experience a partial outage if a request comes in for an offline edge router.\nWe don't want that.  You don't want that.  Using a CNAME DNS record pointing at the \"edge hostname\" will avoid that problem, as it will be updated almost immediately should our edge router configuration change.\nWhy are CNAME records problematic?\nThe DNS specification was originally published in 1987 in RFC 1034 and RFC 1035, long before name-based HTTP hosting became prevalent.  Those RFCs plus the many follow-ups to clarify and expand on it are somewhat vague on the behavior of CNAME, but it's generally understood that an apex domain (example.com) may not be used as an alias in a CNAME record.  That creates a problem if you want to use an apex domain with any container-based managed hosting service like Platform.sh, because of the point above.\nThere's a detailed thread on the subject that provides more technical detail.\nHandling Apex domains\nThere are a number of ways of handling the CNAME-on-Apex limitation of DNS.\nUsing a DNS provider with custom records\nMany DNS providers have found a way around the CNAME-on-Apex limitation.  Some DNS registrars now offer custom, non-standard records (sometimes called ANAME or ALIAS) that you can manage like a CNAME but will do their own internal lookup behind the scenes and then respond to DNS lookups as if they were an A record.  As these are non-standard their behavior (and quality) can vary, and not all DNS registrars offer such a feature.\nIf you want your site to be accessible with https://example.com and not only https://www.example.com this is the best way to do so.  Examples of such workaround records include:\n\nCNAME Flattening at CloudFlare  \nANAME at easyDNS, DNS Made Easy, or Name.com \nALIAS at DNSimple or Cloudns\n\nPlatform.sh recommends ensuring that your DNS Provider supports dynamic apex domains before registering your domain name with them.  If you are using a DNS Provider that does not support dynamic apex domains then you will be unable to use example.com with Platform.sh, and will need to use only www.example.com (or similar) instead.\n(Alternate) Using a DNS provider with apex domain forwarding\nIf you are willing to make the www. version of your site the canonical version (which is recommended), some registrars or DNS providers may provide a domain redirect feature\u2014also known as domain forwarding\u2014from the apex domain example.com to www.example.com.  Before looking to change registrars, check whether your current provider supports both domain forwarding for the Apex and the DNS CNAME record to Platform.sh for the www. at the same time.  The following DNS providers are known to support both apex forwarding and advanced DNS configurations simultaneously:\n\nNamecheap\n\n(Alternate) Using a www redirection service\nIf your preferred registrar/DNS provider doesn't support either custom records or the apex domain forwarding options above, the following free services both allow blind redirects and allow you to use a CNAME record to Platform.sh for www.example.com and an A record to their service at example.com, which will in turn send a redirect.\n\nWWWizer\nredirectssl\n\n(Alternate) Using A records\nIf you absolutely cannot use a DNS provider that supports aliases or a redirection service, it is possible to use A records with Platform.sh.  They will result in a sub-optimal experience, however.\nThis process has a few limitations:\n\nShould we ever need to change one of those IPs your configuration will need to be manually updated.  Until it is some requests will be lost.\nDirectly pointing at the edge routers bypasses their load-balancing functionality.  Should one of them go offline for maintenance (as happens periodically for upgrades) approximately 1/3 of requests to your site will go to the offline router and be lost, making the site appear offline.\n\n\nFor that reason using A records is strongly discouraged and should only be used as a last resort.\n\nSee the Public IP list for the 3 Inbound addresses for your region.  In your DNS provider, configure 3 separate A records for your domain, one for each of those IP addresses.  Incoming requests will then pick one of those IPs at random to use for that request.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Go live", "title": "Go-live steps", "url": "/golive/steps.html", "documentId": "5451d9b1fc1e0c8ff6faae1506e4e730fd38d50c", "text": "\n                        \n                            \n                                \n                                \n                                Going Live - Steps\nGoing live on Platform.sh is a simple two or three step process.\n\nYou can either use the Platform.sh management console or the CLI to configure your project for production. Once you have gone through it once the whole process usually takes a couple of minutes.\n\nThe order of operations is not really important, but if you are migrating a site from an existing provider, you should first configure the domain on the Platform.sh side, and only then switch DNS over.\n\n1. Change your plan to a production plan\nIf you are on a Development plan, you cannot add a domain. You will need to upgrade your subscription to a production plan.\nGo to your account, click on the small wheel next to you project's name and click on edit.\n\nYou can also access information about the project's plan under \"Billing\", and then by selecting the project from your list of projects. You can make changes to the project by clicking \u2018Upgrade Plan\u2019.\n\nYou can make changes to the type of plan, the number of environments, amount of storage and number of users here. When you make changes, it will update the monthly price you will be paying. Click Upgrade plan to save the new settings.\n\nYou can find more information on pricing on the pricing page.\n2. (CDN version) Configure your DNS provider\nIf you are serving the site through a CDN, configure your DNS provider to point at your CDN account.  The address or CNAME to set for that will vary with the CDN provider.  Refer to their documentation or to the CDN guide.\n2. (Non-CDN version) Configure your DNS provider\nConfigure your DNS provider to point your domain to your Platform.sh Master environment domain name.\nThe way to do so will vary somewhat depending on your registrar, but nearly all registrars should allow you to set a CNAME.  Some will call it an Alias or similar alternate name, but either way the intent is to say \"this domain should always resolve to... this other domain\".\nYou can access the CNAME target by running platform environment:info edge_hostname.  That is the host name by which Platform.sh knows your environment.  Add a CNAME record from your desired domain (www.example.com) to the value of the edge_hostname.\nIf you have multiple domains you want to be served by the same application you will need to add a CNAME record for each of them.\nNote that depending on your registrar and the TTL you set, it could take anywhere from 15 minutes to 72 hours for the DNS change to fully propagate across the Internet.\nIf you are using an apex domain (example.com), see the additional information about Apex domains and CNAME records.\n3. (Non-CDN version) Set your domain in Platform.sh\n\nnote\nIf using a CDN, skip this step.  The CDN should already have been configured in advance to point to Platform.sh as its upstream.\n\nThis step will tell the Platform.sh edge layer where to route requests for your web site. You can do this through the CLI with platform domain:add example.com or  using the managment console.\nYou can add multiple domains to point to your project. Each domain can have its own custom SSL certificate, or use the default one provided.\nIf you require access to the site before the domain name becomes active you can create a hosts file entry on your computer and point it to the IP address that resolves when you access your master project branch.\nTo get the IP address, first run platform environment:info edge_hostname.  That will print the \"internal\" domain name for your project.  Run ping &lt;that domain name&gt; to get its IP address.\nIn OS X and Linux you can add that IP  to your /etc/hosts file.  In Windows the file is named c:\\Windows\\System32\\Drivers\\etc\\hosts. You will need to be a admin user to be able to change that file. So in OS X you will usually run something like sudo vi /etc/hosts. After adding the line the file will look something like:\n\nAlternatively there is also an add-on for Firefox and Google Chrome that allow you to dynamically switch DNS IP addresses without modifying your hosts file.\n\nFirefox LiveHosts add-on \nGoogle Chrome LiveHosts add-on\n\n\nnote\nDo not put the IP address you see here, but the one you got from the ping command.\nAlso, remember to remove this entry after you have configured DNS!\n\nSometimes it can take Let's Encrypt a couple of minutes to provision the certificate the first time. This is normal, and only means the first deploy after enabling a domain may take longer than usual.  Setting the CNAME record with your DNS provider first helps to minimize that disruption.\n4. Bonus steps (Optional)\nConfigure health notifications\nWhile not required, it's strongly recommended that you set up health notifications to advise you if your site is experiencing issues such as running low on disk space.  Notifications can be sent via email, Slack, or PagerDuty.\nConfigure production cron tasks\nIt's strongly recommended that you set up automatic backups and automatic certificate renewal cron tasks.  You will first need to set up an API token and install the CLI as part of the build hook.  Then you can easily configure the appropriate cron tasks.  The following snippet is generally sufficient but see the the links above for more details, and please modify the cron schedules listed to match your use case.\ncrons:\n    backup:\n        # Take a backup automatically every night at 3 am (UTC).\n        spec: '0 3 * * *'\n        cmd: |\n            if [ \"$PLATFORM_BRANCH\" = master ]; then\n                platform backup:create --yes --no-wait\n            fi\n    renewcert:\n        # Force a redeploy at 8 am (UTC) on the 14th and 28th of every month.\n        spec: '0 8 14,28 * *'\n        cmd: |\n            if [ \"$PLATFORM_BRANCH\" = master ]; then\n                platform redeploy --yes --no-wait\n            fi\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Go live", "title": "Pre-Launch Checklist", "url": "/golive/checklist.html", "documentId": "907da6f25a2a4fd6104c873a4f274f8a8e68afe7", "text": "\n                        \n                            \n                                \n                                \n                                Going Live - Pre-Launch Checklist\nBefore you can take your site live there are a few preparation steps to take.\n\n1. Register a domain name with a supported provider\nYou have a domain name registered for your site with a Registrar of your choice. The registrar must allow you to use CNAMEs for your domain.  (Some registrars may call these Aliases or similar.). If your domain is currently active elsewhere, the Time-To-Live (TTL) on your domain is set to the lowest possible value in order to minimize transition time.\n\nnote\nYou will not be able to use a A record. Verify your DNS provider supports CNAMES. (If it does not you will want to run away from it anyway). Also you will be much happier if it supports Apex domains (more in the next chapter).\n\n2. Test your site!\nMake sure your site is running and configured as you want it to be, on your master branch.  In particular, see the Routes documentation. You will need your routes configured appropriately before you begin.  Make sure you have turned off basic-authentication if it was turned on during development.\nIf your production environment is on a Dedicated instance, ensure that the code is up to date in both your staging and production branches, as those are what will be mirrored to the Dedicated instances.  Also ensure that the data on the production instance is up to date and ready to launch.\n3. Optionally obtain a 3rd party TLS certificate\nPlatform.sh automatically provides TLS certificates for all sites issued by Let's Encrypt at no charge.  In most cases this is sufficient and no further action is necessary.  However, if you want to use a 3rd party TLS certificate to encrypt your production site you can obtain one from any number of 3rd party TLS issuers.  Platform.sh does not charge for using a 3rd party TLS certificate, although the issuer may.\nPlatform.sh supports all kinds of certificates including domain-validated certificates, extended validation (EV) certificates, high-assurance certificates and wildcard certificates.  The use of HA or EV certificates is the main reason why you may wish to use a third party issuer rather than the default certificate.  You will also need a custom certificate if you use wildcard routes, as Let's Encrypt does not support wildcard certificates.\nIf you do wish to use a 3rd party certificate, ensure it is purchased and active prior to going live.\n4. Optionally configure your CDN\nIf you are using a CDN, either one included with an Enterprise plan or one you provide for a self-service Grid project, ensure that your CDN account is registered and configured in advance.  That includes setting the upstream on your CDN to point to the Platform.sh production instance.  \n\nFor a Grid-based project, that will be the master-xxxx domain.  Run platform environment:info edge_hostname to get the domain name to use.\nFor a Dedicated project, the upstream to use will be provided by your Platform.sh onboarding representative.\n\nConsult your CDN's documentation for how to set the CDN's upstream address.\nFor Enterprise plans you may need to obtain a DNS TXT record from your Platform.sh support representative by opening a ticket.  Consult the documentation for your CDN provider and our own CDN guide.\n\n\nDomain name is registered?\nYour DNS TTL is set as low as possible?\nYour code and data is tested and ready to launch on the master (Grid) or production (Dedicated) branch?\nYour custom TLS certificate is purchased, if you're using one?\nYour CDN is configured to serve from Platform.sh, if you're using one?\n\nTime to Go Live.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Development", "title": "FAQ", "url": "/development/faq.html", "documentId": "d4fa532384800ba85a342b261e47dd141e3300b8", "text": "\n                        \n                            \n                                \n                                \n                                Frequently Asked Questions (FAQ)\nWhat is the difference between a Platform, a Project and an Environment?\nPlatform or Platform.sh is the infrastructure which is running all your projects.\nA project is the site that you're working on. Each project can contain multiple applications and be deployed in their own environments.\nAn environment is a standalone copy of your site, complete with code, data, and running services. The master branch is the production environment, while any other branch can be setup as an otherwise identical testing environment.\nHow can I cancel my subscription?\nIf you want to delete your project and cancel your subscription, simply go to your user profile and click on \"Edit plan\" on the project you want to delete. Then you can click on the link: \"delete your Platform.sh plan\".\nThis will delete your project and stop invoicing for this project. If you have multiple projects, your subscription will continue until you don't have any projects left.\nDo you support MySQL?\nPlatform.sh uses MariaDB to manage and store your databases. It's a fork of MySQL which is more stable and has more interesting features.\nDoes branching an environment duplicate services?\nYes! Branching an environment creates an exact copy (snapshot) of the parent environment, containing the files, the database, the services...\nDo you have a local writable file-system?\nYes! Unlike other PaaS providers Platform.sh supports non-ephemeral storage. When you configure your application you can tell us what directories you want to be read/write (these are called mounts). These will be mounted on a distributed file system (which is transparent for you). When you back-up your environment they will be backed up as well. When you create a new staging environment... these will be cloned with the rest of your data.\nWhat happens if I push a local branch to my project?\nIf you push a local branch that you created with Git, you create what we call an inactive environment, ie. an environment that is not deployed.\nThis means there won't be any services attached to this branch.\nYou are able to convert an inactive environment into an active environment and vice versa back from the environment configuration page or using the CLI.\nHow does Master  (the live site) scale?\nMaster gets all the resources that are divided into each service (PHP 40%, MySQL 30%, Redis 10%, Solr 20%\u2026). Each Development environment gets the Development plan resources.\nWhat exactly am I SSHing into?\nYou're logged in to the PHP service. It's a read-only file system.\nCan I edit a quick fix on a Platform environment without triggering a rebuild?\nNo ! Since the PHP service you access via SSH is a read-only file system, you'll have to push your fix to be able to test it.\nWhat do I see when I push code?\nWe try to make the log as self-explanatory as possible, so you should see the Git output and also output from the drush make...\nYou can also find it back by clicking on the status of the activity in the Platform.sh management console.\nWhat Linux distribution is Platform.sh using?\nPlatform.sh is built on Debian.\nIf I choose the Development plan, can I use that plan for production?\nThe Development plan provides all the tools to build your website. You can create as many development profiles as you wish for yourself and for your team.\nOnce your project is built and ready for production, you can choose another plan to go live. These plans are listed on the pricing page.\nCan I please use tabs in my YAML files?\nNo.\nI am getting weird errors when I push (something with paramiko..)\nPlease validate the syntax of your YAML file. Don't use tabs. And if all fails, contact support.\nWhich geographic zones does Platform.sh cover?\nPlatform leverages the power of public cloud infrastructures like AWS, Microsoft Azure, or Huawei/Orange. We can deploy your site in a region that is very close to your target audience.\nWhy did you choose the .sh extension for your domain?\n'sh' is the short version of shell.\nAccording to Wikipedia\u2122, in computing, a shell) is a user interface for access to an operating system's services. Generally, operating system shells use either a command-line interface  (CLI) or graphical user interface (GUI).  This is exactly what Platform.sh is about: Giving developers tools to build, test, deploy, and run great websites!\n\".sh\" is also the TLD for Saint Helena that looks like a lovely island, and whose motto is: \"Loyal and Unshakeable\" which we also strive to be.\nIDE Specific Tips\nMAMP pro:\nIn order for MAMP to work well with the symlinks created by the Platform.sh CLI, add the following to the section under Hosts &gt; Advanced called \u201cCustomized virtual host general settings.\u201d For more details visit MAMP Pro documentation page.\n&lt;Directory&gt;\n        Options FollowSymLinks\n        AllowOverride All\n&lt;/Directory&gt;\n\n\nnote\nWhen you specify your document root, MAMP will follow the symlink and substitute the actual build folder path. This means that when you rebuild your project locally, you will need to repoint the docroot to the symlink again so it will refresh the build path.\n\nDo you support two-factor authentication?\nYes we do, and it is easy to enable.  To do so please go to your Account Settings on our Account site. Then click on the left tab called Security which will propose you to enable TFA Application.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Development", "title": "Public IPs", "url": "/development/public-ips.html", "documentId": "5ad5fbbdd6ac1ec3d406c540a03225081a82567b", "text": "\n                        \n                            \n                                \n                                \n                                Public IP addresses\nPlatform.sh regions reach the outside through a limited number of IP addresses.\nUse the inbound IP addresses if you have a corporate firewall which blocks outgoing SSH connections.  In that case, simply add our IP addresses for inbound traffic below to your whitelist.\nEurope\nWest (eu.platform.sh)\nOutbound IPs:\n\n54.72.94.105\n54.76.137.67\n54.76.137.94\n\nInbound IPs (gw.eu.platform.sh):\n\n54.76.137.79\n54.76.137.151\n54.76.136.188\n\nWest 2 (eu-2.platform.sh)\nOutbound IPs:\n\n52.208.123.9\n52.214.63.84\n52.30.200.164\n\nInbound IPs (gw.eu-2.platformsh.site):\n\n34.248.104.12\n34.241.191.143\n52.210.208.94\n\nWest 4 (eu-4.platform.sh)\nOutbound IPs:\n\n18.200.158.188\n18.200.157.200\n18.200.184.206\n\nInbound IPs (gw.eu-4.platformsh.site):\n\n52.215.88.119\n52.208.179.40\n18.200.179.139\n\nGermany 2 (de-2.platform.sh) (Data Location Guarantee)\nOutbound IPs:\n\n35.246.248.138\n35.246.184.45\n35.242.229.239\n\nInbound IP (gw.de-2.platformsh.site):\n\n35.246.248.138\n35.246.184.45\n35.242.229.239\n\nFrance 1 (fr-1.platform.sh)\nOutbound IPs:\n\n90.84.47.148\n90.84.46.222\n90.84.46.40\n\nInbound IPs (gw.fr-1.platformsh.site):\n\n90.84.47.148\n90.84.46.222\n90.84.46.40\n\nUnited Kingdom 1 (uk-1.platform.sh)\nOutbound IPs:\n\n35.242.142.110\n35.189.126.202\n35.242.183.249\n\nInbound IPs (gw.uk-1.platformsh.site):\n\n35.242.142.110\n35.189.126.202\n35.242.183.249\n\nUnited States\nEast (us.platform.sh)\nOutbound IPs:\n\n54.88.149.31\n54.209.114.37\n54.210.53.51\n\nInbound IPs (gw.us.platform.sh):\n\n54.210.49.244\n54.210.55.162\n54.88.225.116\n\nEast 2 (us-2.platform.sh)\nOutbound IPs:\n\n34.238.64.193\n52.4.246.137\n54.157.66.30\n\nInbound IPs (gw.us-2.platformsh.site):\n\n34.226.46.235\n34.238.11.122\n54.89.106.200\n\nCanada\nOutbound IPs:\n\n35.182.24.224\n52.60.213.255\n35.182.220.113\n\nInbound IPs:\n\n35.182.174.169\n35.182.59.77\n52.60.219.22\n\nAustralia (au.platform.sh)\nOutbound IPs:\n\n13.55.135.0\n13.54.121.225\n13.55.215.151\n\nInbound IPs (gw.au.platformsh.site):\n\n13.54.88.239\n13.55.140.143\n13.54.222.56\n\nThese IP addresses are stable, but not guaranteed to never change. Prior to\nany future change, all affected customers will receive ample warning.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Development", "title": "Syncing to Dedicated", "url": "/development/transfer-dedicated.html", "documentId": "33377e0dea9fe078a4c1aa9a5984b421c59e15ee", "text": "\n                        \n                            \n                                \n                                \n                                Transferring data to and from a Dedicated cluster\nBacking up staging and production files\nPlatform.sh automatically creates a backup of the staging and production instances on a Dedicated cluster every six hours.  However, those are only useful for a full restore of the environment and can only be done by the Platform.sh team.  At times you'll want to make a manual backup yourself.\nTo create a manual ad-hoc backup of all files on the staging or production environment, use the standard rsync command.\nrsync -avzP &lt;USERNAME&gt;@&lt;CLUSTER_NAME&gt;.ent.platform.sh:pub/static/ pub/static/\n\nThat will copy all files from the pub/static directory on the production instance to the pub/static directory, relative to your local directory where you're running that command.\nBacking up the staging and production database\nTo backup your database to your local system you'll need to get the database credentials to use.\nFirst, login to the cluster and run the following command:\necho $PLATFORM_RELATIONSHIPS | base64 --decode | json_pp\n\nWhich should give a JSON output containing something like this:\n\"database\" : [\n      {\n         \"path\" : \"main\",\n         \"service\" : \"mysqldb\",\n         \"rel\" : \"mysql\",\n         \"host\" : \"database.internal\",\n         \"ip\" : \"246.0.80.64\",\n         \"scheme\" : \"mysql\",\n         \"cluster\" : \"jyu7wavyy6n6q-master-7rqtwti\",\n         \"username\" : \"user\",\n         \"password\" : \"\",\n         \"query\" : {\n            \"is_master\" : true\n         },\n         \"port\" : 3306\n   }\n]\n\nThe part you want is the user, password, and \"path\", which means the DB name.  Ignore the rest.\nNow, run the following command on your local computer:\nssh &lt;USERNAME&gt;@&lt;CLUSTER_NAME&gt;.ent.platform.sh 'mysqldump --single-transaction -u &lt;user&gt; -p&lt;pass&gt; -h localhost &lt;dbname&gt; | gzip' &gt; database.gz\nThat will run a mysqldump command on the server, compress it using gzip, and stream the output to a file named database.gz on your local computer.\n(If you'd prefer, bzip2 and xz are also available.)\nSynchronizing files from dev to staging/production\nTo transfer data into either the staging or production environments, you can either download it from your Platform.sh Development environment to your local system first or transfer it directly between environments using SSH based tools (e.g. SCP, Rsync).\nFirst, set up SSH forwarding by default for Platform.sh domains.\nThen run platform ssh with the master branch checked out to connect to the master dev environment.  Files are the easier data to transfer, and can be done with rsync.\nrsync -avzP pub/static/ &lt;USERNAME&gt;@&lt;CLUSTER_NAME&gt;.ent.platform.sh:pub/static/\n\nReplace pub/static with the path to your files on system, such as web/sites/default/files/.  Note that rsync is very picky about trailing / characters.  Consult the rsync documentation for more that can be done with that command.\nSynchronizing the database from development to staging/production\nThe database can be copied directly from the development environment to staging or production, but doing so requires noting the appropriate credentials first on both systems.\nFirst, login to the production environment over SSH:\nssh &lt;USERNAME&gt;@&lt;CLUSTER_NAME&gt;.ent.platform.sh\n\nOnce there, you can look up database credentials by running:\necho $PLATFORM_RELATIONSHIPS | base64 --decode | json_pp\n\nWhich should give a JSON output containing something like this:\n{\n   \"database\" : [\n      {\n         \"password\" : \"abc123\",\n         \"username\" : \"projectname\",\n         \"path\" : \"projectname\",\n         \"port\" : \"3306\",\n         \"scheme\" : \"mysql\",\n         \"host\" : \"127.0.0.1\",\n         \"query\" : {\n            \"is_master\" : true,\n            \"compression\" : true\n         }\n      }\n   ]\n}\n\nThe part we want is the host, user, password, and the \"path\", which is the database name.  Ignore the rest.\nNow, in a separate terminal login to the development instance using platform ssh.  Run the same echo command as above to get the credentials for the database on the development instance.  (The JSON will be slightly different but again we're only interested in the user, password, host, and \"path\"/database name).\nWith the credentials from both databases we can construct a command that will export data from the dev server and write it directly to the Dedicated cluster's server.\nmysqldump -u &lt;dev_user&gt; -p&lt;dev_password&gt; -h &lt;dev_host&gt; &lt;dev_dbname&gt; --single-transaction | ssh -C &lt;USERNAME&gt;@&lt;CLUSTER_NAME&gt;.ent.platform.sh 'mysql -u &lt;prod_user&gt; -p&lt;prod_password&gt; -h &lt;prod_host&gt; &lt;prod_dbname&gt;'\n\nThat will dump all data from the database as a stream of queries that will get run on the production database without ever having to create an intermediary file.  The -C on the SSH command tells SSH to compress the connection to save time.\n(Be aware, this is a destructive operation that overwrites data.  Backup first.)\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Development", "title": "Git submodules", "url": "/development/submodules.html", "documentId": "04b7e5a2586748bcf28b451c25e9809f601811ca", "text": "\n                        \n                            \n                                \n                                \n                                Using Git submodules\nClone submodules during deployment\nPlatform.sh allows you to use submodules in your Git repository. They are usually listed in a .gitmodules file at the root of your Git repository. When you push via Git, Platform.sh will try to clone them automatically.\nHere is an example of a .gitmodules file:\n[submodule \"app/Oro\"]\n    path = src/Oro\n    url = https://github.com/orocrm/platform.git\n[submodule \"src/OroPackages/src/Oro/Bundle/EntitySerializedFieldsBundle\"]\n    path = src/OroPackages/src/Oro/Bundle/EntitySerializedFieldsBundle\n    url = https://github.com/orocrm/OroEntitySerializedFieldsBundle.git\n[submodule \"src/OroB2B\"]\n    path = src/OroB2B\n    url = https://github.com/orocommerce/orocommerce.git\n\nWhen you run git push, you can see the output of the log:\nValidating submodules.\n  Updated submodule git://github.com/orocommerce/orocommerce: 4 references updated.\n  Updated submodule git://github.com/orocrm/platform: 229 references updated.\n  Updated submodule git://github.com/orocrm/OroEntitySerializedFieldsBundle: 11 references updated.\n\nError when validating submodules\nIf you see the following error:\nValidating submodules.\n  Found unresolvable links, updating submodules.\n\nE: Error validating submodules in tree:\n    - /src/Oro: Exception: commit 03567c6 not found.\n\n   This might be due to the following errors fetching submodules:\n    - git@github.com:orocommerce/orocommerce.git: HangupException: The remote server unexpectedly closed the connection.\n\nSince the Platform.sh Git server cannot connect to Github via SSH without being granted an SSH key to do so, you should not be using an SSH URL: git@github.com:..., but you should use an HTTPS URL instead: https://github.com/....\nUse of private git repositories\nWhen using Git submodules that are hosted on private repositories, using the https protocol will fail with errors like:\nGitProtocolError: unexpected http resp 401 for https://bitbucket.org/myusername/mymodule.git/info/refs?service=git-upload-pack\nTo fix this, you need to:\n\nChange your .gitmodules file from the HTTPS syntax to the SSH syntax, e.g.\n\nfrom:\n[submodule \"support/mymodule\"]\n    path = support/mymodule\n    url = https://bitbucket.org/myusername/mymodule.git\nto:\n[submodule \"support/mymodule\"]\n    path = support/mymodule\n    url=git@bitbucket.org:myusername/mymodule.git\n\nAdd the SSH public key in the Platform.sh project settings \"Deploy Key\" tab in the Web UI as per the Private Repository documentation page, which will allow our Git service to pull the module from the remote git service. This assumes you have configured the remote git repository to allow this by generating a private/public key pair. For example, see the Bitbucket documentation.\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Development", "title": "Using SSH", "url": "/development/ssh.html", "documentId": "45546887925093cc8c030d774782b9912b379935", "text": "\n                        \n                            \n                                \n                                \n                                Using SSH keys\nOne of the ways Platform.sh keeps things secure is by using SSH behind the scenes. Users can interact with their environment through a command shell, or push changes to the environment's Git repository, and both of these features rely on SSH.\nYou can manage SSH keys through the CLI (see below), or through the SSH keys tab under Account Settings.\nFind your Public-Private Keypair\nIf you use Linux, you probably already have keys. The private key is usually in a file named ~/.ssh/id_rsa and the public key in ~/.ssh/id_rsa.pub,\nSearching for a public key file:\n\nOpen up a command prompt.\nRun the following commands:\n\n$ cd ~/.ssh\n$ ls -a\nid_rsa\nid_rsa.pub\nknown_hosts\nauthorized_keys\n\nIf you find a file named id_rsa.pub, you can use it with Platform.sh. If you don't find an existing key, see the steps to create a new one in the next section.\nCreate a New Public-Private Keypair\n\nnote\nIf you already have a SSH keypair, you can skip this step.\n\nCreate a public-private keypair:\n$ ssh-keygen -t rsa -C \"your_email_address@example.com\"\nssh-keygen generates the key pair and will ask you where you want to save the file:\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/Users/your_username/.ssh/id_rsa):\nThe default location is fine in most cases. Now it's time to create a passphrase. A good, strong passphrase is highly recommended, to make your key less useful if it falls into the wrong hands.\nEnter passphrase (empty for no passphrase): [Type a passphrase]\nEnter same passphrase again: [Type passphrase again]\n\nThat's it. Keys generated! Here are the results:\nYour identification has been saved in /Users/your_username/.ssh/id_rsa.\nYour public key has been saved in /Users/your_username/.ssh/id_rsa.pub.\nThe key fingerprint is:\n55:c5:d7:a9:1f:dc:7a:67:31:70:fd:87:5a:a6:d0:69 your_email_address@example.com\n\n\nnote\nMake note of the location of your public key, you're going to need that in the next section.\n\nAdd the SSH key to your Platform account\nYou have your SSH keys (if not, take a look at the section above), but you need to make sure Platform has a copy of your public key. It's pretty easy to add it to your account.\n\nFirst off, you'll need to copy your public key to the clipboard.\nHead over to your user account page on the Platform.sh Accounts page and navigate to the Account Settings tab.\nIn the left side-bar, select SSH keys.\nClick the Add a public key button.\nPaste the key that you copied earlier into the 'Key' text box. You can also add a title if you like, otherwise it will be auto-generated.\nClick 'Save'.\n\n\n  \n\n\nThat's it! You're all set. Now you'll be able to use Git and command shells with any Platform.sh environment that your user account is authorized to work with.\nForwarding keys by default\nIt may be helpful to set your SSH client to always forward keys to Platform.sh servers, which can simplify other SSH or Rsync commands.  To do so, include a block in your local ~/.ssh/config file like so:\nHost *.us.platform.sh\n       ForwardAgent yes\n\nHost *.eu.platform.sh\n       ForwardAgent yes\nInclude one Host entry for each Platform.sh region you want to connect to, such as us-2 or eu-4.  (You can include other configuration as desired.)\nSSH to your Web Server\nIn the management console header, click on the environment tab and select the environment that you want to SSH into. Then click the SSH dropdown button towards the top right.\n\nCopy the SSH URL of that environment and past the link into your terminal. You should see something like this:\n$ ssh wk5fqz6qoo123-master@ssh.eu.platform.sh\n\n   ___ _      _    __\n  | _ \\ |__ _| |_ / _|___ _ _ _ __\n  |  _/ / _` |  _|  _/ _ \\ '_| '  \\\n  |_| |_\\__,_|\\__|_| \\___/_| |_|_|_|\n\n Welcome to Platform.\n\n This is environment master\n of project wk5fqz6qoo123.\n\nweb@wk5fqz6qoo123-master--php:~$\n\nTroubleshoot SSH\nWhile trying to log in via SSH, this can happen:\n$ ssh [SSH-URL]\nPermission denied (publickey).\n\nDon't panic! It's an issue which can happen for the following reasons:\n\nYour environment is inactive\nYou haven't redeployed (i.e. git push) your environment since adding the new public key\nYou didn't upload your public key to your user profile\nYour SSH private key has not been added into your ssh-agent\nYour SSH key files have incorrect permissions\n\nCheck your public key\nMake sure your public key has been uploaded to your user account.\nCheck your ssh-agent\nCheck that your key is properly added to your SSH agent. This is an authentication agent that manages your private key.\n\nCheck your SSH agent. Run the command ssh-add -l in your terminal:\n\n$ ssh-add -l\n2048 12:b0:13:83:7f:56:18:9b:78:ca:54:90:a7:ff:12:69 /Users/nick/.ssh/id_rsa (RSA)\n\n\nCheck that file name on the right (.ssh/id_rsa in the example above). Does it match your private key file?\nIf you don't see your private key file, add your private key:\n\n$ ssh-add path-to-your-key\n\n\nTry again.\n\nSpecify your identity file\nIf your identity (SSH key) associated with Platform.sh is not in a default file name (as may be explained in your SSH software manual, for example) you may have to append a specification like the one below so that the SSH software finds the correct key.\nHost platform.sh\nIdentityFile ~/.ssh/id_platformsh\nBe aware that, above, platform.sh stands for a hostname. Each different hostname you connect to Platform.sh at may have to be specified in the host line, separated by spaces.\nStill having trouble?\nIf you followed all the steps above, you may also notice an error message similar to below while attempting to SSH to platform.sh:\nHello Your Name, you successfully connected, but you do not have access to service 'xxxxxxxxxxxxxx-master': check permissions.\nReceived disconnect from 54.210.49.244: 14: No more auth methods available\n\nThis usually means a deployment has not been committed yet. When a new key is added, it only becomes immediately active for use with Git. For use with SSH, it will not be activated until a deployment is made. An easy way to force this is to create and push an empty commit:\n$ git commit --allow-empty -m 'force redeploy'\n$ git push origin master\n\nIf all else fails, generate some SSH debug information\nIf your private key and public key both look OK but you don't have any luck logging in, print debugging information. These lines often give clues about what is going wrong.\n\nRun the SSH command with the -v option, like this:\n\n$ ssh -v [SSH-URL]\nOpenSSH_6.7.8, OpenSSL 1.2.3 1 Sep 2014\ndebug1: Connecting to ssh.eu.platform.sh [54.32.10.98] port 22.\ndebug1: Connection established.\ndebug1: identity file /Users/nick/.ssh/id_rsa type 1\n...(30 more lines of this light reading)...\ndebug1: Offering RSA public key: /Users/nick/.ssh/id_rsa\ndebug1: Authentications that can continue: publickey\ndebug1: No more authentication methods to try.\nPermission denied (publickey).\n\nor\n$ GIT_SSH_COMMAND=\"git -v\" git clone [REPO-URL]\n\nYou can use this information to make one last check of the private key file.\nIf you're still stuck, don't hesitate to submit a support ticket, we'll help you solve your problem.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Development", "title": "Private repositories", "url": "/development/private-repository.html", "documentId": "0a771d4d340f79effac45e436cd2e9c5fcc6c2cf", "text": "\n                        \n                            \n                                \n                                \n                                Use private Git repository\nPull code from a private Git repository\nLet's say you're building a module (or theme, library...) which is stored in a private Git repository that you have access to, and you want to use it on your project. Platform.sh allows you to include code dependencies that are stored in external private Git repositories (e.g. from a Drupal .make file, a PHP composer.json file).\nTo grant Platform.sh access to your private Git repository, you need to add the project public SSH key to the deploy keys of your Git repository.\nYou can copy your project's public key by going to the Settings tab on the management console and then clicking the Deploy Key tab on the left hand side. \n\nIf your private repository is on GitHub, go to the target repository's settings page. Go to Deploy Keys and click Add deploy key. Paste the public SSH key in and submit. By default, on github, deploy keys are read only, so you don't need to worry about the system pushing code to the private repository.\nIf you're using Drupal for example, you can now use your private module by adding it to your make file:\n; Add private repository from GitHub\nprojects[module_private][type] = module\nprojects[module_private][subdir] = \"contrib\"\nprojects[module_private][download][type] = git\nprojects[module_private][download][branch] = dev\nprojects[module_private][download][url] = \"git@github.com:guguss/module_private.git\"\n\n\nnote\nIn the make file use the &lt;user&gt;@&lt;host&gt;:&lt;path&gt;.git format, or ssh://&lt;user&gt;@&lt;host&gt;:&lt;port&gt;/&lt;path&gt;.git if using a non-standard port.\n\nUsing multiple private Git repositories\nMore complex projects may have many repositories that they want to include, but GitHub only allows you to associate a deploy key with a single repository.\nIf your project needs to access multiple repositories, you can choose to attach an SSH key to an automated user account. Since this account won't be used by a human, it's called a machine user. You can then add the machine account as collaborator or add the machine user to a team with access to the repositories it needs to manipulate.\nMore information about this is available on\nGitHub.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Development", "title": "Troubleshooting", "url": "/development/troubleshoot.html", "documentId": "eb5723fd47c94a6a7b17fe714a9e8b18880201f3", "text": "\n                        \n                            \n                                \n                                \n                                Troubleshooting\n\n\nForce a redeploy\nClear the build cache\nHTTP responses 502 Bad Gateway or 503 Service Unavailable\nError provisioning the new certificate\nTotal disk usage exceeds project maximum\nLow disk space\nCheck your application's disk space\nIncrease the disk space available\nCheck your database disk space\n\n\nNo space left on device\nMySQL lock wait timeout\nMySQL: definer/invoker of view lack rights to use them\nMySQL server has gone away\nDisk space issues\nWorker timeout\nPacket size limitations\n\n\nERROR: permission denied to create database\n\"Read-only file system\" error\nRootNotFoundException from the CLI\n\"File not found\" in Drupal\nPHP-specific error messages\nserver reached max_children\nExecution timeout\nPHP process crashed\nPHP process is killed\n\n\nStuck build or deployment\nSlow or failing build or deployment\nCheck for errors in the logs\nBuild and deploy hooks\nCron jobs\n\n\n\n\nForce a redeploy\nThere are times where you might want to trigger a redeployment of your application. That can be done with the following command:\nplatform redeploy\n\nDo not trigger a redeploy if there are builds in a \"Pending\" state, as these will block deployment. Wait for those builds to complete.\nClear the build cache\nIn rare circumstances the build cache, used to speed up the build process, may become corrupted.  That may happen if, for example, code is being downloaded from a 3rd party language service like Packagist or NPM while that service is experiencing issues.  To flush the build cache entirely run the following command:\nplatform project:clear-build-cache\n\nThat will wipe the build cache for the current project entirely.  Naturally the next build for each environment will likely be longer as the cache rebuilds.\nHTTP responses 502 Bad Gateway or 503 Service Unavailable\nThese errors indicate your application (or application runner, like PHP-FPM) is crashing or unavailable.  Typical causes include:\n\nYour .platform.app.yaml configuration has an error and the process is not starting or requests are not able to be forwarded to it correctly.  Check your web.commands.start entry or that your passthru configuration is correct.\nThe amount of traffic coming to your site exceeds the processing power of your application.\nCertain code path(s) in your application are too slow and timing out.\nA PHP process is crashing because of a segmentation fault (see below).\nA PHP process is killed by the kernel out-of-memory killer (see below).\n\nError provisioning the new certificate\nOne reason Let's Encrypt certificates may fail to provision on your environments has to do with the 64 character limit Let's Encrypt places on URLs. If the names of your branches are too long, the Platform.sh generated environment URL will go over this limit, and the certificate will be rejected.\nSee Let's Encrypt limits and branch names for a more detailed breakdown of this issue.  \nTotal disk usage exceeds project maximum\nOne of the billable parameters in your project's settings is Storage.  This global storage pool is allocated among the various services and application containers in your project via the disk parameter.  The sum of all disk parameters in your project's YAML config files must be less than or equal to the global project storage number.\nError: Resources exceeding plan limit; disk: 8192.00MB &gt; 5120.00MB; try removing a service, or add more storage to your plan\nThis means that you have allocated, for example, disk: 4096 in a MySQL service in services.yaml and also disk: 4096 in the .platform.app.yaml for your application, while only having the minimum default of 5GB storage for your project as a whole.  The solution is either to lower the disk parameters to within the limits of 5GB of storage, or raise the global storage parameter on your project's settings to at least 10GB.  \nBecause storage is a billable component of your project, only the project's owner can make this change.\nLow disk space\nWhen you receive a low-disk space notification for your application container: \nCheck your application's disk space\nRun platform ssh within your project folder to login to the container's shell.  Then use the df command to check the available writable space for your application.\ndf -h -x tmpfs -x squashfs | grep -v /run/shared\nThis command will show the writable mounts on the system, similar to:\nFilesystem                                                       Size  Used Avail Use% Mounted on\n/dev/mapper/platform-syd7waxqy4n5q--master--7rqtwti----app       2.0G   37M  1.9G   2% /mnt\n/dev/mapper/platform-tmp--syd7waxqy4n5q--master--7rqtwti----app  3.9G   42M  3.8G   2% /tmp\nThe first line shows the storage device that is shared by all of your persistent disk mounts.  All defined mounts use a common storage pool.  In this example, the application container has allocated 2 GB of the total disk space. Of those 2GB, 2% (37 MB) is used by all defined mounts.\nThe second line is the operating system temporary directory, which is always the same size.\n  While you can write to the /tmp directory files there are not guaranteed to persist and may be deleted on deploy.\nIncrease the disk space available\nThe sum of all disk keys defined in your project's .platform.app.yaml and .platform/services.yaml files must be equal or less than the available storage in your plan.\n\nBuy extra storage for your project\nEach project comes with 5GB of Disk Storage available to each environment. To increase the disk space available for your project, click on \"Edit Plan\" to increase your storage in bulks of 5GB.  See Extra Storage for more information.\n\nIncrease your application and services disk space\nOnce you have enough storage available, you can increase the disk space allocated for your application and services using disk keys in your .platform.app.yaml and .platform/services.yaml. \nCheck the following resources for more details:\n\nApplication's disk space\nServices' disk space\n\n\n\nCheck your database disk space\nFor a MariaDB database, the command platform db:size will give approximate disk usage as reported by MariaDB.  However, be aware that due to the way MySQL/MariaDB store and pack data this number is not always accurate, and may be off by as much as 10 percentage points.\n+--------------+--------+\n| Property     | Value  |\n+--------------+--------+\n| max          | 2048MB |\n| used         | 189MB  |\n| percent_used | 9%     |\n+--------------+--------+\nFor the most reliable disk usage warnings, we strongly recommend all customers enable Health notifications on all projects.  That will provide you with a push-notification through your choice of channel when the available disk space on any service drops too low.\nNo space left on device\nDuring the build hook, you may run into the following error depending on the size of your application:\nW: [Errno 28] No space left on device: ...\nThe cause of this issue has to do with the amount of disk provided to the build container before it is deployed. Application images are restricted to 4 GB during build, no matter how much writable disk has been set aside for the deployed application.\nSome build tools (yarn/npm) store cache for different versions of their modules. This can cause the build cache to grow over time beyond the maximum of 4GB. Try clearing the build cache and redeploying. In most cases, this will resolve the issue.\nIf for some reason your application requires more than 4 GB during build, you can open a support ticket to have this limit increased.  The most disk space available during build still caps off at 8 GB in these cases.\nMySQL lock wait timeout\nIf you receive MySQL error messages like this:\nSQLSTATE[HY000]: General error: 1205 Lock wait timeout exceeded;\nThis means a process running in your application acquired a lock from MySQL for a long period of time.  That is typically caused by one of the following:\n\nThere are multiple places acquiring locks in different order. For example, code path 1 first locks record A and then locks record B.  Code path 2, in contrast, first locks record B and then locks record A.\nThere is a long running background process executed by your application that holds the lock until it ends.\n\nIf you're using MariaDB 10+, you can use the SQL query SHOW FULL PROCESSLIST \\G to list DB queries waiting for locks.  Find output like the following, and start debugging.\n&lt; skipped &gt;\nCommand: Query\nTime: ...\nState: Waiting for table metadata lock\nInfo: SELECT ...\n&lt; skipped &gt;\nTo find active background processes, run ps aufx on your application container.\nAlso, please make sure that locks are acquired in a pre-defined order and released as soon as possible.\nMySQL: definer/invoker of view lack rights to use them\nThere is a single MySQL user, so you can not use \"DEFINER\" Access Control mechanism for Stored Programs and Views.\nWhen creating a VIEW, you may need to explicitly set the SECURITY parameter to INVOKER:\nCREATE OR REPLACE SQL SECURITY INVOKER\nVIEW `view_name` AS\nSELECT\nMySQL server has gone away\nDisk space issues\nErrors such as \"PDO Exception 'MySQL server has gone away'\" are usually simply the result of exhausting your existing diskspace. Be sure you have sufficient space allocated to the service in .platform/services.yaml.\nThe current disk usage can be checked using the CLI command platform db:size. Because of issues with the way InnoDB reports its size, this can out by up to 20%. As table space can grow rapidly, it is usually advisable to make your database mount size twice the size reported by the db:size command.\nYou are encouraged to add a low-disk warning notification to proactively warn of low disk space before it becomes an issue.\nWorker timeout\nAnother possible cause of \"MySQL server has gone away\" errors is a server timeout.  MySQL has a built-in timeout for idle connections, which defaults to 10 minutes.  Most typical web connections end long before that is ever approached, but it's possible that a long-running worker may idle and not need the database for longer than the timeout.  In that case the same \"server has gone away\" message may appear.\nIf that's the case, the best way to handle it is to wrap your connection logic in code that detects a \"server has gone away\" exception and tries to re-establish the connection.\nAlternatively, if your worker is idle for too long it can self-terminate.  Platform.sh will automatically restart the worker process, and the new process can establish its own new database connection.\nPacket size limitations\nAnother cause of the \"MySQL server has gone away\" errors can be the size of the database packets. If that is the case, the logs may show warnings like  \"Error while sending QUERY packet\" before the error. One way to resolve the issue is to use the max_allowed_packet parameter described above.\nERROR: permission denied to create database\nThe provided user does not have permission to create databases.The database is created for you and can be found in the path field of the $PLATFORM_RELATIONSHIPS environment variable.\n\"Read-only file system\" error\nEverything will be read-only, except the writable mounts you declare.  Writable mounts are there for your data: for file uploads, logs and temporary files. Not for your code.  In order to change code on Platform.sh you have to go through Git.\nThis is what gives you all of the benefits of having repeatable deployments, consistent backups, traceability, and the magically fast creation of new staging/dev environments.\nIn Platform.sh, you cannot just \"hack production\".  It is a constraint, but it is a good constraint.\nDuring the build phase of your application, the main filesystem is writable.  So you can do whatever you want (e.g. compile code or generate anything you need).  But during and after the deploy phase, the main filesystem will be read-only.\nRootNotFoundException from the CLI\nIf you check out a project via Git directly and not using the platform get command, you may end up with the CLI unable to determine what project it's in.  If you run a CLI command from within the project directory you've checked out but get an error like this:\n[RootNotFoundException] Project root not found. This can only be run from inside a project directory.\nThen the CLI hasn't been able to determine the project to use.  To fix that, run:\nplatform project:set-remote &lt;project_id&gt;\nwhere &lt;project_id&gt; is the random-character ID of the project.  That can be found by running platform projects from the command line to list all accessible projects.  Alternatively, it can be found in the management console after the platform get command shown or in the URL of the management console or project domain.\n\"File not found\" in Drupal\nIf you see a bare \"File not found\" error when accessing your Drupal site with a browser, this means that you've pushed your code as a vanilla project but no index.php has been found.\nMake sure your repository contains an index.php file in the web location root, or that your Drush make files are properly named.\nPHP-specific error messages\nserver reached max_children\nYou may see a line like the following in the /var/log/app.log file:\nWARNING: [pool web] server reached max_children setting (2), consider raising it\nThat indicates that the server is receiving more concurrent requests than it has PHP processes allocated, which means some requests will have to wait until another finishes.  In this example there are 2 PHP processes that can run concurrently.\nPlatform.sh sets the number of workers based on the available memory of your container and the estimated average memory size of each process.  There are two ways to increase the number of workers:\n\nAdjust the worker sizing hints for your project.\nUpgrade your subscription on Platform.sh to get more computing resources. To do so, log into your account and edit the project.\n\nExecution timeout\nIf your PHP application is not able to handle the amount of traffic or it is slow, you should see log lines from /var/log/app.log like any of the below:\nWARNING: [pool web] child 120, script '/app/public/index.php' (request: \"GET /index.php\") execution timed out (358.009855 sec), terminating\nThat means your PHP process is running longer than allowed.  You can adjust the max_execution_time value in php.ini, but there is still a 5 minute hard cap on any web request that cannot be adjusted.\nThe most common cause of a timeout is either an infinite loop (which is a bug that you should fix) or the work itself requires a long time to complete. For the latter case, you should consider putting the task into a background job.\nThe following command will identify the 20 slowest requests in the last hour, which can provide an indication of what code paths to investigate.\ngrep $(date +%Y-%m-%dT%H --date='-1 hours') /var/log/php.access.log | sort -k 4 -r -n | head -20\n\nIf you see that the processing time of certain requests is slow (e.g. taking more than 1000ms), you may wish to consider using a profiler like Blackfire to debug the performance issue.\nOtherwise, you may check if the following options are applicable:\n\nFind the most visited pages and see if they can be cached and/or put behind a CDN.  You may refer to how caching works.\nUpgrade your subscription on Platform.sh to get more computing resources. To do so, log into your account and edit the project subscription.\n\nPHP process crashed\nIf your PHP process crashed with a segmentation fault, you should see log lines in /var/log/app.log like below:\nWARNING: [pool web] child 112 exited on signal 11 (SIGSEGV) after 7.405936 seconds from start\nThis is complicated, either a PHP extension is hitting a segmentation fault or your PHP application code is crashing. You should review recent changes in your application and try to find the cause of it, probably with the help of XDebug.\nPHP process is killed\nIf your PHP process is killed by the kernel, you should see log lines in /var/log/app.log like this:\nWARNING: [pool web] child 429 exited on signal 9 (SIGKILL) after 50.938617 seconds from start\nThat means the memory usage of your container exceeds the limit allowed on your plan so the kernel kills the offending process. You should try the following:\n\nCheck if the memory usage of your application is expected and try to optimize it.\nUse sizing hints to reduce the amount of PHP workers which reduces the memory footprint.\nUpgrade your subscription on Platform.sh to get more computing resources. To do so, log into your account and edit the project.\n\nStuck build or deployment\nIf you see a build or deployment running longer than expected, that may be one of the following cases:\n\nThe build is blocked by a process in your build hook.\nThe deployment is blocked by a long running process in your deploy hook.\nThe deployment is blocked by a long running cron job in the environment.\nThe deployment is blocked by a long running cron job in the parent environment.\n\nTo determine if your environment is being stuck in the build or the deployment, you can look at the build log available in the management console.  If you see a line similar to the following:\nRe-deploying environment w6ikvtghgyuty-drupal8-b3dsina.\nIt means the build has completed successfully and the system is trying to deploy.  If that line never appears then it means the build is stuck.\nFor a blocked build (when you don't find the Re-deployment environment ... line), create a support ticket to have the build killed.  In most regions the build will self-terminate after one hour.  In older regions (US and EU) the build will need to be killed by our support team.\nWhen a deployment is blocked, you should try the following:\n\nUse SSH to connect to your environment. Find any long-running cron jobs or deploy hooks on the environment by running ps afx. Once you have identified the long running process on the environment, kill it with kill &lt;PID&gt;. PID stands for the process id shown by ps afx.\nIf you're performing \"Sync\" or \"Activate\" on an environment and the process is stuck, use SSH to connect to the parent environment and identify any long running cron jobs with ps afx. Kill the job(s) if you see any.\n\nSlow or failing build or deployment\nBuilds that take long time or fail is a common problem. Most of the time it's related to an application issue and they can be hard to troubleshoot without guidance.\nHere are a few tips that can help you solve the issues you are experiencing.\nCheck for errors in the logs\nInvisible errors during the build and deploy phase can cause increased wait times, failed builds and other problems. Investigating each log and fixing errors is essential.\nRelated documentation: Accessing logs\nBuild and deploy hooks\nHooks are frequently the cause of long build time. If they run into problem they can cause the build to fail or hang indefinitely.\nThe build hook can be tested in your local environment.  Because the deployed environment on Platform.sh is read-only the build hooks cannot be rerun there.\nDeploy hooks can be tested either locally or by logging into the application over SSH and running them there.  They should execute safely but be aware that depending on what your scripts are doing they may have an adverse impact on the running application (e.g., flushing all caches).\nFurthermore, you can test your hooks with these Linux commands to help figure out any problems:\ntime $cmd # Print execution time\nstrace -T $cmd # Print a system call report\nRelated documentation: Build and deploy hooks\nCron jobs\nContainers cannot be shutdown while long-running tasks are active.  That means long-running cron jobs will block a container from being shut down to make way for a new deploy.\nFor that reason, make sure your custom cron jobs execution times are low and that they are running properly.  Be aware that cron jobs may invoke other services in unexpected ways, which can increase execution time.\nnote\nDrupal's drush core-cron run installed module's cron task. Those can be, for example; evicting invalid cache, updating database records, regenerating assets. Be sure to frequently benchmark the drush core-cron command in all your environments, as it is a common source of performance issues.\nRelated documentation: Cron and scheduled tasks\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Development", "title": "Accessing logs", "url": "/development/logs.html", "documentId": "0f0840cdbf47ce4705da64690549ca25032f5d70", "text": "\n                        \n                            \n                                \n                                \n                                Accessing logs\nLogs for various tasks on an application container are available in the /var/log directory.  They can be accessed on the normal shell after loging in with platform ssh.  Alternatively, they may also be accessed remotely using the platform log command.  The CLI lets you specify which log file to access (the name of the file below minus the .log extension), as well as view the entire file in a pager, only the most recent lines, and so forth.  Run platform log --help for complete documentation.\nA number of different log files are available depending on the application container in use.\nAlthough the files in /var/log are writable, they should not be written to directly. Only write to it via standard logging mechanisms, such as your application's logging facility.  If your application has its own logging mechanism that should be written to a dedicated logs mount in your application.\nAll log files are trimmed to 100 MB automatically. But if you need to have complete logs, you can set up cron which will upload them to third-party storage. Contextual Code made a simple and well-described example how to achieve it.\naccess.log\nThis is the raw access log for the nginx instance running on the application container. That is, it does not include any requests that return a redirect or cache hit from the router.\napp.log\nAny log messages generated by the application will be sent to this file.  That includes language errors such as PHP Errors, Warnings, and Notices, as well as uncaught exceptions.\ncron.log\nThe cron log contains the output of all recent cron executions.  If there is no cron hook specified in the container configuration then this file will be absent. It also will not exist until the first time cron has run.\ndeploy.log\nThe deploy log contains the output of the most recent run of the deploy hook for the container.  If there is no deploy hook then this file will be absent.\nnginx/error.log\nnginx startup log messages will be recorded in this file.  It is rarely needed except when debugging possible nginx configuration errors. This file is not currently available using the platform log command.\nerror.log\nnginx-level errors that occur once nginx has fully started will be recorded here. This will include HTTP 500 errors for missing directories, file types that are excluded based on the .platform.app.yaml file, etc.\nphp.access.log\nOn a PHP container, the php.access.log contains a record of all requests to the PHP service.\npost_deploy.log\nThe post_deploy log contains the output of the most recent run of the post_deploy hook for the container.  If there is no post_deploy hook then this file will be absent.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Development", "title": "Email", "url": "/development/email.html", "documentId": "7133fc177e51bf99a9ee2aa67115cc2673dcc3e2", "text": "\n                        \n                            \n                                \n                                \n                                Sending E-Mail\nBy default only the master environment can send emails.  For the non-master environments, you can configure outgoing emails via the management console.\nEmails from Platform.sh are sent via a SendGrid-based SMTP proxy.  Each Platform.sh project is provisioned as a SendGrid sub-account.  These SendGrid sub-accounts are capped at 12k emails per month.  You can use /usr/sbin/sendmail on your application container to send emails with the assigned SendGrid sub-account. Alternatively, you can use the PLATFORM_SMTP_HOST  environment variable to use in your SMTP configuration.\nWe do not guarantee the deliverability of emails, and we do not support white-labeling them.  Our SMTP proxy is intended as a zero-configuration, best effort service.  If needed, you can instead use your own SMTP server or email delivery service provider. In that case, please bear in mind that TCP port 25 is blocked for security reasons; use TCP port 465 or 587 instead.\n\nnote\nYou may follow the SPF setup guidelines on SendGrid to improve email deliverability with our SMTP proxy. However, as we do not support white-labeling of emails, DKIM is not provided for our standard email handling (meaning that DMARC can't be set up either). Thus, for maximum deliverability own mail host must be engaged.\n\nEnabling/disabling email\nEmail support can be enabled/disabled per-environment.  By default, it is enabled on the master environment and disabled elsewhere.  That can be toggled in through the management console or via the command line, like so:\nplatform environment:info enable_smtp true\n\nplatform environment:info enable_smtp false\n\nWhen SMTP support is enabled the environment variable PLATFORM_SMTP_HOST will be populated with the address of the SMTP host that should be used.  When SMTP support is disabled that environment variable will be empty.\n\nnote\nChanging the SMTP status will not take effect immediately.  You will need to issue a new build, not just a new deploy, for the changes to take effect.\n\nSending email in PHP\nWhen you send email, you can simply use the built-in mail() function in PHP. The PHP runtime is configured to send email automatically via the assigned SendGrid sub-account.  Note that the From header is required; email will not send if that header is missing.\nBeware of the potential security problems when using the mail() function, which arise when using user-supplied input in the fifth ($additional_parameters) argument. See the PHP mail() documentation for more information.\nSwiftMailer\nIn Symfony, if you use the default SwiftMailer service, we recommend the following settings in your app/config/parameters.yaml:\nparameters:\n  mailer_transport: smtp\n  mailer_host: \"%env(PLATFORM_SMTP_HOST)%\"\n  mailer_user: null\n  mailer_password: null\n\nIf you are using a file spool facility, you will probably need to setup a read/write mount for it in .platform.app.yaml, for example:\nmounts:\n    'app/spool':\n        source: local\n        source_path: spool\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Development", "title": "Headers", "url": "/development/headers.html", "documentId": "d749a9f68446d451e8081b76b00e28a6b5630c2b", "text": "\n                        \n                            \n                                \n                                \n                                HTTP headers\nPlatform.sh adds a number of HTTP headers to both inbound and outbound messages.  We do not, however, modify or block existing headers on either request or response.\nRequest headers\nPlatform.sh adds the following HTTP headers in the router to give the application information about the connection.  These are stable and may be examined by the application as necessary.\n\nX-Forwarded-Proto: The protocol forwarded to the application, e.g. \"http\", \"https\".\nX-Client-IP: The remote IP address of the request.\nX-Client-SSL: Set \"on\" only if the client is using SSL connection, otherwise the header is not added.\nX-Original-Route: The route in .platform/routes.yaml which is used currently, e.g. https://{default}/.\n\nResponse headers\nPlatform.sh adds a number of response headers automatically to assist in debugging connections.  These headers should be treated as a semi-private API.  Do not code against them, but they may be inspected to help determine how Platform.sh handled the request to aid in debugging.\n\nX-Platform-Cache: Either HIT or MISS to indicate if the router in your cluster served the response from its own cache or if the request was passed through to the application.\nX-Platform-Cluster: The ID of the cluster that received the request.  The cluster name is formed from the project ID and environment ID.\nX-Platform-Processor: The ID of the container that generated the response.  The container ID is the cluster ID plus the container name.\nX-Platform-Router: The ID of the router that served the request.  The router ID is the processor ID of the router container, specifically.\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Development", "title": "Accessing your site", "url": "/development/access-site.html", "documentId": "dfe740abbfbadb65de7cab17edc74255c1d99279", "text": "\n                        \n                            \n                                \n                                \n                                Accessing your site\nOnce you have an environment running, there are many ways to access it to perform needed tasks. The most obvious of course is to view it in a web browser; the available URLs are shown in the Platform.sh management console and on the command line after every Git push.\nBy design, the only way to deploy new code is to push to the corresponding branch.  That ensures a consistent, repeatable, auditable application instance at all times.\nVisiting the site on the web\nThe web URL(s) for the site are listed in the management console under \"Access site\".\nThey can also be found on the command line, using the Platform.sh CLI:\nplatform url\n\nGenerally there will be two URLs created per route in your routes.yaml file: One HTTPS and one HTTP route that just redirects to HTTPS.  If you are using the {all} placeholder in your routes.yaml file then there will be more, depending on how many domains you have configured in your project.\nAccessing the application with SSH\nMost interactions with Platform.sh require SSH key authentication, and you will need to set up your SSH keys before working on a site.\nOnce that's done, you can easily access the command line on your application over SSH. To log in to the environment that corresponds to your current branch, simply type:\nplatform ssh\n\nTo log in to some other environment, use the -e flag to specify the environment.  \nThe application container is a fully working Linux environment using the bash shell.  Most of the system consists of a read-only file system (either the underlying container image or your built application image), so you cannot edit code live, but otherwise the full system is available to read and peruse. Any file mounts you have declared in your .platform.app.yaml will be writable.\nAdditionally, you will be logged in as the same user that the web server runs as; that means you needn't worry about the common problem of editing a file from the command line and from your application resulting in inconsistent and broken file ownership and permissions.\nUploading and downloading files\nThe writable static files in an application - including uploads, temporary and private files - are stored in mounts.\nThe Platform.sh CLI can list mounts inside an application:\n$ platform mounts\nMounts in the app drupal (environment master):\n+-------------------------+----------------------+\n| Path                    | Definition           |\n+-------------------------+----------------------+\n| web/sites/default/files | shared:files/files   |\n| private                 | shared:files/private |\n| tmp                     | shared:files/tmp     |\n+-------------------------+----------------------+\nThe CLI also helps transferring files to and from a mount, using the mount:upload and mount:download commands. These commands use the rsync utility, which in turn uses SSH.\nFor example, to download files from the 'private' mount:\n$ platform mount:download --mount private --target ./private\n\nThis will add, replace, and delete files in the local directory 'private'.\n\nAre you sure you want to continue? [Y/n]\nDownloading files from the remote mount /app/private to /Users/alice/Projects/foo/private\n  receiving file list ...   done\n\n  sent 16 bytes  received 3.73K bytes  2.50K bytes/sec\n  total size is 1.77M  speedup is 471.78\n  time: 0.91s\nThe download completed successfully.\nUploading files to a mount is similar:\n$ platform mount:upload --mount private --source ./private\n\nThis will add, replace, and delete files in the remote mount 'private'.\n\nAre you sure you want to continue? [Y/n]\nUploading files from /Users/alice/Projects/foo/private to the remote mount /app/private\n  building file list ...   done\n\n  sent 2.35K bytes  received 20 bytes  1.58K bytes/sec\n  total size is 1.77M  speedup is 745.09\n  time: 0.72s\nThe upload completed successfully.\nUsing SSH clients\nMany applications and protocols run on top of SSH, including SFTP, scp, and rsync.\nTo obtain the SSH connection details for the environment either copy them out of the Platform.sh management console (under the \"Access site\" dropdown) or run:\nplatform ssh --pipe\n\nThat will output the connection string for SSH, including the username and host for the current project and environment.  It will look something like &lt;project ID&gt;-&lt;environment ID&gt;--app@ssh.us.platform.sh.  The part before the @ is the username, the part after is the host.  Enter both of those into your SSH/SFTP client.  No password is necessary, but your client will need to have access to the SSH private key that corresponds to the public key on Platform.sh.\nSFTP\nSFTP is another way to upload and download files to and from a remote environment. There are many SFTP clients available for every operating system; use whichever one works for you.\nSCP\nSCP is a simple command-line utility to copy files to and from a remote environment.\nFor example, this command:\nscp \"$(platform ssh --pipe)\":web/uploads/diagram.png .\n\nwill copy the file named diagram.png in the web/uploads directory (relative to the application root) to the current local directory.  Reversing the order of the parameters will copy files up to the Platform.sh environment.  Consult the SCP documentation for other possible options.\nRsync\nFor copying files to and from a remote environment, rsync is the best tool available. It is a little more complicated to use than scp, but it can also be a lot more efficient, especially if you are simply updating files that are already partially copied.\nThe Platform.sh CLI mount:upload and mount:download commands (described above) are helpful wrappers around rsync that make it a little easier to use.\nHowever, it is also possible to use rsync on its own, for example:\nrsync -az \"$(platform ssh --pipe)\":web/uploads/ ./uploads/\n\nThis command will copy all files in the web/uploads directory on the remote environment to the uploads directory locally.  Note that rsync is very sensitive about trailing / characters, so that may change the meaning of a command.  Consult the rsync documentation for more details.  Also see our migrating and exporting guides for more examples using rsync.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Local", "title": "Lando", "url": "/development/local/lando.html", "documentId": "cd3c2b2485799d6a462e2358f42c07ad8d18df5f", "text": "\n                        \n                            \n                                \n                                \n                                Using Lando for local development\nLando is a container-based local development toolchain that plays nicely with Platform.sh.  It is maintained by Tandem, a 3rd party agency, but is a viable option for most Platform.sh projects.\nSee the Lando documentation for installing and setting up Lando on your system.\nLando will ask you to create a .lando.yml file in your application root, which functions similarly to the .platform.app.yaml file.  (Note the different file extension.)  It is safe to check this file into your Git repository as Platform.sh will simply ignore it.\nIf your application is one of those with a specific \"recipe\" available from Lando, you can use that directly in your .lando.yml file.  It can be customized further as needed for your application, and some customizations are specific to certain applications.\n.lando.yml configuration\nIn particular, we recommend:\n# Name the application the same as in your .platform.app.yaml.\nname: app\n# Use the recipe appropriate for your application.\nrecipe: drupal8\n\nconfig:\n  # Lando defaults to Apache. Switch to nginx to match Platform.sh.\n  via: nginx\n\n  # Set the webroot to match your .platform.app.yaml.\n  webroot: web\n\n  # Lando defaults to the latest MySQL release, but Platform.sh uses MariaDB.\n  # Specify the version to match what's in services.yaml.\n  database: mariadb:10.1\n\nDownloading data from Platform.sh into Lando\nIn most cases downloading data from Platform.sh and loading it into Lando is straightforward.  If you have a single MySQL database then the following two commands, run from your application root, will download a compressed database backup and load it into the local Lando database container.\nplatform db:dump --gzip -f database.sql.gz\nlando db-import database.sql.gz\n\nRsync can download user files easily and efficiently.  See the exporting tutorial for information on how to use rsync.\nThen you need to update your sites/default/settings.local.php to configure your codebase to connect to the local database that you just imported:\n/* Working in local with Lando */\nif (getenv('LANDO') === 'ON') {\n  $lando_info = json_decode(getenv('LANDO_INFO'), TRUE);\n  $settings['trusted_host_patterns'] = ['.*'];\n  $settings['hash_salt'] = 'CHANGE THIS TO SOME RANDOMLY GENERATED STRING';\n  $databases['default']['default'] = [\n    'driver' =&gt; 'mysql',\n    'database' =&gt; $lando_info['database']['creds']['database'],\n    'username' =&gt; $lando_info['database']['creds']['user'],\n    'password' =&gt; $lando_info['database']['creds']['password'],\n    'host' =&gt; $lando_info['database']['internal_connection']['host'],\n    'port' =&gt; $lando_info['database']['internal_connection']['port'],\n  ];\n}\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Development", "title": "Variables", "url": "/development/variables.html", "documentId": "bdd5365e02311ee58358de3a9d9c96b3c52cb5f1", "text": "\n                        \n                            \n                                \n                                \n                                Variables\n\n\nTypes\nApplication-provided variables\nProject variables\nEnvironment variables\nPlatform.sh-provided variables\n\n\nAccessing variables\nAt build time\nAt runtime\nIn your application\n\n\nVariable prefixes\nTop-level environment variables\nPHP-specific variables\nDrupal-specific variables\n\n\nShell variables\nHow can I have a script behave differently on a dedicated cluster than on development?\nHow can I have a script behave differently on Production and Staging?\n\n\nPlatform.sh allows a high degree of control over both the build process and the runtime environment of a project.  Part of that control comes in the form of variables that are set independently of the project's code base but available either at build or runtime for your code to leverage.  Platform.sh also exposes additional information to your application that way, including information like database credentials, the host or port it can use, and so forth.\n\n\n\nType\nDefiner\nScope\nInheritance\nBuild\nRuntime\n\n\n\n\nApplication\nApplication\nApplication\nn/a\nYes\nYes\n\n\nProject\nUser\nProject\nn/a\nYes\nYes\n\n\nEnvironment\nUser\nEnvironment\nOptional\nNo\nYes\n\n\nPlatform.sh\nPre-defined\nEnvironment\nn/a\nSome\nYes\n\n\n\nAll of those may be simple strings or base64-encoded JSON-serialized values.  In case of name collisions, Platform.sh-provided values override user-provided environment variables, which override user-provided project-level variables, which override application-provided variables.  (That is, lower items in the list above take precedence.)\nTypes\nApplication-provided variables\nVariables may be set in code, using the .platform.app.yaml file.  These values of course will be the same across all environments and present in the Git repository, which makes them a poor fit for API keys and such.  This capability is mainly to define values that an application expects via an environment variable that should be consistent across all environments.  For example, the PHP Symfony framework has a SYMFONY_ENV property that users may wish to set to prod on all environments to ensure a consistent build, or it may be used to set PHP configuration values.\nApplication-provided variables are available at both build time and runtime.\nProject variables\nProject variables are defined by the user and bound to a whole project.  They are available both at build time (and therefore from a build hook) and at runtime, and are the same for all environments in the project.  New project variables can be added using the CLI.  For example, the following command creates a project-level variable \"foo\" with the value \"bar\":\nplatform variable:create --level project --name foo --value bar\n\nProject variables are a good place to store secret information that is needed at build time, such as credentials for a private 3rd party code repository.\nBy default, project variables will be available at both build time and runtime. You can suppress one or the other with the --no-visible-build and --no-visible-runtime flags, such as if you want to hide certain credentials from runtime entirely.  For example, the following (silly) example will define a project variable but hide it from both build and runtime:\nplatform variable:create --level project --name foo --value bar --visible-build false --visible-runtime false\n\nNaturally in practice you'll want to use only one or the other, or allow the variable to be visible in both cases.\nProject variables may also be marked --sensitive true.  That flag will mark the variable to not be readable through the management console once it is set.  That makes it somewhat more private as requests through the Platform.sh CLI will not be able to view the variable.  However, it will still be readable from within the application container like any other variable.\nEnvironment variables\nEnvironment-level variables can also be set through the management console, or using the CLI. Environment variables are bound to a specific environment or branch.  An environment will also inherit variables from its parent environment, unless it has a variable defined with the same name.  That allows you to define your development variables only once, and use them on all the child environments.  For instance, to create an environment variable \"foo\" with the value \"bar\" on the currently checked out environment/branch, run:\n$ platform variable:create --level environment --name foo --value bar\n\nThat will set a variable on the currently active environment (that is, the branch you have checked out).  To set a variable on a different environment include the -e switch to specify the environment name.\nThere are two additional flags available on environment variables: --inheritable and --sensitive.\n\nSetting --inheritable false will cause the variable to not be inherited by child environments.  That is useful for setting production-only values on the master branch, and allowing all other environments to use a project-level variable of the same name.\nSetting --sensitive true flag will mark the variable to not be readable through the management console once it is set.  That makes it somewhat more private as requests through the Platform.sh CLI will not be able to view the variable.  However, it will still be readable from within the application container like any other variable.\n\nFor example, the following command will allow you to set a PayPal secret value on the master branch only; other environments will not inherit it and either get a project variable of the same name if it exists or no value at all.  It will also not be readable through the API.\n$ platform variable:create --name paypal_id --inheritable false --sensitive true\n\nIf you omit the variable --value from the command line as above, you will be prompted to enter the value interactively.\nChanging an environment variable will cause that environment to be redeployed so that it gets the new value.  However, it will not redeploy any child environments. If you want those to get the new value you will need to redeploy them yourself.\nEnvironment variables are a good place to store values that apply only on Platform.sh and not on your local development environment. This includes API credentials for 3rd party services, mode settings if your application has a separate \"Dev\" and \"Prod\" runtime toggle, etc.\nPlatform.sh-provided variables\nPlatform.sh also provides a series of variables by default.  These inform an application about its runtime configuration.  The most important of these is relationship information, which tells the application how to connect to databases and other services defined in services.yaml.  They are always prefixed with PLATFORM_* to differentiate them from user-provided values.\nThe following variables are only available at build time, and may be used in a build hook:\n\nPLATFORM_OUTPUT_DIR: The output directory for compiled languages at build time. Will be equivalent to PLATFORM_APP_DIR in most cases.\n\nThe following variables are available at both runtime and at build time, and may be used in a build hook:\n\nPLATFORM_APP_DIR: The absolute path to the application directory.\nPLATFORM_APPLICATION: A base64-encoded JSON object that describes the application. It maps the content of the .platform.app.yaml that you have in Git and it has a few subkeys.\nPLATFORM_APPLICATION_NAME: The name of the application, as configured in the .platform.app.yaml file.\nPLATFORM_PROJECT: The ID of the project.\nPLATFORM_TREE_ID: The ID of the tree the application was built from. It's essentially the SHA hash of the tree in Git.  If you need a unique ID for each build for whatever reason this is the value you should use.\nPLATFORM_VARIABLES: A base64-encoded JSON object which keys are variables names and values are variable values (see below).  Note that the values available in this structure may vary between build and runtime depending on the variable type as described above.\nPLATFORM_PROJECT_ENTROPY: A random value created when the project is first created, which is then stable throughout the project's life. This can be used for Drupal hash salt, Symfony secret, or other similar values in other frameworks.\n\nThe following variables exist only at runtime.  If used in a build hook they will evaluate to an empty string like any other unset variable:\n\nPLATFORM_BRANCH: The name of the Git branch.\nPLATFORM_DOCUMENT_ROOT: The absolute path to the web document root, if applicable.\nPLATFORM_ENVIRONMENT: The name of the environment generated by the name of the Git branch.\nPLATFORM_SMTP_HOST: The SMTP host that email messages should be sent through.  This value will be empty if mail is disabled for the current environment.\nPLATFORM_RELATIONSHIPS: A base64-encoded JSON object whose keys are the relationship name and the values are arrays of relationship endpoint definitions. See the documentation for each Service for details on each service type's schema.\nPLATFORM_ROUTES: A base64-encoded JSON object that describes the routes that you defined in the environment. It maps the content of the .platform/routes.yaml file.\n\nOn a Dedicated instance, the following additional variables are available at runtime only:\n\nPLATFORM_MODE: Set to enterprise in an Dedicated environment, both production and staging.  Note that an Enterprise support plan doesn't always imply a Dedicated production, but Dedicated production always implies an Enterprise support plan.\nPLATFORM_CLUSTER: Set to the cluster ID.\nPLATFORM_PROJECT: Set to the document root.  This is typically the same as your cluster name for the production environment, while staging will have _stg or similar appended.\n\nSince values can change over time, the best thing is to inspect the variable at runtime then use it to configure your application. For example:\necho $PLATFORM_RELATIONSHIPS | base64 --decode | json_pp\n\n{\n    \"database\": [\n        {\n            \"host\": \"database.internal\",\n            \"ip\": \"246.0.97.91\",\n            \"password\": \"\",\n            \"path\": \"main\",\n            \"port\": 3306,\n            \"query\": {\n                \"is_master\": true\n            },\n            \"scheme\": \"mysql\",\n            \"username\": \"user\"\n        }\n    ],\n    \"redis\": [\n        {\n            \"host\": \"redis.internal\",\n            \"ip\": \"246.0.97.88\",\n            \"port\": 6379,\n            \"scheme\": \"redis\"\n        }\n    ]\n}\n\nAccessing variables\nYou can get a list of all variables defined on a given environment either via the management console or using the CLI:\n$ platform variables\n\n+---------+-------+-----------+------+\n| ID      | Value | Inherited | JSON |\n+---------+-------+-----------+------+\n| env:FOO | bar   | No        | No   |\n+---------+-------+-----------+------+\n\nAt build time\nOnly Project variables are available at build time.  They will be listed together in a single JSON array and exposed in the $PLATFORM_VARIABLES Unix environment variable.\necho $PLATFORM_VARIABLES | base64 --decode\n{\"my_var\": \"this is a value\"}\n\nThey can also be accessed from within a non-shell script via the language's standard way of accessing environment variables.  For instance, in PHP you would use getenv('PLATFORM_VARIABLES'). Remember that in some cases they may be base64 JSON strings and will need to be unpacked.  To do so from the shell, for instance, you would do:\necho $PLATFORM_VARIABLES | base64 --decode\n{\"myvar\": \"this is a value\"}\n\nSee below for how to expose a project variable as its own Unix environment variable.\nAt runtime\nIn a running container, which includes the deploy hook, your Project variables, Environment variables, and Platform.sh-provided variables are all exposed as Unix environment variables and can be accessed by your application through your language's standard way of accessing environment variables.\nPlatform.sh-defined variables will be exposed directly with the names listed above.  Project and environment variables will be merged together into a single JSON array and exposed in the $PLATFORM_VARIABLES environment variable.  In case of a matching name, an environment variable will override a variable of the same name in a parent environment, and both will override a project variable.\nFor example, suppose we have the following variables defined:\n$ platform variables -e master\nVariables on the project Example (abcdef123456), environment master:\n+----------------+-------------+--------+\n| Name           | Level       | Value  |\n+----------------+-------------+--------+\n| system_name    | project     | Spiffy |\n| system_version | project     | 1.5    |\n| api_key        | environment | abc123 |\n+----------------+-------------+--------+\nAnd the following variables defined on the branch feature-x, a child environment (and branch of) master:\n$ platform variables -e master\nVariables on the project Example (abcdef123456), environment feature-x:\n+----------------+-------------+--------+\n| Name           | Level       | Value  |\n+----------------+-------------+--------+\n| system_name    | project     | Spiffy |\n| system_version | project     | 1.5    |\n| api_key        | environment | def456 |\n| system_version | environment | 1.7    |\n| debug_mode     | environment | 1      |\n+----------------+-------------+--------+\nIn this case, on the master environment $PLATFORM_VARIABLES would look like this:\necho $PLATFORM_VARIABLES | base64 --decode | json_pp\n\n{\n    \"system_name\": \"Spiffy\",\n    \"system_version\": \"1.5\",\n    \"api_key\": \"abc123\"\n}\n\nWhile the same command on the feature-x branch would produce:\n{\n    \"system_name\": \"Spiffy\",\n    \"system_version\": \"1.7\",\n    \"api_key\": \"def456\",\n    \"debug_mode\": \"1\"\n}\n\nIn your application\nCheck the individual documentation pages for accessing environment variables for your given application language.\n\nPHP: the getenv() function\nNode.js: the process.env object\nPython: the os.environ object\nRuby: the ENV accessor\n\nPHPPythonNode.jsNode.js LibraryRuby&lt;?php\n\n// A simple variable.\n$projectId = getenv('PLATFORM_PROJECT');\n\n// A JSON-encoded value.\n$variables = json_decode(base64_decode(getenv('PLATFORM_VARIABLES')), TRUE);import os\nimport json\nimport base64\n\n// A simple variable.\nproject_id = os.getenv('PLATFORM_PROJECT')\n\n// A JSON-encoded value.\nvariables = json.loads(base64.b64decode(os.getenv('PLATFORM_VARIABLES')).decode('utf-8'))// Utility to assist in decoding a packed JSON variable.\nfunction read_base64_json(varName) {\n  try {\n    return JSON.parse(new Buffer(process.env[varName], 'base64').toString());\n  } catch (err) {\n    throw new Error(`no ${varName} environment variable`);\n  }\n};\n\n// A simple variable.\nlet projectId = process.env.PLATFORM_PROJECT;\n\n// A JSON-encoded value.\nlet variables = read_base64_json('PLATFORM_VARIABLES');// Install the utility library:\n// https://github.com/platformsh/platformsh-nodejs-helper\n// $ npm install platformsh --save\n\nconst config = require('platformsh').config();\n\n// This is a string.\nlet projectId = config.project;\n\n// This is a bare object.\nlet variables = config.variables;// A simple variable.\nproject_id = ENV[\"PLATFORM_PROJECT\"] || nil\n\n// A JSON-encoded value.\nvariables = JSON.parse(Base64.decode64(ENV[\"PLATFORM_VARIABLES\"]))\nVariable prefixes\nCertain variable name prefixes have special meaning.  A few of these are defined by Platform.sh and are built-in.  Others are simply available as a convention for your own application code to follow.\nTop-level environment variables\nBy default, project and environment variables are only added as part of the $PLATFORM_VARIABLES Unix environment variable.  However, you can also expose a variable as its own Unix environment variable by giving it the prefix env:.  \nFor example, the variable env:foo will create a Unix environment variable called FOO.  (Note the automatic upper-casing.)\n$ platform variable:create --name env:foo --value bar\n\nWith PHP, you can then access that variable with getenv('FOO').\nPHP-specific variables\nAny variable with the prefix php: will also be added to the php.ini configuration of all PHP-based application containers.  For example, an environment variable named php:display_errors with value On is equivalent to placing the following in php.ini:\ndisplay_errors = On\n\nThis feature is primarily useful to override debug configuration on development environments, such as enabling errors or configuring the XDebug extension.  For applying a configuration setting to all environments, or to vary them between different PHP containers in the same project, specify the variables in the .platform.app.yaml file for your application.  See the PHP configuration page for more information.\nDrupal-specific variables\nAs a convention, our provided Drupal template code will automatically map variables to Drupal's configuration system.  The logic varies slightly depending on the Drupal version.\nOn Drupal 7, any variable that begins with drupal: will be mapped to the global $conf array, which overrides Drupal's variable_get() system.  For instance, to force a site name from the Platform.sh variables (say to set it \"This is a Dev site\") you would set the drupal:site_name variable.\nOn Drupal 8, any variable that begins with drupal: will be mapped to the global $settings array. That is intended for very low-level configuration.\nAlso on Drupal 8, any variable that begins with d8config: will be mapped to the global $config array, which is useful for overriding drupal's exportable configuration system.  The variable name will need to contain two colons, one for d8config: and one for the name of the configuration object to override.  For example, a variable named d8config:system.site:name will override the name property of the system.site configuration object.\nAs the above logic is defined in a file in your Git repository you are free to change it if desired.  The same behavior can also be easily implemented for any other application or framework.\nShell variables\nYou can also provide a .environment file as part of your application, in your application root (as a sibling of your .platform.app.yaml file, or files in the case of a multi-app configuration).  That file will be sourced as a bash script when the container starts and on all SSH logins.  It can be used to set any environment variables directly, such as the PATH variable.  For example, the following .environment file will allow any executable installed using Composer as part of a project to be run regardless of the current directory:\nexport PATH=/app/vendor/bin:$PATH\n\nNote that the file is sourced after all other environment variables above are defined, so they will be available to the script.  That also means the .environment script has the \"last word\" on environment variable values and can override anything it wants to.\nHow can I have a script behave differently on a dedicated cluster than on development?\nThe following sample shell script will output a different value on the Dedicated cluster than the Development environment.\nif [ \"$PLATFORM_MODE\" = \"enterprise\" ] ; then\n    echo \"Hello from the Enterprise\"\nelse\n    echo \"We're on Development\"\nfi\n\nHow can I have a script behave differently on Production and Staging?\nIn most Enterprise configurations the production branch is named production (whereas it is always master on Platform.sh Professional).  The following test therefore should work in almost all cases:\nif [ \"$PLATFORM_MODE\" = \"enterprise\" ] ; then\n    if [ \"$PLATFORM_BRANCH\" = \"production\" ] ; then\n        echo \"This is live on production\"\n    else\n        echo \"This is on staging\"\n    fi\nelse\n    echo \"We're on Development\"\nfi\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Local", "title": "Untethered", "url": "/development/local/untethered.html", "documentId": "5e01698845cf3ab42d2626998d309cadb907180f", "text": "\n                        \n                            \n                                \n                                \n                                Untethered Local\nIt's possible to run your entire site locally on your computer.  That is more performant as there's no extra latency to connect to a remote database and doesn't require an active Internet connection to work.  However, it does require running all necessary services (databases, search servers, etc.) locally.  These can be set up however you prefer, although Platform.sh recommends using a virtual machine to make it easier to share configuration between developers.\nIf you already have a development workflow in place that works for you, you can keep using it with virtually no changes.\nTo synchronize data from an environment on Platform.sh, consult the documentation for each service.  Each service type has its own native data import/export process and Platform.sh does not get in the way of that.  It's also straightforward to download user files from your application using rsync.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Local", "title": "Docksal", "url": "/development/local/docksal.html", "documentId": "796274448c25ce11110de062857ab8ad4a8be451", "text": "\n                        \n                            \n                                \n                                \n                                Using Docksal for local development\nDocksal is a docker based local development tool that plays nicely with Platform.sh. It is maintained by a community of developers and is a viable option for most Platform.sh projects.\nSee the Docksal documentation for installing and setting up Docksal on your system.\nDocksal will ask you to create a .docksal directory in your application root, which functions similarly to the .platform.app.yaml file. It is safe to check this directory into your Git repository as Platform.sh will simply ignore it.\nDocksal Fin (fin) is a command line tool for controlling Docksal and used for interacting with a Docksal project. For more information on the use of the fin command, type the help subcommand to get all available commands and options.\nfin help\n\nUsing Platform.sh CLI within Docksal\nThe SECRET_PLATFORMSH_CLI_TOKEN must be set to use the Platform.sh CLI within your Docksal project. This is an API Token found with your Platform.sh account and can be generated by going to the API Tokens page and clicking the Create API Token link. This will allow for you to interact with your Platform.sh account from within the CLI container.\nfin config set --global SECRET_PLATFORMSH_CLI_TOKEN=XXX\n\nPulling a Platform.sh project\nThe Docksal CLI ships with the Platform.sh CLI tool. To use the tool and pull a project locally, make sure you have uploaded your SSH key to your Platform.sh account. Once that is done and a SECRET_PLATFORMSH_CLI_TOKEN has been added using the above step, you can set up your project with the following instructions. \nNote: Replace PROJECT_ID with your project's ID, which can found within the Platform.sh dashboard. Replace PROJECT_DIRECTORY with the name of the local directory you'd like the project cloned into.\nIf you do not already have Platform.sh CLI installed locally, you can use the one in the CLI image. The advantage of this would mean that the tool would never have to be installed locally and therefore is one less dependency.\nfin run-cli 'platform get PROJECT_ID -e master PROJECT_DIRECTORY'\n\nIf you already have Platform.sh CLI installed locally, you can use that instead.\nplatform get PROJECT_ID -e master PROJECT_DIRECTORY\n\nInitializing a Platform.sh project\nTo start a new Docksal project, initialize the configuration with the fin config generate command and specify the docroot flag.\nfin config generate --docroot=web\nfin project start\n\nThe web directory is one of the many different items that can be set for the document. If this is different or changes over time running the following will fix this.\nfin config set docroot=XXX # Replacing XXX with the new document root.\n\nCustomizing a Platform.sh project\nBy default, Docksal comes configured with a PHP 7.1 container, an Apache 2.4 web container, and a MySQL 5.6 database container. Additional versions are available in the images and you can set the desired versions by setting following variables within your .docksal/docksal.env file.\n# Apache Versions 2.2 / 2.4\n#WEB_IMAGE='docksal/web:2.1-apache-2.2'\nWEB_IMAGE='docksal/web:2.1-apache-2.4'\n\n# MySQL Version: 5.6 / 5.7 / 8.0\n#DB_IMAGE='docksal/db:1.2-mysql-5.6'\nDB_IMAGE='docksal/db:1.2-mysql-5.7'\n#DB_IMAGE='docksal/db:1.2-mysql-8.0'\n\n# PHP Versions Available 5.6 / 7.0 / 7.1 / 7.2\n#CLI_IMAGE='docksal/cli:2.5-php5.6'\n#CLI_IMAGE='docksal/cli:2.5-php7.0'\nCLI_IMAGE='docksal/cli:2.5-php7.1'\n#CLI_IMAGE='docksal/cli:2.5-php7.2'\nYou can further create and customize a .docksal/docksal.yml file within your project. This is a docker-compose file and can be customized as needed for your application, as some customizations are specific to certain applications. See Docksal documentation on extending stock images.\nDownloading MySQL data from Platform.sh into Docksal\nIn most cases, downloading data from Platform.sh and loading it into your project is straightforward. The following commands, run from your application root, will download a compressed database backup and load it into the local Docksal database container.\nfin platform db:dump --gzip -f /tmp/database.sql.gz\nfin exec 'zcat &lt; /tmp/database.sql.gz | mysql -u user -puser -h db default'\n\nConnecting Projects to the Database\nAfter importing your database into the project the next step is connecting to the database server. The following information can be used for setting up a connection for your application.\n\n\n\nKey\nValue\n\n\n\n\nDB Name\ndefault\n\n\nUsername\nuser\n\n\nPassword\nuser\n\n\nHost\ndb\n\n\nPort\n3306\n\n\n\nSee the exporting tutorial for information on how to use rsync.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Development", "title": "Tools", "url": "/development/tools.html", "documentId": "94d5cf0da78b83374df2c6b2952e7e8c37639d59", "text": "\n                        \n                            \n                                \n                                \n                                Technical Requirements: Git and SSH\nGit\nGit is the open source version control system that is utilized by Platform.sh.\nAny change you make to your Platform.sh project will need to be committed via Git. You can see all the Git commit messages of an environment in the activity feed of the management console.\nBefore getting started, make sure you have it installed on your computer to be able to interact with Platform.sh.\n\nsee also\n\nInstall Git\nLearn more about Git\n\n\nSSH\nYou connect to your Platform.sh Git repository and to your applications and services using SSH.  SSH requires two RSA keys:\n\nA private key kept secret by the user\nA public key stored within the Platform.sh account\n\nThese keys are called the public-private keypair and usually look like random lines of characters, like this:\nA private key:\n-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEAtpw0S4DwDVj2q04mhiIMkhvrYU7Z6hRiNbTFsqg3X7x/uYS/\ndcNrSvT82j/jSeYQP3Dsod9GERW+dmOuLaFNeiqOStZi6jRSWo41hCOWOFbpBum3\nra1n6nUO1wa/7O5wbgzhUOfnim77oOK0UgkqPArBCNXiNFTUJAvRyVmCtvJOyrqz\n...(20 more lines of this garbage)...\ncPjJ/wKBgGd3eZIBK6Ak92u65HYXgY9EcX3vBNP4NsF087uxV4YfrM18KlGf5I87\nQGerp3VKaGe0St3ot57GlwCAQUJAf1mit8qDTi0I8MhBe7q2lstXkBvde7GY1gKx\nKng4ohG6xHZ/OvC9tq7/THwAvleaxgLZN5GyXfAqNylDdZ0LtSjl\n-----END RSA PRIVATE KEY-----\nA public key (one very long line):\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2nDRLgPANWParTiaGIgySG+thTtnqFGI1tMWyqDdfvH+5hL91w2tK9PzaP+NJ5hA/cOyh30YRFb52Y64toU16Ko5K1mLqNFJajjWEI5Y4VukG6betrWfqdQ7XBr/s7nBuDOFQ5+eKbvug4rRSCSo8CsEI1eI0VNQkC9HJWYK28k7KurMdTN7X/Z/4vknM4/Rm2bnMk2idoORQgomeZS1p3GkG8dQs/c0j/b4H7azxnqdcCaR4ahbytX3d49BN0WwE84C+ItsnkCt1g5tVADPrab+Ywsm/FTnGY3cJKKdOAHt7Ls5lfpyyug2hNAFeiZF0MoCekjDZ2GH2xdFc7AX/ your_email_address@example.com\nYou will need a SSH public/private keypair in order to interact with Platform.sh. Your public key is uploaded to your Platform.sh user account, and it then governs authentication for Git, SSH sessions (shell access), and other tools that connect to your Platform.sh project.\nGitHub has a good walk-through of creating an SSH keypair on various operating systems.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Local", "title": "Tethered", "url": "/development/local/tethered.html", "documentId": "e0004802f6ba89d89796c3dbcd432c99919836dd", "text": "\n                        \n                            \n                                \n                                \n                                Tethered Local\nThe simplest way to run a project locally is to use a local web server, but keep all other services on Platform.sh and connect to them over an SSH tunnel.  This approach requires very little setup, but depending on the speed of your connection and how I/O intensive your application is may not be performant enough to use regularly.  It will also require an active Internet connection, of course.\nQuick Start\nIn your application directory run platform tunnel:open &amp;&amp;  export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\". This will open an SSH tunnel to your current Platform.sh environment and expose a local environment variable that mimics the relationships array on Platform.sh.\nYou can now run your application locally (for example by running php -d variables_order=EGPCS -S localhost:8001 for PHP), assuming it is configured to read its configuration from the Platform.sh environment variables.\nNote that other Platform.sh environment configuration such as the routes or application secret value will still not be available.  Also be aware that the environment variable exists only in your current shell.  If you are starting multiple local command shells you will need to rerun the export command above in each of them.\nLocal web server\nFor the local web server the approach will vary depending on your language.\n\nFor a self-serving language (Go or Node.js), simply run the program locally.\nFor PHP, you may install your own copy of Nginx (or Apache) and PHP-FPM, or simply use the built-in PHP web server.  Be aware however that by default the PHP web server will ignore environment variables by default.  You will need to explicitly instruct it to read them, like so: php -S -d variables_order=EGPCS localhost:8001.  That will start a basic web server capable of running PHP, serving the current directory, on port 8001, using available environment variables.  See the PHP manual for more information.\nFor other languages it is recommended that you install your own copy of Nginx or Apache.\nA virtual machine or Docker image is also a viable option.\n\nSSH tunneling\nNow that the code is running, it needs to connect it to its services.  For that, open an SSH tunnel to the current project.\n$ platform tunnel:open\nSSH tunnel opened on port 30000 to relationship: redis\nSSH tunnel opened on port 30001 to relationship: database\nLogs are written to: ~/.platformsh/tunnels.log\n\nList tunnels with: platform tunnels\nView tunnel details with: platform tunnel:info\nClose tunnels with: platform tunnel:close\n\nNow you can connect to the remote database normally, as if it were local.\n$ mysql --host=127.0.0.1 --port=30001 --user='user' --password='' --database='main'\n\nThe specific port that each service uses is not guaranteed, but is unlikely to change unless you add an additional service or connect to multiple projects at once.  In most cases it's safe to add a local-configuration file for your application that connects to, in this case, localhost:30001 for the SQL database and localhost:30000 for Redis.\nAfter the tunnel(s) are opened, you can confirm their presence:\nplatform tunnel:list\n\nYou can show more information about the open tunnel(s) with:\nplatform tunnel:info\n\nand you can close tunnels with:\nplatform tunnel:close\n\n\nnote\nThe platform tunnel:open command requires the pcntl and posix PHP extensions. Run php -m | grep -E 'posix|pcntl' to check if they're there.\nIf you don't have these extensions installed, you can use the platform tunnel:single command to open one tunnel at a time. This command also lets you specify a local port number.\n\nLocal environment variables\nAlternatively, you can read the relationship information directly from Platform.sh and expose it locally in the same form.  From the command line, run:\nexport PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\"\n\nThat will create a PLATFORM_RELATIONSHIPS environment variable locally that looks exactly the same as the one you'd see on Platform.sh, but pointing to the locally mapped SSH tunnels.  Whatever code you have that looks for and decodes the relationship information from that variable (which is what runs on Platform.sh) will detect it and use it just as if you were running on Platform.sh.\nNote that the environment variable is set globally so you cannot use this mechanism to load mutiple tethered Platform.sh projects at the same time.  If you need to run multiple tethered environments at once you will have to read the relationships information for each one from the application code, like so:\nPHPPython&lt;?php\nif ($relationships_encoded = shell_exec('platform tunnel:info --encode')) {\n    $relationships = json_decode(base64_decode($relationships_encoded, TRUE), TRUE);\n    // ...\n}import json\nimport base64\nimport subprocess\n\nencoded = subprocess.check_output(['platform', 'tunnel:info', '--encode'])\nif (encoded):\n    json.loads(base64.b64decode(relationships).decode('utf-8'))\n    # ...\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "CLI", "title": "API Tokens", "url": "/development/cli/api-tokens.html", "documentId": "29f2a12639d6953063322fe5ce9091193382a746", "text": "\n                        \n                            \n                                \n                                \n                                API Tokens\nObtaining a token\nThe Platform.sh CLI can also be used from CI services or other automation tools, and supports an API Token authentication option for that purpose.\nAn API token can be created through the management console. Go to the \"User\" page from your account drop-down, then select the \"Account Settings\" tab, then \"API Tokens\".\nClick the \"Create an API Token\" link.\n\nYou may be asked to reverify your password, then enter a unique application name to identify the token.\n\nAfter creating the token it will be displayed once at the top of the page in a green banner.  You may also view it later by clicking the \"view\" link next to the token name.  You will be asked to reverify your password as well when viewing the token.\nNow set that token to an environment variable named PLATFORMSH_CLI_TOKEN on your system where the CLI will run.  Consult the documentation for your CI system to see how to do that.\n\nnote\nIf running CLI commands from any automated system, including a Platform.sh cron task, we urge you to use the --no-wait flag on any commands that may take more than a second or two to avoid blocking the process.\n\nMachine users\nFor security reasons we recommend creating a dedicated machine user to run automation tasks such as taking backups, renewing SSL certificates or triggering source operations. We also recommend creating a unique machine user for each project to be automated.\nLike human users, every machine user account needs its own unique email address.\nThe machine user can be given a very restrictive set of permissions limited to just its needed tasks. Backups, for instance, require Admin access but no SSH key, while checking out code from a CI server to run tests on it would require an SSH key but only Reader access.\nIt will also show up in logs and activity streams as a separate entry from human users.\nConsult the Users documentation for more information about the differences between access levels.\nInstall the CLI on a Platform.sh environment\nA common use case for an API token is to allow the Platform.sh CLI to be run on an app container, often via a cron hook.  An API token is necessary for authentication, but the CLI will be able to auto-detect the current project and environment.\nFirst, create a machine user (see above) that you invite to your project. Then, log in as that machine user to obtain an API token. Set this token as the top-level environment variable env:PLATFORMSH_CLI_TOKEN either through the management console or via the CLI, like so:\nplatform variable:create -e master --level environment --name env:PLATFORMSH_CLI_TOKEN --sensitive true --value 'your API token'\n\n\nnote\nIt is important to include the env: so as to expose $PLATFORMSH_CLI_TOKEN on its own as a top level Unix environment variable, rather than as a part of $PLATFORM_VARIABLES like normal environment variables.\n\nSecond, add a build hook to your .platform.app.yaml file to download the CLI as part of the build process.\nhooks:\n    build: |\n        curl -sS https://platform.sh/cli/installer | php\n\nThis will download the CLI to a known directory, .platformsh/bin, which will be added to the PATH at runtime (via the .environment file). Because the API token is available, the CLI will now be able to run authenticated commands, acting as the user who created the token.\nYou can now call the CLI from within the shell on the app container, or via a cron hook.  Note that if you want a cron to run only on the production environment you will need to wrap it in an if-check on the $PLATFORM_BRANCH variable, like so:\ncrons:\n    backup:\n        spec: '0 5 * * *'\n        cmd: |\n            if [ \"$PLATFORM_BRANCH\" = master ]; then\n                platform backup:create --yes --no-wait\n            fi\n\n\nnote\nSeriously, please use --no-wait for all CLI commands placed in a cron hook. Failure to do so may result in long deploy times and site downtime.\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Development", "title": "Local development", "url": "/development/local.html", "documentId": "8627a6854965e705e19f5aa887efa52eb6d4da3e", "text": "\n                        \n                            \n                                \n                                \n                                Set up your local development environment\nWhile Platform.sh is great as a tool for hosting an application during both development and production, it's naturally not the ideal place to edit code.  You can't, in fact, as the file system is read-only (as it should be).  The proper place to edit your code is on your computer.\nYou must have an SSH key already configured on your account, and have both Git and the Platform.sh CLI installed before continuing.\nDownload the code\nIf you don't already have a local copy of your project's code, run platform get to download one. You can also run platform projects to list all of the projects in your account.\n~/htdocs $ platform projects\n  Your projects are:\n  +---------------+----------------------------+------------------------------------------------+\n  | ID            | Name                       | URL                                            |\n  +---------------+----------------------------+------------------------------------------------+\n  | [project-id]  | New Platform Project       | https://eu.platform.sh/#/projects/[project-id] |\n  +---------------+----------------------------+------------------------------------------------+\n\n  Get a project by running platform get [id].\n  List a project's environments by running platform environments.\n\nNow you can download the code using platform get [project-id] [folder-name]:\n~/htdocs $ platform get [project-id] my-project\n  Cloning into 'my-project/repository'...\n  remote: counting objects: 11, done.\n  Receiving objects: 100% (11/11), 1.36 KiB | 0 bytes/s, done.\n  Checking connectivity... done.\n\nYou should now have a repository folder, based on what you used for [folder-name] in the platform get command above.\nYou will also notice a new directory in your project, .platform/local, which is excluded from Git.  This directory contains builds and any local metadata about your project needed by the CLI.\nBuilding the site locally\nRun the platform build command to run through the same build process as would be run on Platform.sh.  That will produce a _www directory in your project root that is a symlink to the currently active build in the .platform/local/builds folder. It should be used as the document root for your local web server.\n~/htdocs/my-project $ platform build\nBuilding application myapp (runtime type: php)\n  Beginning to build ~/htdocs/my-project/project.make.\n  drupal-7.38 downloaded.\n  drupal patched with install-redirect-on-empty-database-728702-36.patch.\n  Generated PATCHES.txt file for drupal\n  platform-7.x-1.3 downloaded.\nRunning post-build hooks\nSymlinking files from the 'shared' directory to sites/default\n\nBuild complete for application myapp\nWeb root: ~/htdocs/my-project/_www\n~/htdocs/my-project $\n\nBe aware, of course, that the platform build command will run locally, and so require whatever appropriate runtime or other tools you specify.  It may also result in packages referenced in your dependendencies block being installed on your local computer.\nIf that is undesireable, a local virtual machine will let you create an enclosed local development environment that won't affect your main system.\nRunning the code\nPlatform.sh supports whatever local development environment you wish to use.  There is no dependency on any particular tool so if you already have a local development workflow you're comfortable with you can keep using it without changes.  That's the \"untethered\" option.\nFor quick changes, you can also run your code locally but use the services hosted on Platform.sh.  That is, your site is \"tethered\" to Platform.sh.  While this approach requires installing less on your system it can be quite slow as all communication with the database or cache server will need to travel from your computer to Platform.sh's servers.\nSpecific documentation is also available for the local development tools Lando and Docksal, which support most applications that Platform.sh supports.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Development", "title": "Project templates", "url": "/development/templates.html", "documentId": "3363cb41cd67bba3800c0f6f9b094c397848a7c4", "text": "\n                        \n                            \n                                \n                                \n                                Project templates\nYou can also initialize your project using any of the pre-made templates below.  For each one, simply Git Clone the repository from GitHub then push it to Platform.sh using the instructions for a custom repository in the management console, as seen in the video above.\nC#/.NET\n\n\n\n  Frameworks\n  ASP.NET Core\n\n\n\n\nGo\n\n\n\n  Examples\n  Generic Go application\n\n\n\n  Frameworks\n  Beego\n  Echo\n  Gin\n  Hugo\n\n\n\n\nJava\n\n\n\n  Applications\n  Spring Boot MySQL\n  Spring Boot MySQL (Gradle)\n  Spring Boot MongoDB\n  Spring Kotlin\n  Apache Tomee\n  Thorntail\n  Payara Micro\n  KumuluzEE\n  Helidon\n  Open Liberty\n  Jetty\n  Micronaut\n  Jenkins\n  XWiki\n\n\n\n\nLisp\n\n\n\n  Examples\n  Hunchenroot\n\n\n\n\nNode.js\n\n\n\n  Examples\n  Generic Node.js\n  Parse\n  Node.js microservices\n\n\n\n\nPHP\n\n\n\n  Applications\n  Akeneo\n  EZ Platform\n  Drupal 7\n  Drupal 7 (Commerce Kickstart)\n  Drupal 8\n  Drupal 8 (Multisite variant)\n  GovCMS8\n  Laravel\n  Moodle\n  Magento 1\n  Magento 2\n  Opigno\n  Pimcore\n  Sculpin\n  TYPO3\n  WordPress\n\n\n\n  Examples\n  Generic PHP application\n\n\n\n\n  Frameworks\n  AmPHP\n  React PHP\n  Symfony 3.x\n  Symfony 4.x\n\n\n\n\nPython\n\n\n\n  Examples\n  Python UWSGI with Unix Sockets\n\n\n\n  Frameworks\n  Django 1\n  Django 2\n  Flask\n\n\n\n\n  Applications\n  Moin Moin\n  Pelican\n  Pyramid\n  Wagtail\n\n\n\n\nRuby\n\n\n\n  Frameworks\n  Sinatra\n  Sinatra (all services)\n  Ruby on Rails\n\n\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Development", "title": "CLI", "url": "/development/cli.html", "documentId": "07a9b4e6cb663d34a879faf84f0af524d2b234d6", "text": "\n                        \n                            \n                                \n                                \n                                CLI (Command Line Interface)\nThe CLI is the official tool to use and manage your Platform.sh projects directly from your terminal. Anything you can do within the management console can be done with the CLI. Behinds the scenes it uses both the Git interface and our REST API.\nThe source code of the CLI is hosted on GitHub.\nFind detailed information on setting up a local development environment.\nInstallation\nYou can install the CLI easily using this command:\ncurl -sS https://platform.sh/cli/installer | php\n\nYou can find the system requirements and more information in the installation instructions on GitHub.\nAuthentication\nThe Platform.sh CLI will authenticate you with Platform.sh and show your projects. Just type this command to start:\nplatform\n\nYou will be asked to log in via a browser.\nWhen you are logged in, a list of your projects appears, along with some tips for getting started.\nYour command-line tools are now ready to use with Platform.sh.\n\nnote\nPlease consult the full documentation on CLI Authentication on the public CLI Github repository for further details.\n\nUsage\nThe CLI uses Platform.sh API to trigger commands (Branch, Merge...) on your projects.\nIt's also very useful when you work locally since it can simulate a local build of your codebase as if you were pushing a change to Platform.sh.\nOnce you have the CLI installed, run platform list to see all of the available commands.\nYou can preface any command with help to see more information on how to use that command.\n$ platform help domain:add\nCommand: domain:add\nDescription: Add a new domain to the project\n\nUsage:\n domain:add [--project[=\"...\"]] [--cert=\"...\"] [--key=\"...\"] [--chain=\"...\"] [name]\n\nArguments:\n name                  The name of the domain\n\nOptions:\n --project             The project ID\n --cert                The path to the certificate file for this domain.\n --key                 The path to the private key file for the provided certificate.\n --chain               The path to the certificate chain file or files for the provided certificate. (multiple values allowed)\n --help (-h)           Display this help message\n --quiet (-q)          Do not output any message\n --verbose (-v|vv|vvv) Increase the verbosity of messages\n --version (-V)        Display this application version\n --yes (-y)            Answer \"yes\" to all prompts\n --no (-n)             Answer \"no\" to all prompts\n --shell (-s)          Launch the shell\n\nCLI features\nAdditional settings to control the operation of the Platform.sh CLI can be managed in the configuration file (.platform/local/project.yaml) or environment variables. See the README for the CLI for details. \nAuto-selecting your project\nWhen your shell's working directory is inside a local checkout of your project repository, the CLI will autodetect your project ID and environment so you don't need to list them as parameters each time.\nIn your home directory, for example, you need to provide the project ID as an argument each time:\n$ platform project:info --project=acdefghijkl --environment=staging\n\nYou can instead get the same result with just:\n$ cd myproject\n$ platform project:info\n\nYou can also set a preferred project ID with the environment variables PLATFORM_PROJECT, PLATFORM_BRANCH and PLATFORM_APPLICATION_NAME.\nexport PLATFORM_PROJECT=acdefghijkl;\nexport PLATFORM_BRANCH=staging;\nplatform project:info\n\nAutocomplete on the command line\nOnce installed, the platform CLI tool provides tab auto-completion for commands, options, and even some values (your projects, valid regions).\n\nnote\nYour system must include the bash-completion package or equivalent. This is not available by default on OSX, but can be installed via brew. Check your home directory and ensure that the file ~/.platformsh/autocompletion.sh is being included by your shell. platform self:install will attempt a reinstall of this utility if it's needed.\n\nInstalling the CLI on Windows 10\nThere are multiple ways to install the CLI on Windows 10. Platform.sh recommends using Bash for Windows (Windows Subsystem for Linux).\nInstalling Bash for Windows\nYou can install Bash to use the CLI on a Windows 10, 64-bit machine. The Windows 10 Anniversary Update is needed to support Git.\nTo install Bash on Windows 10 Anniversary Edition you need to:\n\nActivate the Developer Mode in \"Update &amp; Security\" in Windows Settings. This will prompt you to restart your computer.\nActivate the \"Windows Subsystem for Linux (Beta)\", under \"Turn Windows features on or off\" in the Programs and Features section of the Control Panel. Once again, you will need to restart your computer.\nIn the Start Menu, search for the program \"bash.exe\", which will prompt you to install it from the Windows Store.\n\nBash is now installed.\nYou can read more on WindowsCentral.\nUpon starting Bash, you will be asked to choose a username. According to the article, it doesn't have to be the same as your current username. However, if the username don't exist, the Linux system might not be able to create the Linux directory (depending on your permissions level). It is therefore recommended you use the same username for Linux as your Windows machine (provided your Windows user name isn't \"Admin\", as that will not be allowed).\nOnce Bash for Windows is installed, you can install the Platform.sh CLI with the same command as above:\ncurl -sS https://platform.sh/cli/installer | php\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Languages", "title": "Ruby", "url": "/languages/ruby.html", "documentId": "80c917342ad95b60bd37c35570c161a71461b258", "text": "\n                        \n                            \n                                \n                                \n                                Ruby\nPlatform.sh supports deploying any Ruby application. Your application can use any Ruby application server such as Unicorn or Puma and deploying a Rails or a Sinatra app is very straight forward.\nSupported versions\nRuby MRI\n\n2.3\n2.4\n2.5\n2.6\n2.7\n\nUnicorn based Rails configuration\nIn this example, we use Unicorn to run our Ruby application. You could use any Ruby application server such as Puma or Thin.\nConfigure the .platform.app.yaml file with a few key settings as listed below, a complete example is included at the end of this section.\n\nSpecify the language of your application (available versions are listed above):\n type: 'ruby:2.7'\n\nBuild your application with the build hook.\n\n\nAssuming you have your  dependencies stored in the Gemfile at the root of your application folder to execute build steps:\nhooks:\n  build: bundle install --without development test\n  deploy: RACK_ENV=production bundle exec rake db:migrate\n\nThese are installed as your project dependencies in your environment. You can also use the dependecies key to install global dependecies theses can be Ruby, Python, NodeJS or PHP libraries.\n\nConfigure the command you use to start serving your application (this must be a foreground-running process) under the web section, e.g.:\n\nweb:\n    upstream:\n        socket_family: unix\n    commands:\n        start: \"unicorn -l $SOCKET -E production config.ru\"\n\nThis assumes you have Unicorn as a dependency in your Gemfile\n# Use Unicorn as the app server\ngroup :production do\n  gem 'unicorn'\nend\n\nand that you have a rackup file config.ru at the root of your repository, for example for a rails application you would put:\nrequire \"rubygems\"\nrequire ::File.expand_path('../config/environment', __FILE__)\nrun Rails.application\n\n\nDefine the web locations your application is using:\n\nweb:\n   locations:\n       \"/\":\n           root: \"public\"\n           passthru: true\n           expires: 1h\n           allow: true\n\nThis configuration asks our web server to handle HTTP requests at \"/static\" to serve static files stored in /app/static/ folder while everything else are forwarded to your application server.\n\nCreate any Read/Write mounts. The root file system is read only. You must explicitly describe writable mounts.\n\nmounts:\n    tmp:\n        source: local\n        source_path: tmp\n    logs:\n        source: local\n        source_path: logs\n\nThis setting allows your application writing files to /app/tmp and have logs stored in /app/logs.\nYou can define other read/writre mounts (your application code itself being deployed to a read-only file system). Note that the file system is persistent, and when you backup your cluster these mounts get backed-up too.\n\nThen, setup the routes to your application in .platform/routes.yaml.\n\n\"https://{default}/\":\n    type: upstream\n    # the first part should be your project name\n    upstream: \"app:http\"\n\nHere is the complete .platform.app.yaml file:\nname: 'app'\ntype: \"ruby:2.7\"\n\nweb:\n    upstream:\n        socket_family: unix\n    commands:\n        start: \"unicorn -l $SOCKET -E production config.ru\"\n\n    locations:\n        \"/\":\n            root: \"public\"\n            passthru: true\n            expires: 1h\n            allow: true\n\nrelationships:\n    database: \"database:mysql\"\n\ndisk: 2048\n\nhooks:\n  build: bundle install --without development test\n  deploy: RACK_ENV=production bundle exec rake db:migrate\n\nmounts:\n    tmp:\n        source: local\n        source_path: tmp\n    logs:\n        source: local\n        source_path: logs\n\nConfiguring services\n\nIn this example we assue in the relationships key that we have a mysql instance. To configure it we need to create a .platform/services.yaml with for eample:\n\ndatabase:\n  type: mysql:10.2\n  disk: 2048\n\nConnecting to services\nYou can define services in your environment. And, link to the services using .platform.app.yaml:\nrelationships:\n    database: \"mysqldb:mysql\"\n\nBy using the following ruby function calls, you can obtain the database details.\nrequire \"base64\"\nrequire \"json\"\nrelationships= JSON.parse(Base64.decode64(ENV['PLATFORM_RELATIONSHIPS']))\n\nWhich should give you something like:\n{\n   \"database\" : [\n      {\n         \"path\" : \"main\",\n         \"query\" : {\n            \"is_master\" : true\n         },\n         \"port\" : 3306,\n         \"username\" : \"user\",\n         \"password\" : \"\",\n         \"host\" : \"database.internal\",\n         \"ip\" : \"246.0.241.50\",\n         \"scheme\" : \"mysql\"\n      }\n   ]\n}\n\nProject templates\nA number of project templates for Ruby applications and typical configurations are available on GitHub.  Not all of them are proactively maintained but all can be used as a starting point or reference for building your own website or web application.\nPlatform.sh also provides a helper library for Ruby applications that simplifies presenting environment information to your application.  It is not required to run Ruby applications on Platform.sh but is recommended.\n\nSinatra\nSinatra (with all services enabled)\nRuby on Rails\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "PHP", "title": "Xdebug", "url": "/languages/php/xdebug.html", "documentId": "d7bb9e2338e628224962560d89087d54ef4ad8ba", "text": "\n                        \n                            \n                                \n                                \n                                Using Xdebug\nXdebug is a real-time debugger extension for PHP.  While usually used for local development, it can also be helpful for debugging aberrant behavior on the server.  It is available on Platform.sh Grid instances running PHP 7.2 and higher.\nAs configured on Platform.sh, it avoids any runtime overhead for non-debug requests, even in production, and only allows SSH-tunneled connections to avoid any security issues.\nSetting up Xdebug\nXdebug is not enabled the same way as other extensions, as it should not be active on most requests.  Xdebug has a substantial impact on performance and should not be run in production.\nInstead, Xdebug can be enabled by adding the following configuration to the application's .platform.app.yaml file:\nruntime:\n    xdebug:\n        idekey: PHPSTORM\n\nThe idekey value can be any arbitrary alphanumeric string, as long as it matches your IDE's configuration.\nWhen that key is defined, Platform.sh will start a second PHP-FPM process on the container that is identically configured but also has Xdebug enabled.  Only incoming requests that have an Xdebug cookie or query parameter set will be forwarded to the debug PHP-FPM process.  All other requests will be directed to the normal PHP-FPM process and thus have no performance impact.\nXdebug has numerous other configuration options available.  They are all set as php.ini values, and can be configured the same way as any other php.ini setting.  Consult the Xdebug documentation for a full list of available options, although in most cases the default configuration is sufficient.\nUsing Xdebug\nOpen a tunnel\nFrom your local checkout of your application, run platform environment:xdebug (or just platform xdebug) to open an SSH tunnel to the server.  That SSH tunnel will allow your IDE and the server to communicate debug information securely.\nBy default, Xdebug operates on port 9000.  Generally, it is best to configure your IDE to use that port.  If you wish to use an alternate port use the --port flag.\nTo close the tunnel and terminate the debug connection, press Ctrl-C.\nInstall an Xdebug helper\nWhile Xdebug can be triggered from the browser by adding a special query parameter, the preferred way is to use a browser plugin helper.  One is available for Firefox and for Chrome.  Their respective plugin pages document how to trigger them when needed.\nUsing PHPStorm\nThe configuration for Xdebug will be slightly different for each IDE.  Platform.sh has no preference as to the IDE or editor you use, but we have provided configuration instructions for PHPStorm/IntelliJ due to its popularity in the PHP ecosystem.\n1. Configure Xdebug\nIn your PHPStorm Settings window, go to Languages &amp; Frameworks &gt; PHP &gt; Debug.\nEnsure that the \"Debug port\" is set to the expected value (9000, or whatever you want to use in the --port flag) and that \"Can accept external connections\" is checked.  Other settings are at your discretion.\n\n2. Set DBGp Proxy\nIn your PHPStorm Settings window, go to Languages &amp; Frameworks &gt; PHP &gt; Debug &gt; DBGp Proxy.\nEnsure that the \"IDE key\" field is set to the same value as the idekey in .platform.app.yaml.  The exact value doesn't matter as long as it matches.\n\n3. Configure a server\nIn your PHPStorm Settings window, go to Languages &amp; Frameworks &gt; PHP &gt; Servers.\nAdd a new server for your Platform.sh environment.  The \"Host\" should be the hostname of the environment on Platform.sh you will be debugging.  The \"Port\" should always be 443 and the \"Debugger\" set to Xdebug.  Ensure that \"Use path mappings\" is checked, which will make available a tree of your project with a column to configure the remote path that it should map to.\nThis page lets you define what remote paths on the server correspond to what path on your local machine.  In the majority of cases you can just define the root of your application (either the repository root or the root of your PHP code base specifically in a multi-app setup) to map to app, as in the example below.\n\n\nNote\nIt may be easier to allow the debug process to connect once, allow it to fail, and then select the \"Configure server mappings\" error message.  That will pre-populate most of the fields in this page and only require you to set the app root mapping.\n\n4. Listen for connections\nToggle on PHPStorm's Xdebug listener.  Either select Run &gt; Start listening for PHP debug connections from the menu or click the  icon in the toolbar.\nTo disable PHPStorm's listener, either select Run &gt; Stop listening for PHP debug connections from the menu or toggle the  icon in the toolbar.\n5. Start debugging\nWhile in listen mode, start the platform xdebug tunnel.  Use the Xdebug helper plugin for your browser to enable debugging.  Set a breakpoint in your application, then load a page in your browser.  The request should pause at the breakpoint and allow you to examine the running application.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "PHP", "title": "PHP-FPM sizing", "url": "/languages/php/fpm.html", "documentId": "479e1b91b9526402811489f220321072324c3825", "text": "\n                        \n                            \n                                \n                                \n                                PHP-FPM workers\nPlatform.sh uses a heuristic to automatically set the number of workers of the PHP-FPM runtime based on the memory available in the container. This heuristic is based on assumptions about the memory necessary on average to process a request. You can tweak those assumptions if your application will typically use considerably more or less memory.\nNote that this value is independent of the memory_limit set in php.ini, which is the maximum amount of memory a single PHP process can use before it is automatically terminated.  These estimates are used only for determining the number of PHP-FPM workers to start.\nThe heuristic\nThe heuristic is based on three input parameters:\n\nThe memory available for the container, which depends on the size of the container (S, M, L),\nThe memory that an average request is expected to require,\nThe memory that should be reserved for things that are not specific to a request (memory for nginx, the op-code cache, some OS page cache, etc.)\n\nThe number of workers is calculated as:\n\nDefaults\nThe default assumptions are:\n\n45 MB for the average per-request memory\n70 MB for the reserved memory\n\nThese are deliberately conservative values that should allow most programs to run without modification.\nYou can change them by using the runtime.sizing_hints.reserved_memory and runtime.sizing_hints.request_memory in your .platform.app.yaml. For example, if your application consumes on average 110 MB of memory for a request use:\nruntime:\n    sizing_hints:\n        request_memory: 110\nThe request_memory has a lower limit of 10 MB while reserved_memory has a lower limit of 70 MB.  Values lower than those will be replaced with those minimums.\nYou can check the maximum number of PHP-FPM workers by opening an SSH session and running following command (example for PHP 7.x):\ngrep -e '^pm.max_children' /etc/php/*/fpm/php-fpm.conf\npm.max_children = 2\nMeasuring PHP worker memory usage\nTo see how much memory your PHP worker processes are using, you can open an SSH session and look at the PHP access log:\nless /var/log/php.access.log\nIn the fifth column, you'll see the peak memory usage that occurred while each request was handled. The peak usage will probably vary between requests, but in order to avoid the severe performance costs that come from swapping, your size hint should be somewhere between the average and worst case memory usages that you observe.\nA good way to determine an optimal request memory is with the following command:\ntail -n5000 /var/log/php.access.log | awk '{print $6}' | sort -n | uniq -c\nThis will print out a table of how many requests used how much memory, in KB, for the last 5000 requests that reached PHP-FPM.  (On an especially busy site you may need to increase that number).  As an example, consider the following output:\n      1\n   4800 2048\n    948 4096\n    785 6144\n    584 8192\n    889 10240\n    492 12288\n    196 14336\n     68 16384\n      2 18432\n      1 22528\n      6 131072\nThis indicates that the majority of requests (4800) used 2048 KB of memory.  In this case that's likely application caching at work.  Most requests used up to around 10 MB of memory, while a few used as much as 18 MB and a very very few (6 requests) peaked at 131 MB.  (In this example those are probably cache clears.)\nA conservative approach would suggest an average request memory of 16 MB should be sufficient.  A more aggressive stance would suggest 10 MB.  The more aggressive approach would potentially allow for more concurrent requests at the risk of some requests needing to use swap memory, thus slowing them down.\nThe web agency Pixelant has also published a log analyzer tool for Platform.sh that offers a better visualization of access logs to determine how much memory requests are using on average.  It also offers additional insights into the operation of your site that can suggest places to further optimize your configuration and provide guidance on when it's time to increase your plan size.  (Please note that this tool is maintained by a 3rd party, not by Platform.sh.)\n\nnote\nIf you are running on PHP 5.x then don't bother adjusting the worker memory usage until you upgrade to PHP 7.x.  PHP 7 is vastly more memory efficient than PHP 5 and you will likely need less than half as much memory per process under PHP 7.\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "PHP", "title": "Performance tuning", "url": "/languages/php/tuning.html", "documentId": "339674e3c591e9103eb33d950829bdf5236a0037", "text": "\n                        \n                            \n                                \n                                \n                                Performance tuning PHP\nOnce your application is up and running it still needs to be kept fast.  Platform.sh offers a wide degree of flexibility in how PHP behaves, but that does mean you may need to take a few steps to ensure your site is running optimally.\nThe following recommendations are guidelines only.  They're also listed in approximately the order we recommend investigating them, although your mileage may vary.\nUpgrade to PHP 7.2+\nThere is very little purpose to trying to optimize a PHP application on PHP 5.  PHP 7 is generally twice as fast and uses half as much memory as PHP 5, making it unquestionably the first step to take when trying to make a PHP-based site run faster.\nTo change your PHP version, simply change the type key in your .platform.app.yaml to the desired PHP version.  As always, test it on a branch first before merging to master.\nEnsure that the router cache is properly configured\nAlthough not PHP-specific, a common source of performance issues is a misconfigured cache.  The most common issue is not whitelisting session cookies, which results in a site with any cookies at all, including from analytics tools, never being cached.  See the router cache documentation, and the cookie entry specifically.\nYou will also need to ensure that your application is sending the correct cache-control header.  The router cache will obey whatever cache headers your application sends, so send it good ones.\nStatic assets cache headers are set using the expires key in .platform.app.yaml.  See the web.locations documentation for more details.\nOptimize the FPM worker count\nPHP-FPM reserves a fixed number of simultaneous worker processes to handle incoming requests.  If more simultaneous requests are received than the number of workers then some requests will wait.  The default worker count is deliberately set rather conservative but can be improved in many cases.  See the PHP-FPM sizing page for how to determine and set a more optimal value.\nEnable preloading\nPHP 7.4 and later supports preloading code files into shared memory once at server startup, bypassing the need to include or autoload them later.  Depending on your application doing so can result in significant improvements to both CPU and memory usage.  If using PHP 7.4, see the PHP Preload instructions for how to configure it on Platform.sh and consult your application's documentation to see if they have any recommendations for an optimal preload configuration.\nIf you are not using PHP 7.4, this is a good reason to upgrade.\nConfigure opcache\nPHP 5.5 and later include an opcache that is enabled at all times, as it should be.  It may still need to be tuned, however.  The opcache can be configured using php.ini values, which in this case are best set using the variables block in .platform.app.yaml.\n\nnote\nIf using opcache preloading on PHP 7.4 or later, configure that first and let the application run for a while before tuning the opcache itself as the preload script may change the necessary configuration here.\n\nThe most important values to set are:\n\nopcache.max_accelerated_files: The max number of files that the opcache may cache at once.  If this is lower than the number of files in the application it will begin thrashing and become less effective.\nopcache.memory_consumption: The total memory that the opcache may use.  If the application is larger than this the cache will start thrashing and become less effective.\n\nTo determine how many files you have, run this command from the root of your application:\nfind . -type f -name '*.php' | wc -l\n\nThat will report the number of files in your file tree that end in .php.  That may not be perfectly accurate (some applications have PHP code in files that don't end in .php, it may not catch generated files that haven't been generated yet, etc.) but it's a reasonable approximation.  Set the opcache.max_accelerated_files option to a value slightly higher than this.  Note that PHP will automatically round the value you specify up to the next highest prime number, for reasons long lost to the sands of time.\nDetermining an optimal opcache.memory_consumption is a bit harder, unfortunately, as it requires executing code via a web request to get adequate statistics.  Fortunately there is a command line tool that will handle most of that.\nChange to the /tmp directory (or any other non-web-accessible writable directory) and install CacheTool.  It has a large number of commands and options but we're only interested in the opcache status for FastCGI command.  The really short version of downloading and using it would be:\ncd /tmp\ncurl -sO http://gordalina.github.io/cachetool/downloads/cachetool.phar\nphp cachetool.phar opcache:status --fcgi=$SOCKET\n\nThe --fcgi=$SOCKET option tells the command how to connect to the PHP-FPM process on the server through the Platform.sh-defined socket.  That command will output something similar to the following:\n+----------------------+---------------------------------+\n| Name                 | Value                           |\n+----------------------+---------------------------------+\n| Enabled              | Yes                             |\n| Cache full           | No                              |\n| Restart pending      | No                              |\n| Restart in progress  | No                              |\n| Memory used          | 29.65 MiB                       |\n| Memory free          | 34.35 MiB                       |\n| Memory wasted (%)    | 0 b (0%)                        |\n+----------------------+---------------------------------+\n| Cached scripts       | 1528                            |\n| Cached keys          | 2609                            |\n| Max cached keys      | 32531                           |\n| Start time           | Mon, 18 Jun 2018 18:19:32 +0000 |\n| Last restart time    | Never                           |\n| Oom restarts         | 0                               |\n| Hash restarts        | 0                               |\n| Manual restarts      | 0                               |\n| Hits                 | 8554                            |\n| Misses               | 1594                            |\n| Blacklist misses (%) | 0 (0%)                          |\n| Opcache hit rate     | 84.29247142294                  |\n+----------------------+---------------------------------+\n\nThe most important values for now are the Memory used, Memory free, and Oom restarts (Out Of Memory Restarts).  If the Oom restarts number is high (meaning more than a handful) it means you don't have enough memory allocated to the opcache.  In this example the opcache is using about half of the 64 MB given to it by default, which is fine.  If Memory free is too low or Oom Restarts too high, set a higher value for the memory consumption.\nRemember to remove the cachetools.phar file once you're done with it.\nYour .platform.app.yaml file will end up including a block similar to:\nvariables:\n    php:\n        'opcache.max_accelerated_files': 22000\n        'opcache.memory_consumption': 96\n\n(Memory consumption is set in megabytes.)\nOptimize your code\nIt's also possible that your own code is doing more work than it needs to.  Profiling and optimizing a PHP application is a much larger topic than will fit here, but Platform.sh recommends enabling Blackfire.io on your project to determine what slow spots can be found and addressed.\nThe web agency Pixelant has also published a log analyzer tool for Platform.sh.  It works only for PHP scripts, but offers good visualizations and insights into the operation of your site that can suggest places to further optimize your configuration and provide guidance on when it's time to increase your plan size.  (Please note that this tool is maintained by a 3rd party, not by Platform.sh.)\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "PHP", "title": "Redis", "url": "/languages/php/redis.html", "documentId": "61e01f8ec5462fef2454454b51625fd25d549ba1", "text": "\n                        \n                            \n                                \n                                \n                                Using Redis with PHP\nRedis is a popular structured key-value service, supported by Platform.sh.  It's frequently used for caching.\nFor PHP, Redis support is provided through a PECL extension called PhpRedis.  Unfortunately, the extension has been known to break its API between versions, even between minor versions.  That makes it difficult for Platform.sh to bundle like other PHP extensions.\nFortunately, the extension is small enough that it's reasonable to compile as part of the build step and enable yourself.  That makes it possible to install the specific version of the extension that your application code requires.  All of the necessary tools to compile PHP extensions are included in our PHP containers, so cloning the source code and compiling it on each build is straightforward.  That does entail a few minute additions to each build, however.\nAlternatively, we have written a shell script that leverages the build cache directory to only compile the extension once, and supports compiling any version of the extension.\nUsing the Redis builder script\n\nCopy the following script into a file named install-redis.sh in your application root (as a sibling of your .platform.app.yaml file).\n\nrun() {\n    # Run the compilation process.\n    cd $PLATFORM_CACHE_DIR || exit 1;\n\n    if [ ! -f \"${PLATFORM_CACHE_DIR}/phpredis/modules/redis.so\" ]; then\n        ensure_source\n        checkout_version \"$1\"\n        compile_source\n    fi\n\n    copy_lib\n    enable_lib\n}\n\nenable_lib() {\n    # Tell PHP to enable the extension.\n    echo \"Enabling PhpRedis extension.\"\n    echo \"extension=${PLATFORM_APP_DIR}/redis.so\" &gt;&gt; $PLATFORM_APP_DIR/php.ini\n}\n\ncopy_lib() {\n    # Copy the compiled library to the application directory.\n    echo \"Installing PhpRedis extension.\"\n    cp $PLATFORM_CACHE_DIR/phpredis/modules/redis.so $PLATFORM_APP_DIR\n}\n\ncheckout_version () {\n    # Check out the specific Git tag that we want to build.\n    git checkout \"$1\"\n}\n\nensure_source() {\n    # Ensure that the extension source code is available and up to date.\n    if [ -d \"phpredis\" ]; then\n        cd phpredis || exit 1;\n        git fetch --all --prune\n    else\n        git clone https://github.com/phpredis/phpredis.git\n        cd phpredis || exit 1;\n    fi\n}\n\ncompile_source() {\n    # Compile the extension.\n    phpize\n    ./configure\n    make\n}\n\nensure_environment() {\n    # If not running in a Platform.sh build environment, do nothing.\n    if [ -z \"${PLATFORM_CACHE_DIR}\" ]; then\n        echo \"Not running in a Platform.sh build environment.  Aborting Redis installation.\"\n        exit 0;\n    fi\n}\n\nensure_arguments() {\n    # If no version was specified, don't try to guess.\n    if [ -z $1 ]; then\n        echo \"No version of the PhpRedis extension specified.  You must specify a tagged version on the command line.\"\n        exit 1;\n    fi\n}\n\nensure_environment\nensure_arguments \"$1\"\nrun \"$1\"\n\n\nInvoke that script from your build hook, specifying a version.  Any tagged version of the library is acceptable:\n\nhooks:\n    build: |\n        set -e\n        bash install-redis.sh 5.1.1\n\n\nIf you ever wish to change the version of PhpRedis you are using, update the build hook and clear the build cache: platform project:clear-build-cache.  The new version will not be used until you clear the build cache.\n\nThere is no need to declare the extension in the runtime block.  That is only for pre-built extensions.\nWhat the script does\n\nDownloads the PhpRedis source code.\nChecks out the version specified in the build hook.\nCompiles the extension.\nCopies the resulting redis.so file to your application root.\nAdds a line to the php.ini file in your application root to enable the extension, creating the file if necessary.\n\nIf the script does not find a $PLATFORM_CACHE_DIR directory defined, it exits silently.  That means if you run the build hook locally it will have no effect.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "PHP", "title": "Frameworks", "url": "/languages/php/frameworks.html", "documentId": "53f1530c034ca526bf3643f0d169424251d14884", "text": "\n                        \n                            \n                                \n                                \n                                PHP Featured Frameworks\nFull Drush support and Composer-based builds make handling dependencies and builds for PHP frameworks as simple as committing your composer.json to your project.\nDrupal\nDrupal is an open-source content management framework written in PHP. Since Composer comes pre-installed on Platform.sh, Drupal can be installed and updated completely using Composer. The default build flavor for PHP application runs composer install during build, handling all of your dependencies automatically.\n\nDrupal 7 Best Practices\nDrupal 8 Best Practices\n\nCommunity Support\nDrupal FAQs, how-to guides and other tutorials right on Platform.sh Community.\n\nDrupal on Platform.sh Community\n\nTemplates\n\nDrupal 7\nDrupal 7 (Vanilla)\nDrupal 8\nDrupal 8 (Multisite variant)\nOpigno\nGovCMS8\n\neZ Platform\neZ Platform is a CMS based on the Symfony full-stack framework. eZ Platform comes pre-configured for use with Platform.sh for versions 1.13 and later, all it takes is mapping a few environment variables to an existing project. Consult the caching, configuration, and local development best practices for eZ Platform and Fastly integration for more information.\n\neZ Platform Best Practices\n\nExample Projects\n\nNote:\nTemplate projects (repositories in the platformsh-templates GitHub organization) are actively maintained by the Platform.sh team. Any other example projects come with less support, and remain in public repositories as proof-of-concepts.\n\n\neZ Platform\n\nSymfony\nSymfony is a web application framework written in PHP. Like Drupal, Symfony projects can utilize native Composer to build applications and manage dependencies.\n\nSymfony Best Practices\n\nCommunity Support\nSymfony FAQs, how-to guides and other tutorials right on Platform.sh Community.\n\nSymfony on Platform.sh Community\n\nTemplates\n\nSymfony 3\nSymfony 4\n\nTYPO3\nTYPO3 is an open-source CMS written in PHP. Utilized Platform.sh native Composer to handle builds and maintain dependencies.\n\nTYPO3 Best Practices\n\nExample Projects\n\nNote:\nTemplate projects (repositories in the platformsh-templates GitHub organization) are actively maintained by the Platform.sh team. Any other example projects come with less support, and remain in public repositories as proof-of-concepts.\n\n\nTYPO3\n\nWordpress\nWordpress is a PHP content management system. Platform.sh recommends using the composer-based installation method for Wordpress.\n\nWordpress Best Practices\n\nCommunity Support\nAll your Wordpress FAQs, plus how-to guides and tutorials right on Platform.sh Community.\n\nWordpress on Platform.sh Community\n\nTemplates\n\nWordpress\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "PHP", "title": "PHP.ini settings", "url": "/languages/php/ini.html", "documentId": "271cbf8bf5183d7fcab8e115f50c1fbca54cd652", "text": "\n                        \n                            \n                                \n                                \n                                Custom php.ini\nThere are two ways to customize php.ini values for your application. The recommended method is to use the variables property of .platform.app.yaml to set ini values using the php prefix. For example, to increase the PHP memory limit you'd put the following in .platform.app.yaml:\n variables:\n    php:\n        memory_limit: 256M\n\nIt's also possible to provide a custom php.ini file in the repository in the root of the application (where your .platform.app.yaml file is).\n; php.ini\n; Increase PHP memory limit\nmemory_limit = 256M\n\nAnother example is to set the timezone of the PHP runtime (though, the timezone settings of containers/services would remain in UTC):\n variables:\n    php:\n        \"date.timezone\": \"Europe/Paris\"\n\nor\n; php.ini\n; Set PHP runtime timezone\ndate.timezone = \"Europe/Paris\"\n\nEnvironment-specific php.ini configuration directives can be provided via environment variables separately from the application code. See the note in the Environment variables section.\nDisabling functions\nA common recommendation for securing a PHP installation is to disable certain built-in functions that are frequently used in remote attacks.  By default, Platform.sh does not disable any functions as they all do have some legitimate use in various applications.  However, you may wish to disable them yourself if you know they are not needed.  For example, to disable pcntl_exec and pcntl_fork (which are not usable in a web request anyway):\n variables:\n    php:\n        \"disable_functions\": \"pcntl_exec,pcntl_fork\"\n\nCommon functions to disable include:\n\ncreate_function - create_function has no useful purpose since PHP 5.3 and should not be used, ever.  It has been effectively replaced by anonymous functions.\nexec,passthru,shell_exec,system,proc_open,popen - These functions all allow a PHP script to run a bash shell command. That is rarely used by web applications, although build scripts may need them.\npcntl_exec,pcntl_fork,pcntl_setpriority - The pcntl_* functions (including those not listed here) are responsible for process management.  Most of them will cause a fatal error if used within a web request.  Cron tasks or workers may make use of them, however.  Most are safe to disable unless you know that you are using them.\ncurl_exec,curl_multi_exec - These functions allow a PHP script to make arbitrary HTTP requests.  Note that they are frequently used by other HTTP libraries such as Guzzle, in which case you should not disable them.\nshow_source - This function shows a syntax highlighted version of a named PHP source file.  That is rarely useful outside of development.\n\nNaturally if your application does make use of any of these functions, it will fail if you disable them.  In that case, do not disable them.\nDefault php.ini settings\nThe default values for some frequently-modified php.ini settings are listed below.\n\nmemory_limit=128M \npost_max_size=64M \nupload_max_filesize=64M \ndisplay_errors=On This value is on by default to ease setting up a project on Platform.sh. We strongly recommend providing a custom error handler in your application or setting this value to Off before you make your site live.\nzend.assertions=-1 Assertions are optimized out of existence and have no impact at runtime. You should have assertions set to `1` for your local development system.\nopcache.memory_consumption=64 This is the number of megabytes available for the opcache. Large applications with many files may want to increase this value.\nopcache.validate_timestamps=On The opcache will check for updated files on disk. This is necessary to support applications that generate compiled PHP code from user configuration. If you are certain your application does not do so then you can disable this setting for a small performance boost.\n\n\n\n\nwarning\nWe do not limit what you can put in your php.ini file, but many\nsettings can break your application. This is a facility for advanced\nusers.\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "PHP", "title": "Extensions", "url": "/languages/php/extensions.html", "documentId": "3c205b1587ab219c507788c0614c47cccaa9c842", "text": "\n                        \n                            \n                                \n                                \n                                PHP extensions\nYou can define the PHP extensions you want to enable or disable:\n# .platform.app.yaml\nruntime:\n    extensions:\n        - http\n        - redis\n        - ssh2\n    disabled_extensions:\n        - sqlite3\n\nThe following extensions are enabled by default:\n\nbcmath\nbz2 (7.1 and later)\ncommon (7.1 and later)\ncurl\ndba (7.1 and later)\nenchant (7.1 and later)\ngd\ninterbase (7.1 and later)\nintl\njson (5.6 and later)\nldap (7.1 and later)\nmbstring (7.1 and later)\nmcrypt (5.6 and earlier)\nmysql\nmysqli (not in 7.1)\nmysqlnd (not in 7.1)\nodbc (7.1 and later)\nopenssl\npdo (not in 7.1)\npdo_mysql (not in 7.1)\npdo_sqlite (not in 7.1)\npgsql (7.1 and later)\npspell (7.1 and later)\nreadline (7.1 and later)\nrecode (7.1 and later)\nsnmp (7.1 and later)\nsoap (7.1 and later)\nsodium (7.2)\nsqlite3\nsockets (7.0 and later)\nsybase (7.1 and later)\ntidy (7.1 and later)\nxml (7.1 and later)\nxmlrpc (7.1 and later)\nzendopcache (5.4 only) / opcache (5.5 and later)\nzip (7.1 and later)\n\nYou can disable those by adding them to the disabled_extensions list.\nThis is the complete list of extensions that can be enabled:\n\n\n\nExtension\n5.4\n5.5\n5.6\n7.0\n7.1\n7.2\n7.3\n7.4\n\n\n\n\namqp\n\n\n\n*\n*\n*\n*\n*\n\n\napc\n*\n*\n\n\n\n\n\n\n\n\napcu\n*\n\n*\n*\n*\n*\n*\n*\n\n\napcu_bc\n\n\n\n*\n*\n*\n*\n*\n\n\napplepay\n\n\n\n*\n*\n\n\n*\n\n\nbcmath\n\n\n\n*\n*\n*\n*\n*\n\n\nblackfire\n*\n*\n*\n*\n*\n*\n*\n*\n\n\nbz2\n\n\n\n*\n*\n*\n*\n*\n\n\ncalendar\n\n\n\n*\n*\n*\n*\n*\n\n\nctype\n\n\n\n*\n*\n*\n*\n*\n\n\ncurl\n*\n*\n*\n*\n*\n*\n*\n*\n\n\ndba\n\n\n\n*\n*\n*\n*\n*\n\n\ndom\n\n\n\n*\n*\n*\n*\n*\n\n\nenchant\n*\n*\n*\n*\n*\n*\n*\n*\n\n\nevent\n\n\n\n\n*\n*\n*\n*\n\n\nexif\n\n\n\n*\n*\n*\n*\n*\n\n\nffi\n\n\n\n\n\n\n\n*\n\n\nfileinfo\n\n\n\n*\n*\n*\n*\n*\n\n\nftp\n\n\n\n*\n*\n*\n*\n*\n\n\ngd\n*\n*\n*\n*\n*\n*\n*\n*\n\n\ngearman\n*\n*\n*\n\n\n\n\n\n\n\ngeoip\n*\n*\n*\n*\n*\n*\n*\n*\n\n\ngettext\n\n\n\n*\n*\n*\n*\n*\n\n\ngmp\n*\n*\n*\n*\n*\n*\n*\n*\n\n\nhttp\n*\n*\n\n\n\n\n\n*\n\n\niconv\n\n\n\n*\n*\n*\n*\n*\n\n\nigbinary\n\n\n\n*\n*\n*\n*\n*\n\n\nimagick\n*\n*\n*\n*\n*\n*\n*\n\n\n\nimap\n*\n*\n*\n*\n*\n*\n*\n*\n\n\ninterbase\n*\n*\n*\n*\n*\n*\n*\n*\n\n\nintl\n*\n*\n*\n*\n*\n*\n*\n*\n\n\nioncube\n\n\n\n*\n*\n*\n\n\n\n\njson\n\n\n*\n*\n*\n*\n*\n*\n\n\nldap\n*\n*\n*\n*\n*\n*\n*\n*\n\n\nmailparse\n\n\n\n*\n*\n*\n\n*\n\n\nmbstring\n\n\n\n*\n*\n*\n*\n*\n\n\nmcrypt\n*\n*\n*\n*\n*\n\n\n\n\n\nmemcache\n*\n*\n*\n\n\n\n\n\n\n\nmemcached\n*\n*\n*\n*\n*\n*\n\n*\n\n\nmongo\n*\n*\n*\n\n\n\n\n\n\n\nmongodb\n\n\n\n*\n*\n*\n*\n\n\n\nmsgpack\n\n\n*\n*\n*\n*\n\n*\n\n\nmssql\n*\n*\n*\n\n\n\n\n\n\n\nmysql\n*\n*\n*\n\n\n\n\n\n\n\nmysqli\n*\n*\n*\n*\n*\n*\n*\n*\n\n\nmysqlnd\n*\n*\n*\n*\n*\n*\n*\n*\n\n\nnewrelic\n\n\n*\n*\n*\n*\n*\n*\n\n\noauth\n\n\n\n*\n*\n*\n*\n*\n\n\nodbc\n*\n*\n*\n*\n*\n*\n*\n*\n\n\nopcache\n\n*\n*\n*\n*\n*\n*\n*\n\n\nopenssl\n\n\n\n\n\n\n\n\n\n\npdo\n*\n*\n*\n*\n*\n*\n*\n*\n\n\npdo_dblib\n*\n*\n*\n*\n*\n*\n*\n*\n\n\npdo_firebird\n*\n*\n*\n*\n*\n*\n*\n*\n\n\npdo_mysql\n*\n*\n*\n*\n*\n*\n*\n*\n\n\npdo_odbc\n*\n*\n*\n*\n*\n*\n*\n*\n\n\npdo_pgsql\n*\n*\n*\n*\n*\n*\n*\n*\n\n\npdo_sqlite\n*\n*\n*\n*\n*\n*\n*\n*\n\n\npdo_sqlsrv\n\n\n\n*\n*\n*\n\n*\n\n\nhttp\n\n\n*\n\n\n\n\n*\n\n\npgsql\n*\n*\n*\n*\n*\n*\n*\n*\n\n\nphar\n\n\n\n*\n*\n*\n*\n*\n\n\npinba\n*\n*\n*\n\n\n\n\n\n\n\nposix\n\n\n\n*\n*\n*\n*\n*\n\n\npropro\n\n\n*\n\n\n\n\n\n\n\npspell\n*\n*\n*\n*\n*\n*\n*\n*\n\n\npthreads\n\n\n\n\n*\n\n\n\n\n\nraphf\n\n\n*\n\n\n\n\n*\n\n\nreadline\n*\n*\n*\n*\n*\n*\n*\n*\n\n\nrecode\n*\n*\n*\n*\n*\n*\n*\n*\n\n\nredis\n*\n*\n*\n*\n*\n*\n*\n\n\n\nshmop\n\n\n\n*\n*\n*\n*\n*\n\n\nsimplexml\n\n\n\n*\n*\n*\n*\n*\n\n\nsnmp\n*\n*\n*\n*\n*\n*\n*\n*\n\n\nsoap\n\n\n\n*\n*\n*\n*\n*\n\n\nsockets\n\n\n\n*\n*\n*\n*\n*\n\n\nsodium\n\n\n\n\n\n*\n*\n*\n\n\nsourceguardian\n\n\n\n*\n*\n\n\n\n\n\nspplus\n*\n*\n\n\n\n\n\n\n\n\nsqlite3\n*\n*\n*\n*\n*\n*\n*\n*\n\n\nsqlsrv\n\n\n\n*\n*\n*\n\n*\n\n\nssh2\n*\n*\n*\n*\n*\n*\n*\n*\n\n\nsysvmsg\n\n\n\n*\n*\n*\n*\n*\n\n\nsysvsem\n\n\n\n*\n*\n*\n*\n*\n\n\nsysvshm\n\n\n\n*\n*\n*\n*\n*\n\n\ntideways\n\n\n\n*\n*\n*\n*\n*\n\n\ntideways-xhprof\n\n\n\n*\n*\n*\n*\n*\n\n\ntidy\n*\n*\n*\n*\n*\n*\n*\n*\n\n\ntokenizer\n\n\n\n*\n*\n*\n*\n*\n\n\nuuid\n\n\n\n\n*\n*\n*\n*\n\n\nwddx\n\n\n\n*\n*\n*\n*\n*\n\n\nxcache\n*\n*\n\n\n\n\n\n\n\n\nxhprof\n*\n*\n*\n\n\n\n\n\n\n\nxml\n\n\n\n*\n*\n*\n*\n*\n\n\nxmlreader\n\n\n\n*\n*\n*\n*\n*\n\n\nxmlrpc\n*\n*\n*\n*\n*\n*\n*\n*\n\n\nxmlwriter\n\n\n\n*\n*\n*\n*\n*\n\n\nxsl\n*\n*\n*\n*\n*\n*\n*\n*\n\n\nyaml\n\n\n\n\n*\n*\n*\n*\n\n\nzbarcode\n\n\n\n*\n*\n*\n*\n\n\n\nzendopcache\n*\n\n\n\n\n\n\n\n\n\nzip\n\n\n\n*\n*\n*\n*\n*\n\n\n\n\nnote\nYou can check out the output of ls /etc/php5/mods-available to\nsee the up-to-date complete list of extensions after you SSH into\nyour environment. For PHP 7, use ls /etc/php/*/mods-available.\n\nCustom PHP extensions\nIt is possible to use an extension not listed here but it takes slightly more work:\n\nDownload the .so file for the extension as part of your build hook using curl or similar. It can also be added to your Git repository if the file is not publicly downloadable, although committing large binary blobs to Git is generally not recommended.\n\nProvide a custom php.ini file in the application root (as a sibling of your .platform.app.yaml file) that loads the extension using an absolute path. For example, if the extension is named spiffy.so and is in the root of your application, you would have a php.ini file that reads:\n\n\nextension=/app/spiffy.so\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Languages", "title": "Python", "url": "/languages/python.html", "documentId": "70c6e99ea2b109de9fca61443c23cbe50d39ca6a", "text": "\n                        \n                            \n                                \n                                \n                                Python\nPlatform.sh supports deploying Python applications. Your application can use WSGI-based (Gunicorn / uWSGI) application server, Tornado, Twisted, or Python 3.5+ asyncio server.\nSupported\n\n2.7\n3.5\n3.6\n3.7\n3.8\n\nSupport libraries\nWhile it is possible to read the environment directly from your application, it is generally easier and more robust to use the platformshconfig pip library which handles decoding of service credential information for you.\nWSGI-based configuration\nIn this example, we use Gunicorn to run our WSGI application.  Configure the .platform.app.yaml file with a few key settings as listed below, a complete example is included at the end of this section.\n\nSpecify the language of your application (available versions are listed above):\n type: 'python:3.8'\n\nBuild your application with the build hook. Assuming you have your pip dependencies stored in requirements.txt and a setup.py at the root of your application folder to execute build steps:\nhooks:\n  build: |\n    pip install -r requirements.txt\n    pip install -e .\n    pip install gunicorn\n\nThese are installed as global dependencies in your environment.\n\nConfigure the command you use to start serving your application (this must be a foreground-running process) under the web section, e.g.:\nweb:\n  commands:\n    start: \"gunicorn -b 0.0.0.0:$PORT project.wsgi:application\"\n\nThis assumes the WSGI file is project/wsgi.py and the WSGI application object is named application in the WSGI file.\n\nDefine the web locations your application is using:\nweb:\n  locations:\n    \"/\":\n      root: \"\"\n      passthru: true\n      allow: false\n    \"/static\":\n      root: \"static/\"\n      allow: true\n\nThis configuration asks our web server to handle HTTP requests at \"/static\" to serve static files stored in /app/static/ folder while everything else is forwarded to your application server.\n\nCreate any Read/Write mounts. The root file system is read only.  You must explicitly describe writable mounts.\nmounts:\n    tmp:\n        source: local\n        source_path: tmp\n    logs:\n        source: local\n        source_path: logs\n\nThis setting allows your application writing files to /app/tmp and have logs stored in /app/logs.\n\n\nThen, set up the routes to your application in .platform/routes.yaml.\n   \"https://{default}/\":\n     type: upstream\n     upstream: \"app:http\"\n\nHere is the complete .platform.app.yaml file:\nname: app\ntype: python:2.7\n\nweb:\n  commands:\n    start: \"gunicorn -b $PORT project.wsgi:application\"\n  locations:\n    \"/\":\n      root: \"\"\n      passthru: true\n      allow: false\n    \"/static\":\n      root: \"static/\"\n      allow: true\n\nhooks:\n  build: |\n    pip install -r requirements.txt\n    pip install -e .\n    pip install gunicorn\n\nmounts:\n   tmp:\n       source: local\n       source_path: tmp\n   logs:\n       source: local\n       source_path: logs\n\ndisk: 512\n\nUsing the asyncio module\nThe above Gunicorn based WSGI example can be modified to use the Python 3.5+ asyncio module.\n\nChange the type to python:3.6.\nChange the start command to use asyncio.\nweb:\n  commands:\n    start: \"gunicorn -b $PORT -k gaiohttp project.wsgi:application\"\n\n\nAdd aiohttp as pip dependency in your build hook.\nhooks:\n  build: |\n    pip install -r requirements.txt\n    pip install -e .\n    pip install gunicorn aiohttp\n\n\n\nAccessing services\nTo access various services with Python, see the following examples.  The individual service pages have more information on configuring each service.\nElasticsearchKafkaMemcachedMongoDBMySQLPostgreSQLRabbitMQRedisSolrimport elasticsearch\nfrom platformshconfig import Config\n\ndef usage_example():\n\n    # Create a new Config object to ease reading the Platform.sh environment variables.\n    # You can alternatively use os.environ yourself.\n    config = Config()\n\n    # Get the credentials to connect to the Elasticsearch service.\n    credentials = config.credentials('elasticsearch')\n\n    try:\n        # The Elasticsearch library lets you connect to multiple hosts.\n        # On Platform.sh Standard there is only a single host so just register that.\n        hosts = {\n            \"scheme\": credentials['scheme'],\n            \"host\": credentials['host'],\n            \"port\": credentials['port']\n        }\n\n        # Create an Elasticsearch client object.\n        client = elasticsearch.Elasticsearch([hosts])\n\n        # Index a few documents\n        es_index = 'my_index'\n        es_type = 'People'\n\n        params = {\n            \"index\": es_index,\n            \"type\": es_type,\n            \"body\": {\"name\": ''}\n        }\n\n        names = ['Ada Lovelace', 'Alonzo Church', 'Barbara Liskov']\n\n        ids = {}\n\n        for name in names:\n            params['body']['name'] = name\n            ids[name] = client.index(index=params[\"index\"], doc_type=params[\"type\"], body=params['body'])\n\n        # Force just-added items to be indexed.\n        client.indices.refresh(index=es_index)\n\n        # Search for documents.\n        result = client.search(index=es_index, body={\n            'query': {\n                'match': {\n                    'name': 'Barbara Liskov'\n                }\n            }\n        })\n\n        table = '''&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;&lt;th&gt;ID&lt;/th&gt;&lt;th&gt;Name&lt;/th&gt;&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;'''\n\n        if result['hits']['hits']:\n            for record in result['hits']['hits']:\n                table += '''&lt;tr&gt;&lt;td&gt;{0}&lt;/td&gt;&lt;td&gt;{1}&lt;/td&gt;&lt;tr&gt;\\n'''.format(record['_id'], record['_source']['name'])\n            table += '''&lt;/tbody&gt;\\n&lt;/table&gt;\\n'''\n\n        # Delete documents.\n        params = {\n            \"index\": es_index,\n            \"type\": es_type,\n        }\n\n        for name in names:\n            client.delete(index=params['index'], doc_type=params['type'], id=ids[name]['_id'])\n\n        return table\n\n    except Exception as e:\n        return e\nfrom json import dumps\nfrom json import loads\nfrom kafka import KafkaConsumer, KafkaProducer\nfrom platformshconfig import Config\n\n\ndef usage_example():\n    # Create a new Config object to ease reading the Platform.sh environment variables.\n    # You can alternatively use os.environ yourself.\n    config = Config()\n    # Get the credentials to connect to the Kafka service.\n    credentials = config.credentials('kafka')\n    \n    try:\n        kafka_server = '{}:{}'.format(credentials['host'], credentials['port'])\n        \n        # Producer\n        producer = KafkaProducer(\n            bootstrap_servers=[kafka_server],\n            value_serializer=lambda x: dumps(x).encode('utf-8')\n        )\n        for e in range(10):\n            data = {'number' : e}\n            producer.send('numtest', value=data)\n        \n        # Consumer\n        consumer = KafkaConsumer(\n            bootstrap_servers=[kafka_server],\n            auto_offset_reset='earliest'\n        )\n        \n        consumer.subscribe(['numtest'])\n        \n        output = ''\n        # For demonstration purposes so it doesn't block.\n        for e in range(10):\n            message = next(consumer)\n            output += str(loads(message.value.decode('UTF-8'))[\"number\"]) + ', '\n\n        # What a real implementation would do instead.\n        # for message in consumer:\n        #     output += loads(message.value.decode('UTF-8'))[\"number\"]\n\n        return output\n    \n    except Exception as e:\n        return e\n\nimport pymemcache\nfrom platformshconfig import Config\n\n\ndef usage_example():\n\n    # Create a new Config object to ease reading the Platform.sh environment variables.\n    # You can alternatively use os.environ yourself.\n    config = Config()\n\n    # Get the credentials to connect to the Memcached service.\n    credentials = config.credentials('memcached')\n\n    try:\n        # Try connecting to Memached server.\n        memcached = pymemcache.Client((credentials['host'], credentials['port']))\n        memcached.set('Memcached::OPT_BINARY_PROTOCOL', True)\n\n        key = \"Deploy_day\"\n        value = \"Friday\"\n\n        # Set a value.\n        memcached.set(key, value)\n\n        # Read it back.\n        test = memcached.get(key)\n\n        return 'Found value &lt;strong&gt;{0}&lt;/strong&gt; for key &lt;strong&gt;{1}&lt;/strong&gt;.'.format(test.decode(\"utf-8\"), key)\n\n    except Exception as e:\n        return e\nfrom pymongo import MongoClient\nfrom platformshconfig import Config\n\n\ndef usage_example():\n\n    # Create a new Config object to ease reading the Platform.sh environment variables.\n    # You can alternatively use os.environ yourself.\n    config = Config()\n\n    # The 'database' relationship is generally the name of primary SQL database of an application.\n    # It could be anything, though, as in the case here here where it's called \"mongodb\".\n    credentials = config.credentials('mongodb')\n\n    try:\n        formatted = config.formatted_credentials('mongodb', 'pymongo')\n\n        server = '{0}://{1}:{2}@{3}'.format(\n            credentials['scheme'],\n            credentials['username'],\n            credentials['password'],\n            formatted\n        )\n\n        client = MongoClient(server)\n\n        collection = client.main.starwars\n\n        post = {\n            \"name\": \"Rey\",\n            \"occupation\": \"Jedi\"\n        }\n\n        post_id = collection.insert_one(post).inserted_id\n\n        document = collection.find_one(\n            {\"_id\": post_id}\n        )\n\n        # Clean up after ourselves.\n        collection.drop()\n\n        return 'Found {0} ({1})&lt;br /&gt;'.format(document['name'], document['occupation'])\n\n    except Exception as e:\n        return e\nimport pymysql\nfrom platformshconfig import Config\n\n\ndef usage_example():\n\n    # Create a new Config object to ease reading the Platform.sh environment variables.\n    # You can alternatively use os.environ yourself.\n    config = Config()\n\n    # The 'database' relationship is generally the name of primary SQL database of an application.\n    # That's not required, but much of our default automation code assumes it.'\n    credentials = config.credentials('database')\n\n    try:\n        # Connect to the database using PDO. If using some other abstraction layer you would inject the values\n        # from `database` into whatever your abstraction layer asks for.\n\n        conn = pymysql.connect(host=credentials['host'],\n                               port=credentials['port'],\n                               database=credentials['path'],\n                               user=credentials['username'],\n                               password=credentials['password'])\n\n        sql = '''\n                CREATE TABLE People (\n                id SERIAL PRIMARY KEY,\n                name VARCHAR(30) NOT NULL,\n                city VARCHAR(30) NOT NULL\n                )\n                '''\n\n        cur = conn.cursor()\n        cur.execute(sql)\n\n        sql = '''\n                INSERT INTO People (name, city) VALUES\n                ('Neil Armstrong', 'Moon'),\n                ('Buzz Aldrin', 'Glen Ridge'),\n                ('Sally Ride', 'La Jolla');\n                '''\n\n        cur.execute(sql)\n\n        # Show table.\n        sql = '''SELECT * FROM People'''\n        cur.execute(sql)\n        result = cur.fetchall()\n\n        table = '''&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;City&lt;/th&gt;&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;'''\n\n        if result:\n            for record in result:\n                table += '''&lt;tr&gt;&lt;td&gt;{0}&lt;/td&gt;&lt;td&gt;{1}&lt;/td&gt;&lt;tr&gt;\\n'''.format(record[1], record[2])\n            table += '''&lt;/tbody&gt;\\n&lt;/table&gt;\\n'''\n\n        # Drop table\n        sql = '''DROP TABLE People'''\n        cur.execute(sql)\n\n        # Close communication with the database\n        cur.close()\n        conn.close()\n\n        return table\n\n    except Exception as e:\n        return e\nimport psycopg2\nfrom platformshconfig import Config\n\n\ndef usage_example():\n    # Create a new Config object to ease reading the Platform.sh environment variables.\n    # You can alternatively use os.environ yourself.\n    config = Config()\n\n    # The 'database' relationship is generally the name of primary SQL database of an application.\n    # That's not required, but much of our default automation code assumes it.' \\\n    database = config.credentials('postgresql')\n\n    try:\n        # Connect to the database.\n        conn_params = {\n            'host': database['host'],\n            'port': database['port'],\n            'dbname': database['path'],\n            'user': database['username'],\n            'password': database['password']\n        }\n\n        conn = psycopg2.connect(**conn_params)\n\n        # Open a cursor to perform database operations.\n        cur = conn.cursor()\n\n        cur.execute(\"DROP TABLE IF EXISTS People\")\n\n        # Creating a table.\n        sql = '''\n                CREATE TABLE IF NOT EXISTS People (\n                id SERIAL PRIMARY KEY,\n                name VARCHAR(30) NOT NULL,\n                city VARCHAR(30) NOT NULL\n                )\n                '''\n\n        cur.execute(sql)\n\n        # Insert data.\n        sql = '''\n                INSERT INTO People (name, city) VALUES\n                ('Neil Armstrong', 'Moon'),\n                ('Buzz Aldrin', 'Glen Ridge'),\n                ('Sally Ride', 'La Jolla');\n                '''\n\n        cur.execute(sql)\n\n        # Show table.\n        sql = '''SELECT * FROM People'''\n        cur.execute(sql)\n        result = cur.fetchall()\n\n        table = '''&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;City&lt;/th&gt;&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;'''\n\n        if result:\n            for record in result:\n                table += '''&lt;tr&gt;&lt;td&gt;{0}&lt;/td&gt;&lt;td&gt;{1}&lt;/td&gt;&lt;tr&gt;\\n'''.format(record[1], record[2])\n            table += '''&lt;/tbody&gt;\\n&lt;/table&gt;\\n'''\n\n        # Drop table\n        sql = \"DROP TABLE People\"\n        cur.execute(sql)\n\n        # Close communication with the database\n        cur.close()\n        conn.close()\n\n        return table\n\n    except Exception as e:\n        return e\n\nimport pika\nfrom platformshconfig import Config\n\n\ndef usage_example():\n    # Create a new Config object to ease reading the Platform.sh environment variables.\n    # You can alternatively use os.environ yourself.\n    config = Config()\n\n    # Get the credentials to connect to the RabbitMQ service.\n    credentials = config.credentials('rabbitmq')\n\n    try:\n        # Connect to the RabbitMQ server\n        creds = pika.PlainCredentials(credentials['username'], credentials['password'])\n        parameters = pika.ConnectionParameters(credentials['host'], credentials['port'], credentials=creds)\n\n        connection = pika.BlockingConnection(parameters)\n        channel = connection.channel()\n\n        # Check to make sure that the recipient queue exists\n        channel.queue_declare(queue='deploy_days')\n\n        # Try sending a message over the channel\n        channel.basic_publish(exchange='',\n                              routing_key='deploy_days',\n                              body='Friday!')\n\n        # Receive the message\n        def callback(ch, method, properties, body):\n            print(\" [x] Received {}\".format(body))\n\n        # Tell RabbitMQ that this particular function should receive messages from our 'hello' queue\n        channel.basic_consume('deploy_days',\n                              callback,\n                              auto_ack=False)\n\n        # This blocks on waiting for an item from the queue, so comment it out in this demo script.\n        # print(' [*] Waiting for messages. To exit press CTRL+C')\n        # channel.start_consuming()\n\n        connection.close()\n\n        return \" [x] Sent 'Friday!'&lt;br/&gt;\"\n\n    except Exception as e:\n        return e\nfrom redis import Redis\nfrom platformshconfig import Config\n\n\ndef usage_example():\n\n    # Create a new config object to ease reading the Platform.sh environment variables.\n    # You can alternatively use os.environ yourself.\n    config = Config()\n\n    # Get the credentials to connect to the Redis service.\n    credentials = config.credentials('redis')\n\n    try:\n        redis = Redis(credentials['host'], credentials['port'])\n\n        key = \"Deploy day\"\n        value = \"Friday\"\n\n        # Set a value\n        redis.set(key, value)\n\n        # Read it back\n        test = redis.get(key)\n\n        return 'Found value &lt;strong&gt;{0}&lt;/strong&gt; for key &lt;strong&gt;{1}&lt;/strong&gt;.'.format(test.decode(\"utf-8\"), key)\n\n    except Exception as e:\n        return e\n\nimport pysolr\nfrom xml.etree import ElementTree as et\nfrom platformshconfig import Config\n\n\ndef usage_example():\n\n    # Create a new Config object to ease reading the Platform.sh environment variables.\n    # You can alternatively use os.environ yourself.\n    config = Config()\n\n    # Get the credentials to connect to the Solr service.\n    credentials = config.credentials('solr')\n\n    try:\n        formatted_url = config.formatted_credentials('solr', 'pysolr')\n\n        # Create a new Solr Client using config variables\n        client = pysolr.Solr(formatted_url)\n\n        # Add a document\n        message = ''\n        doc_1 = {\n            \"id\": 123,\n            \"name\": \"Valentina Tereshkova\"\n        }\n\n        result0 = client.add([doc_1])\n        client.commit()\n        message += 'Adding one document. Status (0 is success): {0} &lt;br /&gt;'.format(et.fromstring(result0)[0][0].text)\n\n        # Select one document\n        query = client.search('*:*')\n        message += '\\nSelecting documents (1 expected): {0} &lt;br /&gt;'.format(str(query.hits))\n\n        # Delete one document\n        result1 = client.delete(doc_1['id'])\n        client.commit()\n        message += '\\nDeleting one document. Status (0 is success): {0}'.format(et.fromstring(result1)[0][0].text)\n\n        return message\n\n    except Exception as e:\n        return e\n\nProject templates\nA number of project templates for Python applications are available on GitHub.  Not all of them are proactively maintained but all can be used as a starting point or reference for building your own website or web application.\nExamples\n\nPython UWSGI with Unix Sockets\n\nFrameworks\n\nDjango 1\nDjango 2\nFlask\n\nApplications\n\nMoin Moin\nPelican\nPyramid\nWagtail\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Node.js", "title": "Alternative Node.js install", "url": "/languages/nodejs/nvm.html", "documentId": "b0d24fa49422856576acd02ef5a83c89b13dbd47", "text": "\n                        \n                            \n                                \n                                \n                                How to use NVM to run different versions of Node.js\nNode Version Manager or NVM is a tool for managing multiple versions of Node.js in one installation. \nYou can use NVM with any of our container types that have node installed to change or update the version. This may be useful, for example, where a container has a Long Term Release (LTS) version available, but you would like to use the latest.\nInstalling NVM is done in the build hook of your .platform.app.yaml, which some additional calls to ensure that environment variables are set correctly.\nhooks:\n    build: |\n        unset NPM_CONFIG_PREFIX\n        curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.35.2/install.sh | dash\n        export NVM_DIR=\"$PLATFORM_APP_DIR/.nvm\"\n        [ -s \"$NVM_DIR/nvm.sh\" ] &amp;&amp; \\. \"$NVM_DIR/nvm.sh\"\n        nvm current\n        nvm install 9.5.0\n    deploy: |\n        unset NPM_CONFIG_PREFIX\n        export NVM_DIR=\"$PLATFORM_APP_DIR/.nvm\"\n        [ -s \"$NVM_DIR/nvm.sh\" ] &amp;&amp; \\. \"$NVM_DIR/nvm.sh\"\n        nvm use 9.5.0\n\nAnd in a .environment file in the root of your project:\n# This is necessary for nvm to work.\nunset NPM_CONFIG_PREFIX\n# Disable npm update notifier; being a read only system it will probably annoy you.\nexport NO_UPDATE_NOTIFIER=1\n# This loads nvm for general usage.\nexport NVM_DIR=\"$PLATFORM_APP_DIR/.nvm\"\n[ -s \"$NVM_DIR/nvm.sh\" ] &amp;&amp; \\. \"$NVM_DIR/nvm.sh\"\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Languages", "title": "Node.js", "url": "/languages/nodejs.html", "documentId": "747a0a91f175766a083802bd50dbc04e3aad36ca", "text": "\n                        \n                            \n                                \n                                \n                                Node.js\nNode.js is a popular JavaScript runtime built on Chrome's V8 JavaScript engine.  Platform.sh supports deploying Node.js applications quickly and easily. Using our Multi-App support you can build a micro-service oriented system mixing both Javascript and PHP applications.\nSupported versions\n\n6\n8\n10\n12\n\nIf you need other versions, take a look at our options for installing them with NVM.\nDeprecated versions\nSome versions with a minor (such as 8.9) are available but are not receiving security updates from upstream, so their use is not recommended. They will be removed at some point in the future.\nSupport libraries\nWhile it is possible to read the environment directly from your application, it is generally easier and more robust to use the platformsh-config NPM library which handles decoding of service credential information for you.\nConfiguration\nTo use Platform.sh and Node.js together, configure the .platform.app.yaml file with a few key settings, as described here (a complete example is included at the end).\n\nSpecify the language of your application (available versions are listed above):\n type: 'nodejs:12'\n\nSpecify your dependencies under the nodejs key, like this:\ndependencies:\n  nodejs:\n    pm2: \"^2.5.0\"\n\nThese are the global dependencies of your project (the ones you would have installed with npm install -g). Here we specify the pm2 process manager that will allow us to run the node process.\n\nConfigure the command you use to start serving your application (this must be a foreground-running process) under the web section, e.g.:\nweb:\n  commands:\n    start: \"PM2_HOME=/app/run pm2 start index.js --no-daemon\"\n\nIf there is a package.json file present at the root of your repository, Platform.sh will automatically install the dependencies. We suggest including the platformsh-config helper npm module, which makes it trivial to access the running environment.\n{\n  \"dependencies\": {\n    \"platformsh-config\": \"^2.0.0\"\n  }\n}\n\n\nNote:\nIf using the pm2 process manager to start your application, it is recommended that you do so directly in web.commands.start as described above, rather than by calling a separate script the contains that command. Calling pm2 start at web.commands.start from within a script, even with the --no-daemon flag, has been found to daemonize itself and block other processes (such as backups) with continuous respawns. \n\n\nCreate any Read/Write mounts. The root file system is read only. You must explicitly describe writable mounts. In (3) we set the home of the process manager to /app/run so this needs to be writable.\nmounts:\n    run:\n        source: local\n        source_path: run\n\n\nInclude any relevant commands needed to build and setup your application in the hooks section, e.g.:\nhooks:\n  build: |\n    npm install\n    npm run build\n    bower install\n\n\nSetup the routes to your Node.js application in .platform/routes.yaml.\n\"https://{default}/\":\n  type: upstream\n  upstream: \"app:http\"\n\n\n(Optional) If Platform.sh detects a package.json file in your repository, it will automatically include a default build flavor, that will run npm prune --userconfig .npmrc &amp;&amp; npm install --userconfig .npmrc. You can modify that process to use an alternative package manager by including the following in your .platform.app.yaml file:\nbuild:\n  flavor: none\n\nConsult the documentation specific to Node.js builds for more information.\n\n\nHere's a complete example that also serves static assets (.png from the /public directory):\nname: node\ntype: nodejs:12\n\nweb:\n  commands:\n    start: \"PM2_HOME=/app/run pm2 start index.js --no-daemon\"\n    #in this setup you will find your application stdout and stderr in /app/run/logs\n  locations:\n    \"/public\":\n      passthru: false\n      root: \"public\"\n      # Whether to allow files not matching a rule or not.\n      allow: true\n      rules:\n        '\\.png$':\n          allow: true\n          expires: -1\ndependencies:\n  nodejs:\n    pm2: \"^2.5.0\"\nmounts:\n   run:\n       source: local\n       source_path: run\ndisk: 512\n\nIn your application...\nFinally, make sure your Node.js application is configured to listen over the port given by the environment (here we use the platformsh helper and get it from config.port) that is available in the environment variable PORT.  Here's an example:\n// Load the http module to create an http server.\nconst http = require('http');\n\n// Load the Platform.sh configuration\nconst config = require('platformsh-config').config();\n\nconst server = http.createServer(function (request, response) {\n  response.writeHead(200, {\"Content-Type\": \"text/html\"});\n  response.end(\"&lt;html&gt;&lt;head&gt;&lt;title&gt;Hello Node.js&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;&lt;img src='public/js.png'&gt;Hello Node.js&lt;/h1&gt;&lt;h3&gt;Platform configuration:&lt;/h3&gt;&lt;pre&gt;\"+JSON.stringify(config, null, 4) + \"&lt;/pre&gt;&lt;/body&gt;&lt;/html&gt;\");\n});\n\nserver.listen(config.port);\n\nAccessing services\nTo access various services with Node.js, see the following examples.  The individual service pages have more information on configuring each service.\nElasticsearchMemcachedMongoDBMySQLPostgreSQLRedisSolrconst elasticsearch = require('elasticsearch');\nconst config = require(\"platformsh-config\").config();\n\nexports.usageExample = async function() {\n\n    const credentials = config.credentials('elasticsearch');\n\n    var client = new elasticsearch.Client({\n        host: `${credentials.host}:${credentials.port}`,\n    });\n\n\n    let index = 'my_index';\n    let type = 'People';\n\n    // Index a few document.\n    let names = ['Ada Lovelace', 'Alonzo Church', 'Barbara Liskov'];\n\n    let message = {\n        refresh: \"wait_for\",\n        body: []\n    };\n    names.forEach((name) =&gt; {\n        message.body.push({index: {_index: index, _type: type}});\n        message.body.push({name: name});\n    });\n    await client.bulk(message);\n\n    // Search for documents.\n    const response = await client.search({\n        index: index,\n        q: 'name:Barbara Liskov'\n    });\n\n    let output = '';\n\n    if(response.hits.total.value &gt; 0) {\n        output += `&lt;table&gt;\n        &lt;thead&gt;\n        &lt;tr&gt;&lt;th&gt;ID&lt;/th&gt;&lt;th&gt;Name&lt;/th&gt;&lt;/tr&gt;\n        &lt;/thead&gt;\n        &lt;tbody&gt;`;\n        response.hits.hits.forEach((record) =&gt; {\n            output += `&lt;tr&gt;&lt;td&gt;${record._id}&lt;/td&gt;&lt;td&gt;${record._source.name}&lt;/td&gt;&lt;/tr&gt;\\n`;\n        });\n        output += \"&lt;/tbody&gt;\\n&lt;/table&gt;\\n\";\n    }\n    else {\n        output = \"No records found.\";\n    }\n\n    // Clean up after ourselves.\n    response.hits.hits.forEach((record) =&gt; {\n        client.delete({\n            index: index,\n            type: type,\n            id: record._id,\n        });\n    });\n\n    return output;\n};\nconst Memcached = require('memcached');\nconst config = require(\"platformsh-config\").config();\nconst { promisify } = require('util');\n\nexports.usageExample = async function() {\n\n    const credentials = config.credentials('memcached');\n\n    let client = new Memcached(`${credentials.host}:${credentials.port}`);\n\n    // The MemcacheD client is not Promise-aware, so make it so.\n    const memcachedGet = promisify(client.get).bind(client);\n    const memcachedSet = promisify(client.set).bind(client);\n\n    let key = 'Deploy-day';\n    let value = 'Friday';\n\n    // Set a value.\n    await memcachedSet(key, value, 10);\n\n    // Read it back.\n    let test = await memcachedGet(key);\n\n    let output = `Found value &lt;strong&gt;${test}&lt;/strong&gt; for key &lt;strong&gt;${key}&lt;/strong&gt;.`;\n\n    return output;\n};\nconst mongodb = require('mongodb');\nconst config = require(\"platformsh-config\").config();\n\nexports.usageExample = async function() {\n    const credentials = config.credentials('mongodb');\n    const MongoClient = mongodb.MongoClient;\n\n    var client = await MongoClient.connect(config.formattedCredentials('mongodb', 'mongodb'));\n\n    let db = client.db(credentials[\"path\"]);\n\n    let collection = db.collection(\"startrek\");\n\n    const documents = [\n        {'name': 'James Kirk', 'rank': 'Admiral'},\n        {'name': 'Jean-Luc Picard', 'rank': 'Captain'},\n        {'name': 'Benjamin Sisko', 'rank': 'Prophet'},\n        {'name': 'Katheryn Janeway', 'rank': 'Captain'},\n    ];\n\n    await collection.insert(documents, {w: 1});\n\n    let result = await collection.find({rank:\"Captain\"}).toArray();\n\n    let output = '';\n\n    output += `&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Rank&lt;/th&gt;&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;`;\n\n    Object.keys(result).forEach((key) =&gt; {\n        output += `&lt;tr&gt;&lt;td&gt;${result[key].name}&lt;/td&gt;&lt;td&gt;${result[key].rank}&lt;/td&gt;&lt;/tr&gt;\\n`;\n    });\n\n    output += `&lt;/tbody&gt;\\n&lt;/table&gt;\\n`;\n\n    // Clean up after ourselves.\n    collection.remove();\n\n    return output;\n};\nconst mysql = require('mysql2/promise');\nconst config = require(\"platformsh-config\").config();\n\nexports.usageExample = async function() {\n\n    const credentials = config.credentials('database');\n\n    const connection = await mysql.createConnection({\n        host: credentials.host,\n        port: credentials.port,\n        user: credentials.username,\n        password: credentials.password,\n        database: credentials.path,\n    });\n\n    let sql = '';\n\n    // Creating a table.\n    sql = `CREATE TABLE IF NOT EXISTS People (\n        id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n            name VARCHAR(30) NOT NULL,\n            city VARCHAR(30) NOT NULL\n        )`;\n    await connection.query(sql);\n\n    // Insert data.\n    sql = `INSERT INTO People (name, city) VALUES\n    ('Neil Armstrong', 'Moon'),\n        ('Buzz Aldrin', 'Glen Ridge'),\n        ('Sally Ride', 'La Jolla');`;\n    await connection.query(sql);\n\n    // Show table.\n    sql = `SELECT * FROM People`;\n    let [rows] = await connection.query(sql);\n\n    let output = '';\n\n    if (rows.length &gt; 0) {\n        output +=`&lt;table&gt;\n            &lt;thead&gt;\n            &lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;City&lt;/th&gt;&lt;/tr&gt;\n            &lt;/thead&gt;\n            &lt;tbody&gt;`;\n\n        rows.forEach((row) =&gt; {\n            output += `&lt;tr&gt;&lt;td&gt;${row.name}&lt;/td&gt;&lt;td&gt;${row.city}&lt;/td&gt;&lt;/tr&gt;\\n`;\n        });\n\n        output += `&lt;/tbody&gt;\\n&lt;/table&gt;\\n`;\n    }\n\n    // Drop table.\n    sql = `DROP TABLE People`;\n    await connection.query(sql);\n\n    return output;\n};\nconst pg = require('pg');\nconst config = require(\"platformsh-config\").config();\n\nexports.usageExample = async function() {\n\n    const credentials = config.credentials('postgresql');\n\n    const client = new pg.Client({\n        host: credentials.host,\n        port: credentials.port,\n        user: credentials.username,\n        password: credentials.password,\n        database: credentials.path,\n    });\n\n    client.connect();\n\n    let sql = '';\n\n    // Creating a table.\n    sql = `CREATE TABLE IF NOT EXISTS People (\n      id SERIAL PRIMARY KEY,\n      name VARCHAR(30) NOT NULL,\n      city VARCHAR(30) NOT NULL\n      )`;\n    await client.query(sql);\n\n    // Insert data.\n    sql = `INSERT INTO People (name, city) VALUES \n        ('Neil Armstrong', 'Moon'), \n        ('Buzz Aldrin', 'Glen Ridge'), \n        ('Sally Ride', 'La Jolla');`;\n    await client.query(sql);\n\n    // Show table.\n    sql = `SELECT * FROM People`;\n    let result = await client.query(sql);\n\n    let output = '';\n\n    if (result.rows.length &gt; 0) {\n        output +=`&lt;table&gt;\n            &lt;thead&gt;\n            &lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;City&lt;/th&gt;&lt;/tr&gt;\n            &lt;/thead&gt;\n            &lt;tbody&gt;`;\n\n        result.rows.forEach((row) =&gt; {\n            output += `&lt;tr&gt;&lt;td&gt;${row.name}&lt;/td&gt;&lt;td&gt;${row.city}&lt;/td&gt;&lt;/tr&gt;\\n`;\n        });\n\n        output += `&lt;/tbody&gt;\\n&lt;/table&gt;\\n`;\n    }\n\n    // Drop table.\n    sql = `DROP TABLE People`;\n    await client.query(sql);\n\n    return output;\n};\nconst redis = require('redis');\nconst config = require(\"platformsh-config\").config();\nconst { promisify } = require('util');\n\nexports.usageExample = async function() {\n\n    const credentials = config.credentials('redis');\n\n    var client = redis.createClient(credentials.port, credentials.host);\n\n    // The Redis client is not Promise-aware, so make it so.\n    const redisGet = promisify(client.get).bind(client);\n    const redisSet = promisify(client.set).bind(client);\n\n    let key = 'Deploy day';\n    let value = 'Friday';\n\n    // Set a value.\n    await redisSet(key, value);\n\n    // Read it back.\n    let test = await redisGet(key);\n\n    let output = `Found value &lt;strong&gt;${test}&lt;/strong&gt; for key &lt;strong&gt;${key}&lt;/strong&gt;.`;\n\n    return output;\n};\nconst solr = require('solr-node');\nconst config = require(\"platformsh-config\").config();\n\nexports.usageExample = async function() {\n\n    let client = new solr(config.formattedCredentials('solr', 'solr-node'));\n\n    let output = '';\n\n    // Add a document.\n    let addResult = await client.update({\n        id: 123,\n        name: 'Valentina Tereshkova',\n    });\n\n    output += \"Adding one document. Status (0 is success): \" + addResult.responseHeader.status +  \"&lt;br /&gt;\\n\";\n\n    // Flush writes so that we can query against them.\n    await client.softCommit();\n\n    // Select one document:\n    let strQuery = client.query().q();\n    let writeResult = await client.search(strQuery);\n    output += \"Selecting documents (1 expected): \" + writeResult.response.numFound + \"&lt;br /&gt;\\n\";\n\n    // Delete one document.\n    let deleteResult = await client.delete({id: 123});\n    output += \"Deleting one document. Status (0 is success): \" + deleteResult.responseHeader.status + \"&lt;br /&gt;\\n\";\n\n    return output;\n};\n\nProject templates\nA number of project templates for Node.js applications and typical configurations are available on GitHub. Not all of them are proactively maintained but all can be used as a starting point or reference for building your own website or web application.\n\nGeneric Node.js\nParse\nNode.js-based microservices\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Languages", "title": "Lisp", "url": "/languages/lisp.html", "documentId": "cfdf36d8cb7165c2271ffb74e9d2ad5b76c38fff", "text": "\n                        \n                            \n                                \n                                \n                                Lisp\nPlatform.sh supports building and deploying applications written in Lisp using Common Lisp (the SBCL version) with ASDF and Quick Lisp support.  They are compiled during the Build phase, and support both committed dependencies and download-on-demand.\nSupported versions\n\n1.5\n\nTo specify a Lisp container, use the type property in your .platform.app.yaml.\ntype: 'lisp:1.5'\nAssumptions\nPlatform.sh is making assumptions about your application to provide a more streamlined experience. These assumptions are the following:\n\nYour .asd file is named like your system name. E.g. example.asd will have (defsystem example ...).\n\nPlatform.sh will then run (asdf:make :example) on your system to build a binary.\nIf you don't want these assumptions, you can disable this behavior by specifying in your .platform.app.yaml:\nbuild:\n    flavor: none\n\nDependencies\nThe recommended way to handle Lisp dependencies on Platform.sh is using ASDF. Commit a .asd file in your repository and the system will automatically download the dependencies using QuickLisp.\nQuickLisp options\nIf you wish to change the distributions that QuickLisp is using, you can specify those as follows, specifying a distribution name, its URL and, an optional version:\nruntime:\n    quicklisp:\n        &lt;distribution name&gt;:\n            url: \"...\"\n            version: \"...\"\n\nFor example:\nruntime:\n    quicklisp:\n        quicklisp:\n            url: 'http://beta.quicklisp.org/dist/quicklisp.txt'\n            version: '2019-07-11'\n\nPlatform.sh variables\nPlatform.sh exposes relationships and other configuration as environment variables. Most notably, it allows a program to determine at runtime what HTTP port it should listen on and what the credentials are to access other services.\nTo get the PORT environment variable (the port on which your web application is supposed to listen) you would:\n(parse-integer (uiop:getenv \"PORT\"))\n\nBuilding and running the application\nAssuming example.lisp and example.asd are present in your repository, the application will be automatically built on push.  You can then start it from the web.commands.start directive.  Note that the start command must run in the foreground. Should the program terminate for any reason it will be automatically restarted. In the example below we sleep for a very, very long time. You could also choose to join the thread of your web server, or use other methods to make sure the program does not terminate.\nThe following basic .platform.app.yaml file is sufficient to run most Lisp applications.\nname: app\ntype: lisp:1.5\nweb:\n    commands:\n        start: ./example\n    locations:\n        /:\n            allow: false\n            passthru: true\ndisk: 512\n\nNote that there will still be a proxy server in front of your application.  If desired, certain paths may be served directly by our router without hitting your application (for static files, primarily) or you may route all requests to the Lisp application unconditionally, as in the example above.\nAccessing Services\nThe services configuration is available in the environment variable PLATFORM_RELATIONSHIPS.\nTo parse them, add the dependencies to your .asd file:\n:depends-on (:jsown :babel :s-base64)\n\nThe following is an example of accessing a PostgreSQL instance:\n(defun relationships ()\n  (jsown:parse\n   (babel:octets-to-string\n    (with-input-from-string (in (uiop:getenv \"PLATFORM_RELATIONSHIPS\"))\n      (s-base64:decode-base64-bytes in)))))\n\nGiven a relationship defined in .platform.app.yaml:\nrelationships:\n  pg: postgresql:postgresql\n\nThe following would access that relationship, and provide your Lisp program the credentials to connect to a PostgreSQL instance. Add this to your .asd file:\n:depends-on (:postmodern)\n\nThen in your program you could access the PostgreSQL instance as follows:\n(defvar *pg-spec* nil)\n\n(defun setup-postgresql ()\n  (let* ((pg-relationship (first (jsown:val (relationships) \"pg\")))\n         (database (jsown:val pg-relationship \"path\"))\n         (username (jsown:val pg-relationship \"username\"))\n         (password (jsown:val pg-relationship \"password\"))\n         (host (jsown:val pg-relationship \"host\")))\n    (setf *pg-spec*\n      (list database username password host)))\n  (postmodern:with-connection *pg-spec*\n    (unless (member \"example_table\" (postmodern:list-tables t) :test #'string=)\n      (postmodern:execute \"create table example_table (\n    a_field TEXT NOT NULL UNIQUE,\n    another_field TEXT NOT NULL UNIQUE\n\"))))\n\nProject templates\nPlatform.sh offers a project template for Lisp applications using the structure described above.  It can be used as a starting point or reference for building your own website or web application.\nThe following is a simple example of a Hunchentoot based web application (you can find the corresponding .asd and Platform.sh .yaml files in the linked Github repository):\n(defpackage #:example\n  (:use :hunchentoot :cl-who :cl)\n  (:export main))\n\n(in-package #:example)\n\n(define-easy-handler (greet :uri \"/hello\") (name)\n  (with-html-output-to-string (s) (htm (:body (:h1 \"hello, \" (str name))))))\n\n(defun main ()\n  (let ((acceptor (make-instance\n                   'easy-acceptor\n                   :port (parse-integer (uiop:getenv \"PORT\")))))\n    (start acceptor)\n    (sleep most-positive-fixnum)))\n\nNotice how we get the PORT from the environment, and how we sleep at the end, as (start acceptor) will immediately yield and Platform.sh requires applications to run in the foreground.\nHunchentoot Lisp application\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Languages", "title": "PHP", "url": "/languages/php.html", "documentId": "2478f741089b05675d6b8955dccf3c2bcbb450b5", "text": "\n                        \n                            \n                                \n                                \n                                PHP Support\n\n\nSupported versions\nDeprecated versions\nSupport libraries\nAlternate start commands\nExpanded dependencies\nOpcache preloading\nFFI\nDebug PHP-FPM\nAccessing services\nRuntime configuration\nProject templates\nApplications\nExamples\nFrameworks\n\n\n\n\nPHP is a popular scripting language designed especially for the web. It currently powers over 80% of websites.\nSupported versions\n\n7.2\n7.3\n7.4\n\nNote that as of PHP 7.1 we use the Zend Thread Safe (ZTS) version of PHP.\nTo specify a PHP container, use the type property in your .platform.app.yaml.\ntype: 'php:7.4'\nDeprecated versions\nThe following versions are available but are not receiving security updates from upstream, so their use is not recommended. They will be removed at some point in the future.\n\n5.4\n5.5\n5.6\n7.0\n7.1\n\nSupport libraries\nWhile it is possible to read the environment directly from your application, it is generally easier and more robust to use the platformsh/config-reader Composer library which handles decoding of service credential information for you.\nAlternate start commands\nPHP is most commonly run in a CGI mode, using PHP-FPM. That is the default on Platform.sh. However, you can also start alternative processes if desired, such as if you're running an Async PHP daemon, a thread-based worker process, etc. To do so, simply specify an alternative start command in platform.app.yaml, similar to the following:\nweb:\n    commands:\n        start: php run.php\n    upstream:\n            socket_family: tcp\n            protocol: http\n\nThe above configuration will execute the run.php script in the application root when the container starts using the PHP-CLI SAPI, just before the deploy hook runs, but will not launch PHP-FPM. It will also tell the front-controller (Nginx) to connect to your application via a TCP socket, which will be specified in the PORT environment variable. Note that the start command must run in the foreground.\nIf not specified, the effective default start command varies by PHP version:\n\nOn PHP 5.x, it's /usr/sbin/php5-fpm.\nOn PHP 7.0, it's /usr/sbin/php-fpm7.0.\nOn PHP 7.1, it's /usr/sbin/php-fpm7.1-zts.\nOn PHP 7.2, it's /usr/sbin/php-fpm7.2-zts.\nOn PHP 7.3, it's /usr/sbin/php-fpm7.3-zts.\nOn PHP 7.4, it's /usr/sbin/php-fpm7.4-zts.\n\nWhile you can call it manually that is generally not necessary. Note that PHP-FPM cannot run simultaneously along with another persistent process (such as ReactPHP or Amp). If you need both they will have to run in separate containers.\nExpanded dependencies\nIn addition to the standard dependencies format, it is also possible to specify alternative repositories for use by Composer.  The standard format like so:\ndependencies:\n    php:\n        \"platformsh/client\": \"dev-master\"\n\nis equivalent to composer require platform/client dev-master.  However, you can also specify explicit require and repositories blocks:\ndependencies:\n    php:\n        require:\n            \"platformsh/client\": \"dev-master\"\n        repositories:\n            - type: vcs\n              url: \"git@github.com:platformsh/platformsh-client-php.git\"\n\nThat would install platformsh/client from the alternate repository specified, as a global dependency.  That is, it is equivalent to the following composer.json file:\n{\n    \"repositories\": [\n        {\n            \"type\": \"vcs\",\n            \"url\":  \"git@github.com:platformsh/platformsh-client-php.git\"\n        }\n    ],\n    \"require\": {\n        \"platformsh/client\": \"dev-master\"\n    }\n}\n\nThat allows you to install a forked version of a global dependency from a custom repository.\nOpcache preloading\nPHP 7.4 introduced a new feature called Opcache Preloading, which allows you to load selected files into shared memory when PHP-FPM starts.  That means functions and classes in those files are always available and do not need to be autoloaded, at the cost of any changes to those files requiring a PHP-FPM restart.  Since PHP-FPM restarts anyway when a new deploy happens this feature is a major win on Platform.sh, and we recommend using it aggressively.\nTo enable preloading, add a php.ini value that specifies a preload script.  Any php.ini mechanism will work, but using a variable in .platform.app.yaml is the recommended approach:\nvariables:\n    php:\n        opcache.preload: 'preload.php'\n\nThe opcache.preload value is evaluated as a file path relative to the application root (where .platform.app.yaml is), and it may be any PHP script that calls opcache_compile_file().  The following example will preload all .php files anywhere in the vendor directory:\n$directory = new RecursiveDirectoryIterator(getenv('PLATFORM_APP_DIR') . '/vendor');\n$iterator = new RecursiveIteratorIterator($directory);\n$regex = new RegexIterator($iterator, '/^.+\\.php$/i', RecursiveRegexIterator::GET_MATCH);\n\nforeach ($regex as $key =&gt; $file) {\n    // This is the important part!\n    opcache_compile_file($file[0]);\n}\n\nNote: Preloading all .php files may not be optimal for your application, and may even introduce errors.  Your application framework may provide recommendations or a pre-made presload script to use instead.  Determining an optimal preloading strategy is the user's responsibility.\nFFI\nPHP 7.4 introduced support for Foreign Function Interfaces (FFI), which allows user-space code to bridge to existing C-ABI-compatible libraries.  FFI is fully supported on Platform.sh.\nNote: FFI is only intended for advanced use cases, and is rarely a net win for routine web requests.  Use with caution.\nThere are a few steps to leveraging FFI:\n\nEnable the FFI extension in .platform.app.yaml:\n runtime:\n     extensions:\n         - ffi\n\n\nSpecify a preload file in which you can call FFI::load().  Using FFI::load() in preload will be considerably faster than loading the linked library on each request or script run.\n\nEnsure the library is available locally, but not in a web-accessible directory.  .so files may included in your repository, downloaded i your build hook, or compiled in your build hook.  If compiling C code, gcc is available by default.  If compiling Rust code, you can download the Rust compiler in the build hook.\n\nFor running FFI from the command line, you will need to enable the opcache for command line scripts in addition to the preloader.  The standard pattern for the command would be php -d opcache.preload=\"your-preload-script.php\" -d opcache.enable_cli=true your-cli-script.php.\n\n\nA working FFI example is available online for both C and Rust.\nDebug PHP-FPM\nIf you want to inspect what's going on with PHP-FPM, you can install this small CLI:\ndependencies:\n  php:\n    wizaplace/php-fpm-status-cli: \"^1.0\"\n\nThen when you are connected to your project over SSH you can run:\n$ php-fpm-status --socket=unix://$SOCKET --path=/-/status --full\n\nAccessing services\nTo access various services with PHP, see the following examples.  The individual service pages have more information on configuring each service.\nElasticsearchMemcachedMongoDBMySQLPostgreSQLRabbitMQRedisSolr&lt;?php\n\ndeclare(strict_types=1);\n\nuse Elasticsearch\\ClientBuilder;\nuse Platformsh\\ConfigReader\\Config;\n\n// Create a new config object to ease reading the Platform.sh environment variables.\n// You can alternatively use getenv() yourself.\n$config = new Config();\n\n// Get the credentials to connect to the Elasticsearch service.\n$credentials = $config-&gt;credentials('elasticsearch');\n\ntry {\n    // The Elasticsearch library lets you connect to multiple hosts.\n    // On Platform.sh Standard there is only a single host so just\n    // register that.\n    $hosts = [\n        [\n            'scheme' =&gt; $credentials['scheme'],\n            'host' =&gt; $credentials['host'],\n            'port' =&gt; $credentials['port'],\n        ]\n    ];\n\n    // Create an Elasticsearch client object.\n    $builder = ClientBuilder::create();\n    $builder-&gt;setHosts($hosts);\n    $client = $builder-&gt;build();\n\n    $index = 'my_index';\n    $type = 'People';\n\n    // Index a few document.\n    $params = [\n        'index' =&gt; $index,\n        'type' =&gt; $type,\n    ];\n\n    $names = ['Ada Lovelace', 'Alonzo Church', 'Barbara Liskov'];\n\n    foreach ($names as $name) {\n        $params['body']['name'] = $name;\n        $client-&gt;index($params);\n    }\n\n    // Force just-added items to be indexed.\n    $client-&gt;indices()-&gt;refresh(array('index' =&gt; $index));\n\n\n    // Search for documents.\n    $result = $client-&gt;search([\n        'index' =&gt; $index,\n        'type' =&gt; $type,\n        'body' =&gt; [\n            'query' =&gt; [\n                'match' =&gt; [\n                    'name' =&gt; 'Barbara Liskov',\n                ],\n            ],\n        ],\n    ]);\n\n    if (isset($result['hits']['hits'])) {\n        print &lt;&lt;&lt;TABLE\n&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;&lt;th&gt;ID&lt;/th&gt;&lt;th&gt;Name&lt;/th&gt;&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\nTABLE;\n        foreach ($result['hits']['hits'] as $record) {\n            printf(\"&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;\\n\", $record['_id'], $record['_source']['name']);\n        }\n        print \"&lt;/tbody&gt;\\n&lt;/table&gt;\\n\";\n    }\n\n    // Delete documents.\n    $params = [\n        'index' =&gt; $index,\n        'type' =&gt; $type,\n    ];\n\n    $ids = array_map(function($row) {\n        return $row['_id'];\n    }, $result['hits']['hits']);\n\n    foreach ($ids as $id) {\n        $params['id'] = $id;\n        $client-&gt;delete($params);\n    }\n\n} catch (Exception $e) {\n    print $e-&gt;getMessage();\n}\n&lt;?php\n\ndeclare(strict_types=1);\n\nuse Platformsh\\ConfigReader\\Config;\n\n// Create a new config object to ease reading the Platform.sh environment variables.\n// You can alternatively use getenv() yourself.\n$config = new Config();\n\n// Get the credentials to connect to the Memcached service.\n$credentials = $config-&gt;credentials('memcached');\n\ntry {\n    // Connecting to Memcached server.\n    $memcached = new Memcached();\n    $memcached-&gt;addServer($credentials['host'], $credentials['port']);\n    $memcached-&gt;setOption(Memcached::OPT_BINARY_PROTOCOL, true);\n\n    $key = \"Deploy day\";\n    $value = \"Friday\";\n\n    // Set a value.\n    $memcached-&gt;set($key, $value);\n\n    // Read it back.\n    $test = $memcached-&gt;get($key);\n\n    printf('Found value &lt;strong&gt;%s&lt;/strong&gt; for key &lt;strong&gt;%s&lt;/strong&gt;.', $test, $key);\n\n} catch (Exception $e) {\n    print $e-&gt;getMessage();\n}\n&lt;?php\n\ndeclare(strict_types=1);\n\nuse Platformsh\\ConfigReader\\Config;\nuse MongoDB\\Client;\n\n// Create a new config object to ease reading the Platform.sh environment variables.\n// You can alternatively use getenv() yourself.\n$config = new Config();\n\n// The 'database' relationship is generally the name of primary database of an application.\n// It could be anything, though, as in the case here here where it's called \"mongodb\".\n$credentials = $config-&gt;credentials('mongodb');\n\ntry {\n\n    $server = sprintf('%s://%s:%s@%s:%d/%s',\n        $credentials['scheme'],\n        $credentials['username'],\n        $credentials['password'],\n        $credentials['host'],\n        $credentials['port'],\n        $credentials['path']\n    );\n\n    $client = new Client($server);\n    $collection = $client-&gt;main-&gt;starwars;\n\n    $result = $collection-&gt;insertOne([\n        'name' =&gt; 'Rey',\n        'occupation' =&gt; 'Jedi',\n    ]);\n\n    $id = $result-&gt;getInsertedId();\n\n    $document = $collection-&gt;findOne([\n        '_id' =&gt; $id,\n    ]);\n\n    // Clean up after ourselves.\n    $collection-&gt;drop();\n\n    printf(\"Found %s (%s)&lt;br /&gt;\\n\", $document-&gt;name, $document-&gt;occupation);\n\n} catch (\\Exception $e) {\n    print $e-&gt;getMessage();\n}\n&lt;?php\n\ndeclare(strict_types=1);\n\nuse Platformsh\\ConfigReader\\Config;\n\n// Create a new config object to ease reading the Platform.sh environment variables.\n// You can alternatively use getenv() yourself.\n$config = new Config();\n\n// The 'database' relationship is generally the name of primary SQL database of an application.\n// That's not required, but much of our default automation code assumes it.\n$credentials = $config-&gt;credentials('database');\n\ntry {\n    // Connect to the database using PDO.  If using some other abstraction layer you would\n    // inject the values from $database into whatever your abstraction layer asks for.\n    $dsn = sprintf('mysql:host=%s;port=%d;dbname=%s', $credentials['host'], $credentials['port'], $credentials['path']);\n    $conn = new \\PDO($dsn, $credentials['username'], $credentials['password'], [\n        // Always use Exception error mode with PDO, as it's more reliable.\n        \\PDO::ATTR_ERRMODE =&gt; \\PDO::ERRMODE_EXCEPTION,\n        // So we don't have to mess around with cursors and unbuffered queries by default.\n        \\PDO::MYSQL_ATTR_USE_BUFFERED_QUERY =&gt; TRUE,\n        // Make sure MySQL returns all matched rows on update queries including\n        // rows that actually didn't have to be updated because the values didn't\n        // change. This matches common behavior among other database systems.\n        \\PDO::MYSQL_ATTR_FOUND_ROWS =&gt; TRUE,\n    ]);\n\n    // Creating a table.\n    $sql = \"CREATE TABLE People (\n      id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n      name VARCHAR(30) NOT NULL,\n      city VARCHAR(30) NOT NULL\n      )\";\n    $conn-&gt;query($sql);\n\n    // Insert data.\n    $sql = \"INSERT INTO People (name, city) VALUES \n        ('Neil Armstrong', 'Moon'), \n        ('Buzz Aldrin', 'Glen Ridge'), \n        ('Sally Ride', 'La Jolla');\";\n    $conn-&gt;query($sql);\n\n    // Show table.\n    $sql = \"SELECT * FROM People\";\n    $result = $conn-&gt;query($sql);\n    $result-&gt;setFetchMode(\\PDO::FETCH_OBJ);\n\n    if ($result) {\n        print &lt;&lt;&lt;TABLE\n&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;City&lt;/th&gt;&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\nTABLE;\n        foreach ($result as $record) {\n            printf(\"&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;\\n\", $record-&gt;name, $record-&gt;city);\n        }\n        print \"&lt;/tbody&gt;\\n&lt;/table&gt;\\n\";\n    }\n\n    // Drop table\n    $sql = \"DROP TABLE People\";\n    $conn-&gt;query($sql);\n\n} catch (\\Exception $e) {\n    print $e-&gt;getMessage();\n}\n&lt;?php\n\ndeclare(strict_types=1);\n\nuse Platformsh\\ConfigReader\\Config;\n\n// Create a new config object to ease reading the Platform.sh environment variables.\n// You can alternatively use getenv() yourself.\n$config = new Config();\n\n// The 'database' relationship is generally the name of primary SQL database of an application.\n// It could be anything, though, as in the case here here where it's called \"postgresql\".\n$credentials = $config-&gt;credentials('postgresql');\n\ntry {\n    // Connect to the database using PDO.  If using some other abstraction layer you would\n    // inject the values from $database into whatever your abstraction layer asks for.\n    $dsn = sprintf('pgsql:host=%s;port=%d;dbname=%s', $credentials['host'], $credentials['port'], $credentials['path']);\n    $conn = new \\PDO($dsn, $credentials['username'], $credentials['password'], [\n        // Always use Exception error mode with PDO, as it's more reliable.\n        \\PDO::ATTR_ERRMODE =&gt; \\PDO::ERRMODE_EXCEPTION,\n        // So we don't have to mess around with cursors and unbuffered queries by default.\n    ]);\n\n    $conn-&gt;query(\"DROP TABLE IF EXISTS People\");\n\n    // Creating a table.\n    $sql = \"CREATE TABLE IF NOT EXISTS People (\n      id SERIAL PRIMARY KEY,\n      name VARCHAR(30) NOT NULL,\n      city VARCHAR(30) NOT NULL\n      )\";\n    $conn-&gt;query($sql);\n\n    // Insert data.\n    $sql = \"INSERT INTO People (name, city) VALUES\n        ('Neil Armstrong', 'Moon'),\n        ('Buzz Aldrin', 'Glen Ridge'),\n        ('Sally Ride', 'La Jolla');\";\n    $conn-&gt;query($sql);\n\n    // Show table.\n    $sql = \"SELECT * FROM People\";\n    $result = $conn-&gt;query($sql);\n    $result-&gt;setFetchMode(\\PDO::FETCH_OBJ);\n\n    if ($result) {\n        print &lt;&lt;&lt;TABLE\n&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;City&lt;/th&gt;&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\nTABLE;\n        foreach ($result as $record) {\n            printf(\"&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;\\n\", $record-&gt;name, $record-&gt;city);\n        }\n        print \"&lt;/tbody&gt;\\n&lt;/table&gt;\\n\";\n    }\n\n    // Drop table.\n    $sql = \"DROP TABLE People\";\n    $conn-&gt;query($sql);\n\n} catch (\\Exception $e) {\n    print $e-&gt;getMessage();\n}\n&lt;?php\n\ndeclare(strict_types=1);\n\nuse Platformsh\\ConfigReader\\Config;\nuse PhpAmqpLib\\Connection\\AMQPStreamConnection;\nuse PhpAmqpLib\\Message\\AMQPMessage;\n\n// Create a new config object to ease reading the Platform.sh environment variables.\n// You can alternatively use getenv() yourself.\n$config = new Config();\n\n// Get the credentials to connect to the RabbitMQ service.\n$credentials = $config-&gt;credentials('rabbitmq');\n\ntry {\n\n    $queueName = 'deploy_days';\n\n    // Connect to the RabbitMQ server.\n    $connection = new AMQPStreamConnection($credentials['host'], $credentials['port'], $credentials['username'], $credentials['password']);\n    $channel = $connection-&gt;channel();\n\n    $channel-&gt;queue_declare($queueName, false, false, false, false);\n\n    $msg = new AMQPMessage('Friday');\n    $channel-&gt;basic_publish($msg, '', 'hello');\n\n    echo \"[x] Sent 'Friday'&lt;br/&gt;\\n\";\n\n    // In a real application you't put the following in a separate script in a loop.\n    $callback = function ($msg) {\n        printf(\"[x] Deploying on %s&lt;br /&gt;\\n\", $msg-&gt;body);\n    };\n\n    $channel-&gt;basic_consume($queueName, '', false, true, false, false, $callback);\n\n    // This blocks on waiting for an item from the queue, so comment it out in this demo script.\n    //$channel-&gt;wait();\n\n    $channel-&gt;close();\n    $connection-&gt;close();\n\n} catch (Exception $e) {\n    print $e-&gt;getMessage();\n}\n&lt;?php\n\ndeclare(strict_types=1);\n\nuse Platformsh\\ConfigReader\\Config;\n\n// Create a new config object to ease reading the Platform.sh environment variables.\n// You can alternatively use getenv() yourself.\n$config = new Config();\n\n// Get the credentials to connect to the Redis service.\n$credentials = $config-&gt;credentials('redis');\n\ntry {\n    // Connecting to Redis server.\n    $redis = new Redis();\n    $redis-&gt;connect($credentials['host'], $credentials['port']);\n\n    $key = \"Deploy day\";\n    $value = \"Friday\";\n\n    // Set a value.\n    $redis-&gt;set($key, $value);\n\n    // Read it back.\n    $test = $redis-&gt;get($key);\n\n    printf('Found value &lt;strong&gt;%s&lt;/strong&gt; for key &lt;strong&gt;%s&lt;/strong&gt;.', $test, $key);\n\n} catch (Exception $e) {\n    print $e-&gt;getMessage();\n}\n&lt;?php\ndeclare(strict_types=1);\n\nuse Platformsh\\ConfigReader\\Config;\nuse Solarium\\Client;\n\n// Create a new config object to ease reading the Platform.sh environment variables.\n// You can alternatively use getenv() yourself.\n$config = new Config();\n\n// Get the credentials to connect to the Solr service.\n$credentials = $config-&gt;credentials('solr');\n\ntry {\n\n    $config = [\n        'endpoint' =&gt; [\n            'localhost' =&gt; [\n                'host' =&gt; $credentials['host'],\n                'port' =&gt; $credentials['port'],\n                'path' =&gt; \"/\" . $credentials['path'],\n            ]\n        ]\n    ];\n\n    $client = new Client($config);\n\n    // Add a document\n    $update = $client-&gt;createUpdate();\n\n    $doc1 = $update-&gt;createDocument();\n    $doc1-&gt;id = 123;\n    $doc1-&gt;name = 'Valentina Tereshkova';\n\n    $update-&gt;addDocuments(array($doc1));\n    $update-&gt;addCommit();\n\n    $result = $client-&gt;update($update);\n    print \"Adding one document. Status (0 is success): \" .$result-&gt;getStatus(). \"&lt;br /&gt;\\n\";\n\n    // Select one document\n    $query = $client-&gt;createQuery($client::QUERY_SELECT);\n    $resultset = $client-&gt;execute($query);\n    print  \"Selecting documents (1 expected): \" .$resultset-&gt;getNumFound() . \"&lt;br /&gt;\\n\";\n\n    // Delete one document\n    $update = $client-&gt;createUpdate();\n\n    $update-&gt;addDeleteById(123);\n    $update-&gt;addCommit();\n    $result = $client-&gt;update($update);\n    print \"Deleting one document. Status (0 is success): \" .$result-&gt;getStatus(). \"&lt;br /&gt;\\n\";\n\n} catch (Exception $e) {\n    print $e-&gt;getMessage();\n}\n\nRuntime configuration\nIt is possible to change the PHP-FPM runtime configuration via the runtime block on your .platform.app.yaml. The PHP-FPM options below are configurable:\n\nrequest_terminate_timeout - The timeout for serving a single request after which the PHP-FPM worker process will be killed.  That is separate from the PHP runtime's max_execution_time ini option, which is preferred.  This option may be used if the PHP process is dying without cleaning up properly and causing the FPM process to hang.\n\nruntime:\n    request_terminate_timeout: 300\n\nProject templates\nA number of project templates for major PHP applications are available on GitHub. Not all of them are proactively maintained but all can be used as a starting point or reference for building your own website or web application.\nApplications\n\nAkeneo\nEZ Platform\nDrupal 7\nDrupal 7 (Commerce Kickstart)\nDrupal 8\nDrupal 8 (Multisite variant)\nGovCMS8\nLaravel\nMoodle\nMagento 1\nMagento 2\nOpigno\nPimcore\nSculpin\nTYPO3\nWordpress\n\nExamples\n\nGeneric PHP application\n\nFrameworks\n\nAmPHP\nReact PHP\nSymfony 3.x\nSymfony 4.x\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Java", "title": "Frameworks", "url": "/languages/java/frameworks.html", "documentId": "f99801c2233859a71bd443b0d79d34ed2ecf9509", "text": "\n                        \n                            \n                                \n                                \n                                Java Featured Frameworks\nHibernate\nHibernate ORM is an object-relational mapping tool for the Java programming language. It provides a framework for mapping an object-oriented domain model to a relational database. Hibernate handles object-relational impedance mismatch problems by replacing direct, persistent database accesses with high-level object handling functions.\n\nHibernate Best Practices\n\nJakarta EE/ Eclipse MicroProfile\nEclipse MicroProfile is a semi-new community dedicated to optimizing the Enterprise Java mission for microservice-based architectures. Now Enterprise Java has been standardized under the Eclipse Foundation as Jakarta EE.\n\nJakarta EE/ Eclipse MicroProfile Best Practices\n\nTemplates\n\nApache Tomee\nThorntail\nPayara Micro\nKumuluzEE\nHelidon\nOpen Liberty\n\nSpring\nThe Spring Framework provides a comprehensive programming and configuration model for modern Java-based enterprise applications - on any kind of deployment platform. Platform.sh is flexible, and allows you to use Spring Framework in several flavors such as Spring MVC and Spring Boot.\n\nSpring Best Practices\n\nTemplates\n\nSpring Boot MySQL\nSpring Boot MongoDB\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Java", "title": "Tuning", "url": "/languages/java/tuning.html", "documentId": "952990a320d5766d3d24e9808c92910d4ab39473", "text": "\n                        \n                            \n                                \n                                \n                                Performance tuning Java\nThere are a number of settings that can be adjusted for each application to optimize its performance on Platform.sh.\nMemory limits\nThe JVM generally requires specifying a maximum memory size it is allowed to use, using the Xmx parameter.  That should be set based on the available memory on the application container, which will vary with its size.\nTo extract the container-scaled value on the command line, use $(jq .info.limits.memory /run/config.json).\nYou should also set the ExitOnOutOfMemoryError.  When you enable this option, the JVM exits on the first occurrence of an out-of-memory error.  Platform.sh will restart the application automatically.\nThese are the recommended parameters for running a Java application. Thus, the command to use to start a Java application is:\njava -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError //The rest of the arguments and the jar file.\nGarbage collection\nWhen migrating the application to a cloud environment, it is often essential to analyze the Garbage Collector's log and behavior. For this, there are two options:\n\nPlacing the log into the Platform.sh /var/log/app.log file (which captures STDOUT).\nCreating a log file specifically for the GC.\n\nTo use the STDOUT log, you can add the parameter -XX: + PrintGCDetails, E.g.:\njava -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails //The rest of the arguments and the jar file.\nJava supports a number of different garbage collection strategies.  Which one is optimal for your application will vary depending on your available memory, Java version, and application profile.  Determining which is best for your application is out of scope, but the main options and how to enable them are:\n\n\n\nName\nCommand  Flag\nDescription\n\n\n\n\nSerial Garbage Collector\n-XX:+UseSerialGC\nThis is the simplest GC implementation, as it basically works with a single thread.\n\n\nParallel Garbage Collector\n-XX:+UseParallelGC\nUnlike Serial Garbage Collector, this uses multiple threads for managing heap space. But it also freezes other application threads while performing GC.\n\n\nCMS Garbage Collector\n-XX:+USeParNewGC\nThe Concurrent Mark Sweep (CMS) implementation uses multiple garbage collector threads for garbage collection. It's for applications that prefer shorter garbage collection pauses, and that can afford to share processor resources with the garbage collector while the application is running.\n\n\nG1 Garbage Collector\n-XX:+UseG1GC\nGarbage First, G1, is for applications running on multiprocessor machines with large memory space.\n\n\n\nThe default strategy on Java 9 and later is G1.  The GC strategy to use can be set in the start line with:\nSerial\njava -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails -XX:+UseSerialGC //The rest of the arguments and the jar file.\nParallel Garbage Collector\njava -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails -XX:+UseParallelGC //The rest of the arguments and the jar file.\nCMS Garbage Collector\njava -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails -XX:+USeParNewGC //The rest of the arguments and the jar file.\nG1\njava -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails -XX:+UseG1GC //The rest of the arguments and the jar file.\nJava 8 Optimization\nIdeally, all applications should run the latest LTS release of the JVM at least.  That is currently Java 11.  Java 11 has a number of performance improvements, particularly on container-based environments such as Platform.sh.\nHowever, in many cases, this is not possible.  If you are still running on Java 8 there are two additional considerations.\nThe default garbage collector for Java 8 is Parallel GC.  In most cases G1 will offer better performance.  We recommend enabling it, as above.\nFurthermore, there is the UseStringDeduplication flag which works to eliminate duplicate Strings within the GC process.  That flag can save between 13% to 30% of memory, depending on application. However, this can impact on the pause time of your app.\njava -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails\nReferences\n\nJava Memory Commands\nHow to Migrate my Java application to Platform.sh\nGarbage Collector Log\nIntroduction to Garbage Collection Tuning\n\n\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Languages", "title": "Java", "url": "/languages/java.html", "documentId": "7b778bf575740053d30896f4047ad72cfc5536df", "text": "\n                        \n                            \n                                \n                                \n                                Java\nJava is a general-purpose programming language, and one of the most popular in the world today. Platform.sh supports Java runtimes that can be used with build management tools such as Gradle, Maven, and Ant.\nSupported versions\nOpenJDK versions:\n\n8\n11\n12\n13\n\nTo specify a Java container, use the type property in your .platform.app.yaml.\ntype: 'java:13'\nSupport libraries\nWhile it is possible to read the environment directly from your application, it is generally easier and more robust to use the platformsh/config-reader which handles decoding of service credential information for you.\nSupport build automation\nPlatform.sh supports the most common project management tools in the Java ecosystem, including:\n\nGradle\nMaven\nAnt\n\nAccessing services\nTo access various services with Java, see the following examples.  The individual service pages have more information on configuring each service.\nElasticsearchKafkaMemcachedMongoDBMySQLPostgreSQLRabbitMQRedisSolrpackage sh.platform.languages.sample;\n\nimport org.elasticsearch.action.admin.indices.refresh.RefreshRequest;\nimport org.elasticsearch.action.admin.indices.refresh.RefreshResponse;\nimport org.elasticsearch.action.delete.DeleteRequest;\nimport org.elasticsearch.action.index.IndexRequest;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.search.SearchHit;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\nimport sh.platform.config.Config;\nimport sh.platform.config.Elasticsearch;\n\nimport java.io.IOException;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.function.Supplier;\n\nimport static java.util.concurrent.ThreadLocalRandom.current;\n\npublic class ElasticsearchSample implements Supplier&lt;String&gt; {\n\n    @Override\n    public String get() {\n        StringBuilder logger = new StringBuilder();\n\n        // Create a new config object to ease reading the Platform.sh environment variables.\n        // You can alternatively use getenv() yourself.\n        Config config = new Config();\n\n        Elasticsearch elasticsearch = config.getCredential(\"elasticsearch\", Elasticsearch::new);\n\n        // Create an Elasticsearch client object.\n        RestHighLevelClient client = elasticsearch.get();\n\n        try {\n\n            String index = \"animals\";\n            String type = \"mammals\";\n            // Index a few document.\n            final List&lt;String&gt; animals = Arrays.asList(\"dog\", \"cat\", \"monkey\", \"horse\");\n            for (String animal : animals) {\n                Map&lt;String, Object&gt; jsonMap = new HashMap&lt;&gt;();\n                jsonMap.put(\"name\", animal);\n                jsonMap.put(\"age\", current().nextInt(1, 10));\n                jsonMap.put(\"is_cute\", current().nextBoolean());\n\n                IndexRequest indexRequest = new IndexRequest(index, type)\n                        .id(animal).source(jsonMap);\n                client.index(indexRequest, RequestOptions.DEFAULT);\n            }\n\n            RefreshRequest refresh = new RefreshRequest(index);\n\n            // Force just-added items to be indexed\n            RefreshResponse refreshResponse = client.indices().refresh(refresh, RequestOptions.DEFAULT);\n\n            // Search for documents.\n            SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n            sourceBuilder.query(QueryBuilders.termQuery(\"name\", \"dog\"));\n            SearchRequest searchRequest = new SearchRequest();\n            searchRequest.indices(index);\n            searchRequest.source(sourceBuilder);\n\n            SearchResponse search = client.search(searchRequest, RequestOptions.DEFAULT);\n\n            for (SearchHit hit : search.getHits()) {\n                String id = hit.getId();\n                final Map&lt;String, Object&gt; source = hit.getSourceAsMap();\n                logger.append(String.format(\"result id %s source: %s\", id, source)).append('\\n');\n            }\n\n            // Delete documents.\n            for (String animal : animals) {\n                client.delete(new DeleteRequest(index, type, animal), RequestOptions.DEFAULT);\n            }\n        } catch (IOException exp) {\n            throw new RuntimeException(\"An error when execute Elasticsearch: \" + exp.getMessage());\n        }\n        return logger.toString();\n    }\n}package sh.platform.languages.sample;\n\nimport org.apache.kafka.clients.consumer.Consumer;\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.producer.Producer;\nimport org.apache.kafka.clients.producer.ProducerConfig;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.clients.producer.RecordMetadata;\nimport sh.platform.config.Config;\nimport sh.platform.config.Kafka;\n\nimport java.time.Duration;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.function.Supplier;\n\npublic class KafkaSample implements Supplier&lt;String&gt; {\n\n    @Override\n    public String get() {\n        StringBuilder logger = new StringBuilder();\n\n        // Create a new config object to ease reading the Platform.sh environment variables.\n        // You can alternatively use getenv() yourself.\n        Config config = new Config();\n\n        try {\n            // Get the credentials to connect to the Kafka service.\n            final Kafka kafka = config.getCredential(\"kafka\", Kafka::new);\n            Map&lt;String, Object&gt; configProducer = new HashMap&lt;&gt;();\n            configProducer.putIfAbsent(ProducerConfig.CLIENT_ID_CONFIG, \"animals\");\n            final Producer&lt;Long, String&gt; producer = kafka.getProducer(configProducer);\n\n            // Sending data into the stream.\n            RecordMetadata metadata = producer.send(new ProducerRecord&lt;&gt;(\"animals\", \"lion\")).get();\n            logger.append(\"Record sent with to partition \").append(metadata.partition())\n                    .append(\" with offset \").append(metadata.offset()).append('\\n');\n\n            metadata = producer.send(new ProducerRecord&lt;&gt;(\"animals\", \"dog\")).get();\n            logger.append(\"Record sent with to partition \").append(metadata.partition())\n                    .append(\" with offset \").append(metadata.offset()).append('\\n');\n\n            metadata = producer.send(new ProducerRecord&lt;&gt;(\"animals\", \"cat\")).get();\n            logger.append(\"Record sent with to partition \").append(metadata.partition())\n                    .append(\" with offset \").append(metadata.offset()).append('\\n');\n\n            // Consumer, read data from the stream.\n            final HashMap&lt;String, Object&gt; configConsumer = new HashMap&lt;&gt;();\n            configConsumer.put(ConsumerConfig.GROUP_ID_CONFIG, \"consumerGroup1\");\n            configConsumer.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n\n            Consumer&lt;Long, String&gt; consumer = kafka.getConsumer(configConsumer, \"animals\");\n            ConsumerRecords&lt;Long, String&gt; consumerRecords = consumer.poll(Duration.ofSeconds(3));\n\n            // Print each record.\n            consumerRecords.forEach(record -&gt; {\n                logger.append(\"Record: Key \" + record.key());\n                logger.append(\" value \" + record.value());\n                logger.append(\" partition \" + record.partition());\n                logger.append(\" offset \" + record.offset()).append('\\n');\n            });\n\n            // Commits the offset of record to broker.\n            consumer.commitSync();\n\n            return logger.toString();\n        } catch (Exception exp) {\n            throw new RuntimeException(\"An error when execute Kafka\", exp);\n        }\n    }\n}package sh.platform.languages.sample;\n\nimport net.spy.memcached.MemcachedClient;\nimport sh.platform.config.Config;\n\nimport java.util.function.Supplier;\n\nimport sh.platform.config.Memcached;\n\npublic class MemcachedSample implements Supplier&lt;String&gt; {\n\n    @Override\n    public String get() {\n        StringBuilder logger = new StringBuilder();\n\n        // Create a new config object to ease reading the Platform.sh environment variables.\n        // You can alternatively use getenv() yourself.\n        Config config = new Config();\n\n        // Get the credentials to connect to the Memcached service.\n        Memcached memcached = config.getCredential(\"memcached\", Memcached::new);\n\n        final MemcachedClient client = memcached.get();\n\n        String key = \"cloud\";\n        String value = \"platformsh\";\n\n        // Set a value.\n        client.set(key, 0, value);\n\n        // Read it back.\n        Object test = client.get(key);\n\n        logger.append(String.format(\"Found value %s for key %s.\", test, key));\n\n        return logger.toString();\n    }\n}package sh.platform.languages.sample;\n\nimport com.mongodb.MongoClient;\nimport com.mongodb.client.MongoCollection;\nimport com.mongodb.client.MongoDatabase;\nimport org.bson.Document;\nimport sh.platform.config.Config;\nimport sh.platform.config.MongoDB;\n\nimport java.util.function.Supplier;\n\nimport static com.mongodb.client.model.Filters.eq;\n\npublic class MongoDBSample implements Supplier&lt;String&gt; {\n\n    @Override\n    public String get() {\n        StringBuilder logger = new StringBuilder();\n\n        // Create a new config object to ease reading the Platform.sh environment variables.\n        // You can alternatively use getenv() yourself.\n        Config config = new Config();\n\n        // The 'database' relationship is generally the name of primary database of an application.\n        // It could be anything, though, as in the case here here where it's called \"mongodb\".\n        MongoDB database = config.getCredential(\"mongodb\", MongoDB::new);\n        MongoClient mongoClient = database.get();\n        final MongoDatabase mongoDatabase = mongoClient.getDatabase(database.getDatabase());\n        MongoCollection&lt;Document&gt; collection = mongoDatabase.getCollection(\"scientist\");\n        Document doc = new Document(\"name\", \"Ada Lovelace\")\n                .append(\"city\", \"London\");\n\n        collection.insertOne(doc);\n        Document myDoc = collection.find(eq(\"_id\", doc.get(\"_id\"))).first();\n        logger.append(myDoc.toJson()).append('\\n');\n        logger.append(collection.deleteOne(eq(\"_id\", doc.get(\"_id\"))));\n        return logger.toString();\n    }\n}package sh.platform.languages.sample;\n\nimport sh.platform.config.Config;\nimport sh.platform.config.MySQL;\n\nimport javax.sql.DataSource;\nimport java.sql.Connection;\nimport java.sql.ResultSet;\nimport java.sql.SQLException;\nimport java.sql.Statement;\nimport java.util.function.Supplier;\n\npublic class MySQLSample implements Supplier&lt;String&gt; {\n\n    @Override\n    public String get() {\n        StringBuilder logger = new StringBuilder();\n\n        // Create a new config object to ease reading the Platform.sh environment variables.\n        // You can alternatively use getenv() yourself.\n        Config config = new Config();\n\n        // The 'database' relationship is generally the name of primary SQL database of an application.\n        // That's not required, but much of our default automation code assumes it.\n        MySQL database = config.getCredential(\"database\", MySQL::new);\n        DataSource dataSource = database.get();\n\n        // Connect to the database\n        try (Connection connection = dataSource.getConnection()) {\n\n            // Creating a table.\n            String sql = \"CREATE TABLE JAVA_PEOPLE (\" +\n                    \" id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY,\" +\n                    \"name VARCHAR(30) NOT NULL,\" +\n                    \"city VARCHAR(30) NOT NULL)\";\n\n            final Statement statement = connection.createStatement();\n            statement.execute(sql);\n\n            // Insert data.\n            sql = \"INSERT INTO JAVA_PEOPLE (name, city) VALUES\" +\n                    \"('Neil Armstrong', 'Moon'),\" +\n                    \"('Buzz Aldrin', 'Glen Ridge'),\" +\n                    \"('Sally Ride', 'La Jolla')\";\n\n            statement.execute(sql);\n\n            // Show table.\n            sql = \"SELECT * FROM JAVA_PEOPLE\";\n            final ResultSet resultSet = statement.executeQuery(sql);\n            while (resultSet.next()) {\n                int id = resultSet.getInt(\"id\");\n                String name = resultSet.getString(\"name\");\n                String city = resultSet.getString(\"city\");\n                logger.append(String.format(\"the JAVA_PEOPLE id %d the name %s and city %s\", id, name, city));\n                logger.append('\\n');\n            }\n            statement.execute(\"DROP TABLE JAVA_PEOPLE\");\n            return logger.toString();\n        } catch (SQLException exp) {\n            throw new RuntimeException(\"An error when execute MySQL\", exp);\n        }\n    }\n}package sh.platform.languages.sample;\n\nimport sh.platform.config.Config;\nimport sh.platform.config.MySQL;\nimport sh.platform.config.PostgreSQL;\n\nimport javax.sql.DataSource;\nimport java.sql.Connection;\nimport java.sql.ResultSet;\nimport java.sql.SQLException;\nimport java.sql.Statement;\nimport java.util.function.Supplier;\n\npublic class PostgreSQLSample implements Supplier&lt;String&gt; {\n\n    @Override\n    public String get() {\n        StringBuilder logger = new StringBuilder();\n\n        // Create a new config object to ease reading the Platform.sh environment variables.\n        // You can alternatively use getenv() yourself.\n        Config config = new Config();\n\n        // The 'database' relationship is generally the name of primary SQL database of an application.\n        // It could be anything, though, as in the case here here where it's called \"postgresql\".\n        PostgreSQL database = config.getCredential(\"postgresql\", PostgreSQL::new);\n        DataSource dataSource = database.get();\n\n        // Connect to the database\n        try (Connection connection = dataSource.getConnection()) {\n\n            // Creating a table.\n            String sql = \"CREATE TABLE JAVA_FRAMEWORKS (\" +\n                    \" id SERIAL PRIMARY KEY,\" +\n                    \"name VARCHAR(30) NOT NULL)\";\n\n            final Statement statement = connection.createStatement();\n            statement.execute(sql);\n\n            // Insert data.\n            sql = \"INSERT INTO JAVA_FRAMEWORKS (name) VALUES\" +\n                    \"('Spring'),\" +\n                    \"('Jakarta EE'),\" +\n                    \"('Eclipse JNoSQL')\";\n\n            statement.execute(sql);\n\n            // Show table.\n            sql = \"SELECT * FROM JAVA_FRAMEWORKS\";\n            final ResultSet resultSet = statement.executeQuery(sql);\n            while (resultSet.next()) {\n                int id = resultSet.getInt(\"id\");\n                String name = resultSet.getString(\"name\");\n                logger.append(String.format(\"the JAVA_FRAMEWORKS id %d the name %s \", id, name));\n                logger.append('\\n');\n            }\n            statement.execute(\"DROP TABLE JAVA_FRAMEWORKS\");\n            return logger.toString();\n        } catch (SQLException exp) {\n            throw new RuntimeException(\"An error when execute PostgreSQL\", exp);\n        }\n    }\n}package sh.platform.languages.sample;\n\nimport sh.platform.config.Config;\nimport sh.platform.config.RabbitMQ;\n\nimport javax.jms.Connection;\nimport javax.jms.ConnectionFactory;\nimport javax.jms.MessageConsumer;\nimport javax.jms.MessageProducer;\nimport javax.jms.Queue;\nimport javax.jms.Session;\nimport javax.jms.TextMessage;\nimport java.util.function.Supplier;\n\npublic class RabbitMQSample implements Supplier&lt;String&gt; {\n\n    @Override\n    public String get() {\n        StringBuilder logger = new StringBuilder();\n\n        // Create a new config object to ease reading the Platform.sh environment variables.\n        // You can alternatively use getenv() yourself.\n        Config config = new Config();\n        try {\n            // Get the credentials to connect to the RabbitMQ service.\n            final RabbitMQ credential = config.getCredential(\"rabbitmq\", RabbitMQ::new);\n            final ConnectionFactory connectionFactory = credential.get();\n\n            // Connect to the RabbitMQ server.\n            final Connection connection = connectionFactory.createConnection();\n            connection.start();\n            final Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);\n            Queue queue = session.createQueue(\"cloud\");\n            MessageConsumer consumer = session.createConsumer(queue);\n\n            // Sending a message into the queue.\n            TextMessage textMessage = session.createTextMessage(\"Platform.sh\");\n            textMessage.setJMSReplyTo(queue);\n            MessageProducer producer = session.createProducer(queue);\n            producer.send(textMessage);\n\n            // Receive the message.\n            TextMessage replyMsg = (TextMessage) consumer.receive(100);\n\n            logger.append(\"Message: \").append(replyMsg.getText());\n\n            // close connections.\n            producer.close();\n            consumer.close();\n            session.close();\n            connection.close();\n            return logger.toString();\n        } catch (Exception exp) {\n            throw new RuntimeException(\"An error when execute RabbitMQ\", exp);\n        }\n    }\n}\npackage sh.platform.languages.sample;\n\nimport redis.clients.jedis.Jedis;\nimport redis.clients.jedis.JedisPool;\nimport sh.platform.config.Config;\nimport sh.platform.config.Redis;\n\nimport java.util.Set;\nimport java.util.function.Supplier;\n\npublic class RedisSample implements Supplier&lt;String&gt; {\n\n    @Override\n    public String get() {\n        StringBuilder logger = new StringBuilder();\n\n        // Create a new config object to ease reading the Platform.sh environment variables.\n        // You can alternatively use getenv() yourself.\n        Config config = new Config();\n\n        // The 'database' relationship is generally the name of primary database of an application.\n        // It could be anything, though, as in the case here here where it's called \"redis\".\n        Redis database = config.getCredential(\"redis\", Redis::new);\n        JedisPool dataSource = database.get();\n\n        // Get a Redis Client\n        final Jedis jedis = dataSource.getResource();\n\n        // Set a values\n        jedis.sadd(\"cities\", \"Salvador\");\n        jedis.sadd(\"cities\", \"London\");\n        jedis.sadd(\"cities\", \"S\u00e3o Paulo\");\n\n        // Read it back.\n        Set&lt;String&gt; cities = jedis.smembers(\"cities\");\n        logger.append(\"cities: \" + cities);\n        jedis.del(\"cities\");\n        return logger.toString();\n    }\n}\npackage sh.platform.languages.sample;\n\nimport org.apache.solr.client.solrj.SolrQuery;\nimport org.apache.solr.client.solrj.SolrServerException;\nimport org.apache.solr.client.solrj.impl.HttpSolrClient;\nimport org.apache.solr.client.solrj.impl.XMLResponseParser;\nimport org.apache.solr.client.solrj.response.QueryResponse;\nimport org.apache.solr.client.solrj.response.UpdateResponse;\nimport org.apache.solr.common.SolrDocumentList;\nimport org.apache.solr.common.SolrInputDocument;\nimport sh.platform.config.Config;\nimport sh.platform.config.Solr;\n\nimport java.io.IOException;\nimport java.util.function.Supplier;\n\npublic class SolrSample implements Supplier&lt;String&gt; {\n\n    @Override\n    public String get() {\n\n        StringBuilder logger = new StringBuilder();\n\n        // Create a new config object to ease reading the Platform.sh environment variables.\n        // You can alternatively use getenv() yourself.\n        Config config = new Config();\n\n        Solr solr = config.getCredential(\"solr\", Solr::new);\n\n        try {\n\n            final HttpSolrClient solrClient = solr.get();\n            solrClient.setParser(new XMLResponseParser());\n\n            // Add a document\n            SolrInputDocument document = new SolrInputDocument();\n            final String id = \"123456\";\n            document.addField(\"id\", id);\n            document.addField(\"name\", \"Ada Lovelace\");\n            document.addField(\"city\", \"London\");\n            solrClient.add(document);\n            final UpdateResponse response = solrClient.commit();\n            logger.append(\"Adding one document. Status (0 is success): \")\n                    .append(response.getStatus()).append('\\n');\n\n            SolrQuery query = new SolrQuery();\n            query.set(\"q\", \"city:London\");\n            QueryResponse queryResponse = solrClient.query(query);\n\n            SolrDocumentList results = queryResponse.getResults();\n            logger.append(String.format(\"Selecting documents (1 expected):  %d \\n\", results.getNumFound()));\n\n            // Delete one document\n            solrClient.deleteById(id);\n\n            logger.append(String.format(\"Deleting one document. Status (0 is success):  %s \\n\",\n                    solrClient.commit().getStatus()));\n        } catch (SolrServerException | IOException exp) {\n            throw new RuntimeException(\"An error when execute Solr \", exp);\n        }\n\n        return logger.toString();\n    }\n}\nProject templates\nA number of project templates for major Java applications are available on GitHub. Not all of them are proactively maintained but all can be used as a starting point or reference for building your own website or web application.\nApplications\nSpring\n\nSpring Boot MySQL\nSpring Boot MongoDB\nSpring Kotlin\nSpring Boot Gradle\n\nJakarta EE/ Eclipse MicroProfile\n\nApache Tomee\nThorntail\nPayara Micro\nKumuluzEE\nHelidon\nOpen Liberty\n\nOther Frameworks\n\nMicronaut\nJettry\nJenkins\nXwiki\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Languages", "title": "Go", "url": "/languages/go.html", "documentId": "971e2691300bfdf6437211cd5fac72af9b025bbf", "text": "\n                        \n                            \n                                \n                                \n                                Go\nPlatform.sh supports building and deploying applications written in Go using Go modules.  They are compiled during the Build hook phase, and support both committed dependencies and download-on-demand.\nSupported versions\n\n1.11\n1.12\n1.13\n1.14\n\nTo specify a Go container, use the type property in your .platform.app.yaml.\ntype: 'golang:1.14'\nDeprecated versions\nThe following container versions are also available.  However, due to their lack of Go module support and the difficulties in supporting the GOPATH during the Platform.sh build they are not recommended.\n\n1.8\n1.9\n1.10\n\nGo modules\nThe recommended way to handle Go dependencies on Platform.sh is using Go module support in Go 1.11 and later.  That allows the build process to use go build directly without any extra steps, and you can specify an output executable file of your choice.  (See the examples below.)\nPlatform.sh variables\nPlatform.sh exposes relationships and other configuration as environment variables.  To make them easier to access you should use the provided GoHelper library.  Most notably, it allows a program to determine at runtime what HTTP port it should listen on and what the credentials are to access other services.\npackage main\n\nimport (\n    _ \"github.com/go-sql-driver/mysql\"\n    psh \"github.com/platformsh/gohelper\"\n    \"net/http\"\n)\n\nfunc main() {\n\n    p, err := psh.NewPlatformInfo()\n\n    if err != nil {\n        panic(\"Not in a Platform.sh Environment.\")\n    }\n\n    http.HandleFunc(\"/bar\", func(w http.ResponseWriter, r *http.Request) {\n        // ...\n    })\n\n    http.ListenAndServe(\":\"+p.Port, nil)\n}\n\nBuilding and running the application\nAssuming your go.mod and go.sum files are present in your repository, the application may be built with a simple go build command that will produce a working executable.  You can then start it from the web.commands.start directive.  Note that the start command must run in the foreground. Should the program terminate for any reason it will be automatically restarted.\nThe following basic .platform.app.yaml file is sufficient to run most Go applications.\nname: app\n\ntype: golang:1.14\n\nhooks:\n    build: |\n        # Modify this line if you want to build differently or use an alternate name for your executable.\n        go build -o bin/app\n\nweb:\n    upstream:\n        socket_family: tcp\n        protocol: http\n\n    commands:\n        # If you change the build output in the build hook above, update this line as well.\n        start: ./bin/app\n\n    locations:\n        /:\n            # Route all requests to the Go app, unconditionally.\n            # If you want some files served directly by the web server without hitting Go, see\n            # https://docs.platform.sh/configuration/app/web.html\n            allow: false\n            passthru: true\n\ndisk: 1024\n\nNote that there will still be an Nginx proxy server sitting in front of your application.  If desired, certain paths may be served directly by Nginx without hitting your application (for static files, primarily) or you may route all requests to the Go application unconditionally, as in the example above.\nAccessing services\nTo access various services with Go, see the following examples. The individual service pages have more information on configuring each service.\nMemcachedMongoDBMySQLPostgreSQLRabbitMQSolrpackage examples\n\nimport (\n\t\"fmt\"\n\t\"github.com/bradfitz/gomemcache/memcache\"\n\tpsh \"github.com/platformsh/config-reader-go/v2\"\n\tgomemcache \"github.com/platformsh/config-reader-go/v2/gomemcache\"\n)\n\nfunc UsageExampleMemcached() string {\n\n\t// Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables.\n\t// You can alternatively use os.Getenv() yourself.\n\tconfig, err := psh.NewRuntimeConfig()\n\tcheckErr(err)\n\n\t// Get the credentials to connect to the Solr service.\n\tcredentials, err := config.Credentials(\"memcached\")\n\tcheckErr(err)\n\n\t// Retrieve formatted credentials for gomemcache.\n\tformatted, err := gomemcache.FormattedCredentials(credentials)\n\tcheckErr(err)\n\n\t// Connect to Memcached.\n\tmc := memcache.New(formatted)\n\n\t// Set a value.\n\tkey := \"Deploy_day\"\n\tvalue := \"Friday\"\n\n\terr = mc.Set(&amp;memcache.Item{Key: key, Value: []byte(value)})\n\n\t// Read it back.\n\ttest, err := mc.Get(key)\n\n\treturn fmt.Sprintf(\"Found value &lt;strong&gt;%s&lt;/strong&gt; for key &lt;strong&gt;%s&lt;/strong&gt;.\", test.Value, key)\n}\npackage examples\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\tpsh \"github.com/platformsh/config-reader-go/v2\"\n\tmongoPsh \"github.com/platformsh/config-reader-go/v2/mongo\"\n\t\"go.mongodb.org/mongo-driver/bson\"\n\t\"go.mongodb.org/mongo-driver/mongo\"\n\t\"go.mongodb.org/mongo-driver/mongo/options\"\n\t\"time\"\n)\n\nfunc UsageExampleMongoDB() string {\n\n\t// Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables.\n\t// You can alternatively use os.Getenv() yourself.\n\tconfig, err := psh.NewRuntimeConfig()\n\tcheckErr(err)\n\n\t// Get the credentials to connect to the Solr service.\n\tcredentials, err := config.Credentials(\"mongodb\")\n\tcheckErr(err)\n\n\t// Retrieve the formatted credentials for mongo-driver.\n\tformatted, err := mongoPsh.FormattedCredentials(credentials)\n\tcheckErr(err)\n\n\t// Connect to MongoDB using the formatted credentials.\n\tctx, _ := context.WithTimeout(context.Background(), 10*time.Second)\n\tclient, err := mongo.Connect(ctx, options.Client().ApplyURI(formatted))\n\tcheckErr(err)\n\n\t// Create a new collection.\n\tcollection := client.Database(\"main\").Collection(\"starwars\")\n\n\t// Clean up after ourselves.\n\terr = collection.Drop(context.Background())\n\tcheckErr(err)\n\n\t// Create an entry.\n\tres, err := collection.InsertOne(ctx, bson.M{\"name\": \"Rey\", \"occupation\": \"Jedi\"})\n\tcheckErr(err)\n\n\tid := res.InsertedID\n\n\t// Read it back.\n\tcursor, err := collection.Find(context.Background(), bson.M{\"_id\": id})\n\tcheckErr(err)\n\n\tvar name string\n\tvar occupation string\n\n\tfor cursor.Next(context.Background()) {\n\t\tdocument := struct {\n\t\t\tName       string\n\t\t\tOccupation string\n\t\t}{}\n\t\terr := cursor.Decode(&amp;document)\n\t\tcheckErr(err)\n\n\t\tname = document.Name\n\t\toccupation = document.Occupation\n\t}\n\n\treturn fmt.Sprintf(\"Found %s (%s)\", name, occupation)\n}\npackage examples\n\nimport (\n\t\"database/sql\"\n\t\"fmt\"\n\t_ \"github.com/go-sql-driver/mysql\"\n\tpsh \"github.com/platformsh/config-reader-go/v2\"\n\tsqldsn \"github.com/platformsh/config-reader-go/v2/sqldsn\"\n)\n\nfunc UsageExampleMySQL() string {\n\n\t// Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables.\n\t// You can alternatively use os.Getenv() yourself.\n\tconfig, err := psh.NewRuntimeConfig()\n\tcheckErr(err)\n\n\t// The 'database' relationship is generally the name of the primary SQL database of an application.\n\t// That's not required, but much of our default automation code assumes it.\n\tcredentials, err := config.Credentials(\"database\")\n\tcheckErr(err)\n\n\t// Using the sqldsn formatted credentials package.\n\tformatted, err := sqldsn.FormattedCredentials(credentials)\n\tcheckErr(err)\n\n\tdb, err := sql.Open(\"mysql\", formatted)\n\tcheckErr(err)\n\n\tdefer db.Close()\n\n\t// Force MySQL into modern mode.\n\tdb.Exec(\"SET NAMES=utf8\")\n\tdb.Exec(\\x60SET sql_mode = 'ANSI,STRICT_TRANS_TABLES,STRICT_ALL_TABLES,\n    NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,\n    NO_AUTO_CREATE_USER,ONLY_FULL_GROUP_BY'\\x60)\n\n\t// Creating a table.\n\tsqlCreate := \\x60\nCREATE TABLE IF NOT EXISTS PeopleGo (\nid SERIAL PRIMARY KEY,\nname VARCHAR(30) NOT NULL,\ncity VARCHAR(30) NOT NULL)\\x60\n\n\t_, err = db.Exec(sqlCreate)\n\tcheckErr(err)\n\n\t// Insert data.\n\tsqlInsert := \\x60\nINSERT INTO PeopleGo (name, city) VALUES\n('Neil Armstrong', 'Moon'),\n('Buzz Aldrin', 'Glen Ridge'),\n('Sally Ride', 'La Jolla');\\x60\n\n\t_, err = db.Exec(sqlInsert)\n\tcheckErr(err)\n\n\ttable := \\x60&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;City&lt;/th&gt;&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\\x60\n\n\tvar id int\n\tvar name string\n\tvar city string\n\n\trows, err := db.Query(\"SELECT * FROM PeopleGo\")\n\tif err != nil {\n\t\tpanic(err)\n\t} else {\n\t\tfor rows.Next() {\n\t\t\terr = rows.Scan(&amp;id, &amp;name, &amp;city)\n\t\t\tcheckErr(err)\n\t\t\ttable += fmt.Sprintf(\"&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td&gt;%s&lt;/td&gt;&lt;tr&gt;\\n\", name, city)\n\t\t}\n\t\ttable += \"&lt;/tbody&gt;\\n&lt;/table&gt;\\n\"\n\t}\n\n\t_, err = db.Exec(\"DROP TABLE PeopleGo;\")\n\tcheckErr(err)\n\n\treturn table\n}\npackage examples\n\nimport (\n\t\"database/sql\"\n\t\"fmt\"\n\t_ \"github.com/lib/pq\"\n\tpsh \"github.com/platformsh/config-reader-go/v2\"\n\tlibpq \"github.com/platformsh/config-reader-go/v2/libpq\"\n)\n\nfunc UsageExamplePostgreSQL() string {\n\n\t// Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables.\n\t// You can alternatively use os.Getenv() yourself.\n\tconfig, err := psh.NewRuntimeConfig()\n\tcheckErr(err)\n\n\t// The 'database' relationship is generally the name of the primary SQL database of an application.\n\t// It could be anything, though, as in the case here where it's called \"postgresql\".\n\tcredentials, err := config.Credentials(\"postgresql\")\n\tcheckErr(err)\n\n\t// Retrieve the formatted credentials.\n\tformatted, err := libpq.FormattedCredentials(credentials)\n\tcheckErr(err)\n\n\t// Connect.\n\tdb, err := sql.Open(\"postgres\", formatted)\n\tcheckErr(err)\n\n\tdefer db.Close()\n\n\t// Creating a table.\n\tsqlCreate := \\x60\nCREATE TABLE IF NOT EXISTS PeopleGo (\nid SERIAL PRIMARY KEY,\nname VARCHAR(30) NOT NULL,\ncity VARCHAR(30) NOT NULL);\\x60\n\n\t_, err = db.Exec(sqlCreate)\n\tcheckErr(err)\n\n\t// Insert data.\n\tsqlInsert := \\x60\nINSERT INTO PeopleGo(name, city) VALUES\n('Neil Armstrong', 'Moon'),\n('Buzz Aldrin', 'Glen Ridge'),\n('Sally Ride', 'La Jolla');\\x60\n\n\t_, err = db.Exec(sqlInsert)\n\tcheckErr(err)\n\n\ttable := \\x60&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;City&lt;/th&gt;&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\\x60\n\n\tvar id int\n\tvar name string\n\tvar city string\n\n\t// Read it back.\n\trows, err := db.Query(\"SELECT * FROM PeopleGo\")\n\tif err != nil {\n\t\tpanic(err)\n\t} else {\n\t\tfor rows.Next() {\n\t\t\terr = rows.Scan(&amp;id, &amp;name, &amp;city)\n\t\t\tcheckErr(err)\n\t\t\ttable += fmt.Sprintf(\"&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td&gt;%s&lt;/td&gt;&lt;tr&gt;\\n\", name, city)\n\t\t}\n\t\ttable += \"&lt;/tbody&gt;\\n&lt;/table&gt;\\n\"\n\t}\n\n\t_, err = db.Exec(\"DROP TABLE PeopleGo;\")\n\tcheckErr(err)\n\n\treturn table\n}\npackage examples\n\nimport (\n\t\"fmt\"\n\tpsh \"github.com/platformsh/config-reader-go/v2\"\n\tamqpPsh \"github.com/platformsh/config-reader-go/v2/amqp\"\n\t\"github.com/streadway/amqp\"\n\t\"sync\"\n)\n\nfunc UsageExampleRabbitMQ() string {\n\n\t// Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables.\n\t// You can alternatively use os.Getenv() yourself.\n\tconfig, err := psh.NewRuntimeConfig()\n\tcheckErr(err)\n\n\t// Get the credentials to connect to RabbitMQ.\n\tcredentials, err := config.Credentials(\"rabbitmq\")\n\tcheckErr(err)\n\n\t// Use the amqp formatted credentials package.\n\tformatted, err := amqpPsh.FormattedCredentials(credentials)\n\tcheckErr(err)\n\n\t// Connect to the RabbitMQ server.\n\tconnection, err := amqp.Dial(formatted)\n\tcheckErr(err)\n\tdefer connection.Close()\n\n\t// Make a channel.\n\tchannel, err := connection.Channel()\n\tcheckErr(err)\n\tdefer channel.Close()\n\n\t// Create a queue.\n\tq, err := channel.QueueDeclare(\n\t\t\"deploy_days\", // name\n\t\tfalse,         // durable\n\t\tfalse,         // delete when unused\n\t\tfalse,         // exclusive\n\t\tfalse,         // no-wait\n\t\tnil,           // arguments\n\t)\n\n\tbody := \"Friday\"\n\tmsg := fmt.Sprintf(\"Deploying on %s\", body)\n\n\t// Publish a message.\n\terr = channel.Publish(\n\t\t\"\",     // exchange\n\t\tq.Name, // routing key\n\t\tfalse,  // mandatory\n\t\tfalse,  // immediate\n\t\tamqp.Publishing{\n\t\t\tContentType: \"text/plain\",\n\t\t\tBody:        []byte(msg),\n\t\t})\n\tcheckErr(err)\n\n\toutputMSG := fmt.Sprintf(\"[x] Sent '%s' &lt;br&gt;\", body)\n\n\t// Consume the message.\n\tmsgs, err := channel.Consume(\n\t\tq.Name, // queue\n\t\t\"\",     // consumer\n\t\ttrue,   // auto-ack\n\t\tfalse,  // exclusive\n\t\tfalse,  // no-local\n\t\tfalse,  // no-wait\n\t\tnil,    // args\n\t)\n\tcheckErr(err)\n\n\tvar received string\n\tvar wg sync.WaitGroup\n\twg.Add(1)\n\tgo func() {\n\t\tfor d := range msgs {\n\t\t\treceived = fmt.Sprintf(\"[x] Received message: '%s' &lt;br&gt;\", d.Body)\n\t\t\twg.Done()\n\t\t}\n\t}()\n\n\twg.Wait()\n\n\toutputMSG += received\n\n\treturn outputMSG\n}\npackage examples\n\nimport (\n\t\"fmt\"\n\tpsh \"github.com/platformsh/config-reader-go/v2\"\n\tgosolr \"github.com/platformsh/config-reader-go/v2/gosolr\"\n\tsolr \"github.com/rtt/Go-Solr\"\n)\n\nfunc UsageExampleSolr() string {\n\n\t// Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables.\n\t// You can alternatively use os.Getenv() yourself.\n\tconfig, err := psh.NewRuntimeConfig()\n\tcheckErr(err)\n\n\t// Get the credentials to connect to the Solr service.\n\tcredentials, err := config.Credentials(\"solr\")\n\tcheckErr(err)\n\n\t// Retrieve Solr formatted credentials.\n\tformatted, err := gosolr.FormattedCredentials(credentials)\n\tcheckErr(err)\n\n\t// Connect to Solr using the formatted credentials.\n\tconnection := &amp;solr.Connection{URL: formatted}\n\n\t// Add a document and commit the operation.\n\tdocAdd := map[string]interface{}{\n\t\t\"add\": []interface{}{\n\t\t\tmap[string]interface{}{\"id\": 123, \"name\": \"Valentina Tereshkova\"},\n\t\t},\n\t}\n\n\trespAdd, err := connection.Update(docAdd, true)\n\tcheckErr(err)\n\n\t// Select the document.\n\tq := &amp;solr.Query{\n\t\tParams: solr.URLParamMap{\n\t\t\t\"q\": []string{\"id:123\"},\n\t\t},\n\t}\n\n\tresSelect, err := connection.CustomSelect(q, \"query\")\n\tcheckErr(err)\n\n\t// Delete the document and commit the operation.\n\tdocDelete := map[string]interface{}{\n\t\t\"delete\": map[string]interface{}{\n\t\t\t\"id\": 123,\n\t\t},\n\t}\n\n\tresDel, err := connection.Update(docDelete, true)\n\tcheckErr(err)\n\n\tmessage := fmt.Sprintf(\\x60Adding one document - %s&lt;br&gt;\nSelecting document (1 expected): %d&lt;br&gt;\nDeleting document - %s&lt;br&gt;\n  \\x60, respAdd, resSelect.Results.NumFound, resDel)\n\n\treturn message\n}\n\nProject templates\nPlatform.sh offers a project templates for Go applications using the structure described above.  It can be used as a starting point or reference for building your own website or web application.\n\nGeneric Go application\nBeego\nEcho\nGin\nHugo\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Languages", "title": "C#/.NET", "url": "/languages/dotnet.html", "documentId": "878c9dbe42adc9cf4268e58af5d06736aae274b2", "text": "\n                        \n                            \n                                \n                                \n                                .NET Core\nPlatform.sh supports deploying .NET applications by allowing developers to define a build process and pass its variables to the .NET Core build environment.\nSupported versions\n\n2.0\n2.1\n2.2\n3.1\n\nTo specify a .NET Core container, use the type property in your .platform.app.yaml.\ntype: 'dotnet:3.1'\nBuilding the application\nFor simple applications, using the dotnet publish default framework-dependent deployment method is sufficient for building applications in .NET containers:\nhooks:\n    build: |\n        set -xe\n        dotnet publish --output \"$PLATFORM_OUTPUT_DIR\" -p:UseRazorBuildServer=false -p:UseSharedCompilation=false\n\nwhere PLATFORM_OUTPUT_DIR is the output directory for compiled languages available at build time.\nTypically .NET Core builds will start a collection of build servers, which are helpful for repeated builds. On Platform.sh, however, if this process is not disabled, the build process will not finish until the idle timeout is reached.\nAs a result, it is recommended to include -p toggles that disable the Razor compiler for dynamic cshtml pages (UseRazorBuildServer) and the .NET msbuild compiler (UseSharedCompilation).\nIf making multiple builds is desired for your application, make sure to call dotnet build-server shutdown at the end of your build hook.\nRunning the application\n.NET Core applications should be started using the web.commands.start directive in .platform.app.yaml. This ensures that the command starts at the right moment and stops gracefully when a re-deployment needs to be executed. Also, should the program terminate for any reason, it will be automatically restarted. Note that the start command must run in the foreground.\nIncoming requests are passed to the application using either a TCP (default) or UNIX socket. The application must use the appropriate environment variable to determine the URI to listen on. In case of a TCP socket (recommended), the application must listen on http://127.0.0.1, using the PORT environment variable.\nThere will be an Nginx server sitting in front of your application. Serving static content via Nginx is recommended, as this allows easy control of headers (including cache headers) and also has marginal performance benefits.\nNote that HTTPS is also terminated at the Ngnix proxy, so the app.UseHttpsRedirection(); line in Startup.cs should be removed. To force HTTPS-only, please refer to the routes documentation.\nThe following example configures an environment to serve the static content folders commonly found in ASP.NET MVC templates using Nginx, while routing other traffic to the .NET application.\nweb:\n  locations:\n    \"/\":\n      root: \"wwwroot\"\n      allow: true\n      passthru: true\n      rules:\n        # Serve these common asset types with customs cache headers.\n        \\.(jpe?g|png|gif|svgz?|css|js|map|ico|bmp|eot|woff2?|otf|ttf)$:\n          allow: true\n          expires: 300s\n\n  commands:\n    start: \"dotnet WebApplication1.dll\"\n\nYou can also route all requests to the application unconditionally:\nweb:\n  locations:\n    \"/\":\n      allow: false\n      passthru: true\n\n  commands:\n    start: \"dotnet WebApplication1.dll\"\n\nProject templates\nPlatform.sh offers project templates for .NET Core applications using the structure described above.  They can be used as a starting point or reference for building your own website or web application.\nASP.NET Core\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Best practices", "title": "One site or many", "url": "/bestpractices/oneormany.html", "documentId": "99e5b365739b9149278ce4d3c767b01cb938655c", "text": "\n                        \n                            \n                                \n                                \n                                One or multiple projects\nPlatform.sh supports running multiple \"application containers\" in a single project.  That can be extremely powerful in some cases, but if misused can lead to unnecessary maintenance difficulty and excessive costs.\nThe way to determine what setup is appropriate for your use case is to think of your project as a collection of services, some of which you've written yourself.  That is, put \"your code\" and \"the database\" on the same level.  (That is essentially true from the Platform.sh perspective.)  Does your project consist of multiple \"your code\" pieces, but they all are parts of the same project?  Or are they discrete applications that conceptually exist independently of each other?\nDiscrete projects\nIf your applications are discrete systems that are only incidentally related (such as because you wrote both of them), make them separate projects.  That will provide the most flexible development workflow.  It will also be cheaper, as running multiple applications in a single project requires at least a Medium plan, which costs more than two Standard plans.\nDiscrete projects are appropriate if:\n\nYou want to deploy new releases of each application independently of the others.\nThe projects are for different customers/clients.\nThe projects do not need deep internal knowledge of each other's data.\nDifferent teams will be working on different applications.\nYou want to develop true-microservices, where each microservice is fully stand-alone process with its own data.\n\nIf you are uncertain how your needs map to projects, it probably means they should be separate, discrete projects.\nClustered applications\nA clustered application is one where your project requires multiple \"app services\", written by you, but are all part of the same conceptual project.  That is, removing one of the app services would render the others broken.\nIn a clustered application, you either have multiple .platform.app.yaml files in different directories with separate code bases that deploy separately or you have a single application that spawns one or more worker instances that run background processes.  (See the link for details on how to set those up.)\nA Clustered application requires at least a Medium plan.\nWith a clustered application, you often will not need multiple service instances.  The MySQL, MariaDB, and Solr services support defining multiple databases on a single service, which is significantly more efficient than defining multiple services.  Redis, Memcached, Elasticsearch, and RabbitMQ natively support multiple bins, or queues, or indexes (the terminology varies) defined by the client application as part of the request so they need no additional configuration on Platform.sh, although they may need application configuration.\nClustered applications are appropriate if:\n\nYou want one user-facing application and an entirely separate admin-facing application that are both operating on the same data.\nYou want to have a user-facing application and a separate worker process (either the same code or separate) that handles background tasks.\nYou want a single conceptual application written in multiple programming languages, such as a PHP frontend with Node.js background worker.\n\nMulti-site applications\nSome Content Management Systems or other applications support running multiple logical \"sites\" off of a single code base.  Those will usually work on Platform.sh depending on the configuration details of the application but are generally not recommended.  Often their multi-site logic is dependant on the domain name of the incoming request, which on Platform.sh will vary by branch.  They also often recommend running multiple databases, which while supported just fine on Platform.sh makes the setup process for each site more difficult.\nLeveraging multi-site capabilities of an application are appropriate if, and only if:\n\nThere is only a single team working on all of the \"sites\" involved.\nAll \"sites\" should be updated simultaneously as a single unit.\nEach individual site is relatively low traffic, such that the aggregate traffic is appropriate for your plan size.\nAll sites really do use the same codebase with no variation, just different data.\n\nIf any of those is not the case, discrete projects will be a better long term plan.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Best practices", "title": "HTTP Caching", "url": "/bestpractices/http-caching.html", "documentId": "f8f4eb39bca1b50eea5b1d4221dabab0ced0e8a7", "text": "\n                        \n                            \n                                \n                                \n                                HTTP Caching\nThere are several different \"levels\" at which you could configure HTTP caching for your site on Platform.sh.  Which one you want to use depends on your specific use case.  You should use only one of these at a time and disable any others.  Mixing them together will most likely result in stale and unclearable caches.\n\nThe Platform.sh Router cache.  Every project includes a router instance, which includes optional HTTP caching.  It is reasonably configurable and obeys HTTP cache directives, but does not support push-based clearing.  If you are uncertain what caching tool to use, start with this one.  It is more than sufficient for the majority of use cases.\n\nA Content Delivery Network (CDN).  Platform.sh is compatible with most commercial CDNs.  If your Platform.sh Enterprise project has a Dedicated production environment it will typically come with the Fastly CDN.  A CDN will generally offer the best performance as it is the only option that includes multiple geographic locations, but it also tends to be the most expensive.  Functionality will vary widely depending on the CDN.  Setup instructions for Fastly and Cloudflare are available, and will be similar for most other CDNs.\n\nVarnish.  Platform.sh offers a Varnish service that you can declare as part of your application and insert between the router and your application.  Performance will be roughly comparable to the Router cache.  Varnish is more configurable than the Router cache as you are able to customize your VCL file, but make sure you are comfortable with Varnish configuration.  Platform.sh does not provide assistance with VCL configuration and a misconfiguration may cause difficult to debug behavior.  Generally speaking, you should use Varnish only if your application requires push-based clearing or relies on Varnish-specific business logic.\n\nApplication-specific caching.  Many web applications and frameworks include a built-in web cache layer that mimics what Varnish or the Router cache would do.  Most of the time they will be slower than a dedicated caching service as they still require invoking the application server, and only serve as a fallback for users that do not have a dedicated caching service available.  Generally speaking the only reason to use an application-specific web cache is if it includes some application-specific business logic that you depend on, such as application-sensitive selective cache clearing or partial page caching.\n\n\nNote that this refers only to HTTP level caching.  Many applications have an internal application cache for data objects or similar.  That should remain active regardless of the HTTP cache in use.\nCookies and caching\nHTTP-based caching systems generally default to including cookie values in cache keys so as to avoid serving authenticated content to the wrong user.  While a safe default, it also has the side effect that any cookie will effectively disable the cache, including mundane cookies like analytics.  \nThe solution is to whitelist the cookies that should impact the cache and include only the application session cookie(s).  For the Router cache see our documentation.  For other cache systems consult their documentation.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "App", "title": "Upgrading", "url": "/configuration/app/upgrading.html", "documentId": "6aeae85b6cf3b440b77a94b11ae250a411ce85f1", "text": "\n                        \n                            \n                                \n                                \n                                Upgrading\nChanges in version 2019.05\n\nThe !archive tag in YAML has been un-deprecated, and is now favored over the !include option. !include is still available for other include types (yaml, binary, and string).\n\nChanges in version 2017.11 (2017-11-09)\n\nThe !archive tag in YAML files is now deprecated in favor of the more generic !include.  For example, the following services.yaml snippet:\n\nmysearch:\n    type: solr:6.3\n    disk: 1024\n    configuration:\n        core_config: !archive \"myconfdir\"\n\nCan now be written as:\nmysearch:\n    type: solr:6.3\n    disk: 1024\n    configuration:\n        core_config: !include\n            type: archive\n            path: \"myconfdir\"\n\n\nThe syntax for the mounts key in .platform.app.yaml has changed.  Rather than a parsed string, the value of each mount is a multi-key definition.  That is, the following example:\n\nmounts:\n    \"tmp\": \"shared:files/tmp\"\n    \"logs\": \"shared:files/logs\"\n\nCan now be written as:\nmounts:\n    tmp:\n        source: local\n        source_path: tmp\n    logs:\n        source: local\n        source_path: logs\n\nChanges in version 2016.6 (2016-11-18)\n\nApplication containers now include the latest LTS version of Node.js, 6.9.1. The previously included version was 4.6.1.\nComposer was briefly called with --no-dev, but as of 2016-11-21 this change has been reverted, because of the unintended effect it had on projects using the Symfony framework.\n\nChanges in version 2016.5\nAs of October 2016, the default behaviour of the expires key, which controls\nclient-side caching of static files, has changed. Previously, if the key was\nunset, the Expires and Cache-Control HTTP headers were left unset in the\nresponse, which meant that client side caching behaviour was left undefined.\nTo ensure consistent behaviour that doesn't depend on which browser the client\nis using, the new default behaviour is to set these headers to values that\ndisable client-side caching. This change only affects static files served\ndirectly by the web server. Responses served from passthru URLs continue to use\nwhatever caching headers were set by the application..\nTo enable caching on your static files, make sure you include an expires key\nin your web configuration,\nas shown below:\n    web:\n        locations:\n            \"/\":\n                root: \"public\"\n                passthru: \"/index.php\"\n                index:\n                    - index.php\n                expires: 300\n                scripts: true\n                allow: true\n                rules:\n                    \\.mp4$:\n                        allow: false\n                        expires: -1\n            \"/sites/default/files\":\n                expires: 300\n                passthru: true\n                allow: true\nChanges in version 2016.4\nAs of July 2016, we no longer create default configuration files if one is not provided.  The defaults we used to provide were tailored specifically for Drupal 7, which is now a legacy-support version with the release of Drupal 8 and not especially useful for non-Drupal or non-PHP sites.  They also defaulted to software versions that are no longer current and recommended.  Instead, you must provide your own .platform.app.yaml, .platform/routes.yaml, and .platform/services.yaml files.\nAdditionally, a version for a language or service should always be specified as well. That allows you to control when you upgrade from one version to another without relying on a network default.\nThe previous default files, for reference, are:\n.platform.app.yaml\nname: php\ntype: \"php:5.4\"\nbuild:\n    flavor: \"drupal\"\naccess:\n    ssh: contributor\nrelationships:\n    database: \"mysql:mysql\"\n    solr: \"solr:solr\"\n    redis: \"redis:redis\"\nweb:\n    document_root: \"/\"\n    passthru: \"/index.php\"\ndisk: 2048\nmounts:\n    \"public/sites/default/files\": \"shared:files/files\"\n    \"tmp\": \"shared:files/tmp\"\n    \"private\": \"shared:files/private\"\ncrons:\n    drupal:\n        spec: \"*/20 * * * *\"\n        cmd: \"cd public ; drush core-cron\"\n\n.platform/routes.yaml\n \"http://{default}/\":\n     type: upstream\n     upstream: \"php:http\"\n     cache:\n         enabled: true\n     ssi:\n         enabled: false\n\n \"http://www.{default}/\":\n     type: redirect\n     to: \"http://{default}/\"\n\n.platform/services.yaml\n mysql:\n     type: mysql:5.5\n     disk: 2048\n\n redis:\n     type: redis:2.8\n\n solr:\n     type: solr:3.6\n     disk: 1024\n\nChanges in version 2016.3\nAs we are aiming to always provide you more control and flexibility on how to deploy your applications, the .platform.app.yaml format has been greatly improved. It is now way more flexible, and also much more explicit to describe what you want to do.\nThe web key is now a set of locations where you can define very precisely the behavior of each URL prefix.\nNote, we no longer move your application from \"/\" to \"public/\" automatically if the new format is adopted.\nIf you are using Drupal, move all of your Drupal files into \"public/\" in the Git repository.\nOld format:\nweb:\n    document_root: \"/\"\n    passthru: \"/index.php\"\n    index_files:\n        - \"index.php\"\n    expires: 300\n    whitelist:\n        - \\.html$\nNew format:\nweb:\n    locations:\n        \"/\":\n            root: \"public\"\n            passthru: \"/index.php\"\n            index:\n                - index.php\n            expires: 300\n            scripts: true\n            allow: true\n            rules:\n                \\.mp4$:\n                    allow: false\n                    expires: -1\n        \"/sites/default/files\":\n            expires: 300\n            passthru: true\n            allow: true\nBackward compatibility\nOf course, we alway keep backward compatibility with the previous configuration format. Here is what happens if you don't upgrade your configuration:\n# The following parameters are automatically moved as a \"/\" block in the\n# \"locations\" object, and are invalid if there is a valid \"locations\" block.\ndocument_root: \"/public\"      # Converted to [locations][/][root]\npassthru: \"/index.php\"        # Converted to [locations][/][passthru]\nindex_files:\n    - index.php               # Converted to [locations][/][index]\nwhitelist: [ ]                # Converted to [locations][/][rules]\nblacklist: [ ]                # Converted to [locations][/][rules]\nexpires: 3d                   # Converted to [locations][/][expires]\nChanges in version 2015.7\nThe .platform.app.yaml configuration file now allows for a much clearer syntax, which you can (and should) start using now.\nThe old format had a single string to identify the 'toolstack' you use:\ntoolstack: \"php:drupal\"\n\nThe new syntax allows to separate the concerns of what language you are running\nand the build process that is going to happen on deployment:\ntype: php\nbuild:\n    flavor: drupal\n\nCurrently we only support php in the 'type' key. Current supported build\nflavors are drupal, composer and symfony.\nChanges in version 2014.9\nThis version introduces changes in\nthe configuration files format. Most of the old configuration format is\nstill supported, but customers are invited to move to the new format.\nFor an example upgrade path, see the Drupal 7.x branch of the\nPlatformsh-examples\nrepository\non GitHub.\nConfiguration items for PHP that previously was part of\n.platform/services.yaml are now moved into .platform.app.yaml, which\ngains the following top-level items:\n\nname: should be \"php\"\nrelationships, access and disk: should be the same as the\nrelationships key of PHP in .platform/services.yaml\n\nNote that there is now a sane default for access (SSH access to PHP is\ngranted to all users that have role \"collaborator\" and above on the\nenvironment) so most customers can now just omit this key in\n.platform.app.yaml.\nIn addition, version 1.7.0 now has consistency checks for configuration\nfiles and will reject git push operations that contain configuration\nfiles that are invalid. In this case, just fix the issues as they are\nreported, commit and push again.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "App", "title": "Source Operations", "url": "/configuration/app/source-operations.html", "documentId": "e2c07a5c4777b7c2133b05e019eb2cf26a1d3363", "text": "\n                        \n                            \n                                \n                                \n                                [Beta] Source operations\n\nnote\nSource Operations are currently in Beta. While the syntax is not expected to change, some behavior might in the future.\nAlso, Source Operations are only available to Enterprise and Elite customers. Contact our sales team for more information.\n\nAn application can define a number of operations that apply to its source code and that can be automated.\nA basic, common source operation could be to automatically update Composer dependencies like this:\nsource:\n    operations:\n        update:\n            command: |\n                set -e\n                composer update\n                git add composer.lock\n                git commit -m \"Update Composer dependencies.\"\n\nThe update key is the name of the operation. It is arbitrary, and multiple source operations can be defined.\n(You may wish to include more robust error handling than this example.)\nThe environment resource gets a new source-operation action which can be triggered by the CLI:\nplatform source-operation:run update\n\nThe source-operation:run command takes the command name to run. Additional variables can be added to inject into the environment of the source operation.  They will be interpreted the same way as any other variable set through the UI or CLI, which means you need an env: prefix to expose them as a Unix environment variable.  They can then be referenced by the source operation like any other variable.\nplatform source-operation:run update --variable env:FOO=bar --variable env:BAZ=beep\n\nWhen this operation is triggered:\n\nA clean Git checkout of the current environment HEAD commit is created; this checkout doesn't have any remotes, has all the tags defined in the project, but only has the current environment branch.\nSequentially, for each application that has defined this operation, the operation command is launched in the container image of the application.  The environment will have all of the variables available during the build phase, optionally overridden by the variables specified in the operation payload.\nAt the end of the process, if any commits were created, the new commits are pushed to the repository and the normal build process of the environment is triggered.\n\nNote that these operations run in an isolated container: it is not part of the runtime cluster of the environment, and doesn't require the environment to be running.  Also be aware that if multiple applications in a single project both result in a new commit, that will appear as two distinct commits in the Git history but only a single new build/deploy cycle will occur.\nSource Operations with an external Git integration\nGit integration can be configured to send commits made to the Platform.sh Git remote, to the upstream repository instead. This means that if a source operation did generate a new commit, the commit will be pushed to the upstream repository.\n\nnote\nCurrently, this configuration requires the enable_codesource_integration_push setting to be turned on by a Platform.sh staff and is only available to selected Beta customers.\n\nSource Operations can only be triggered on environment created by a branch, and not to environment created by a Pull Request on the external upstream (GitHub, Bitbucket, Gitlab).\nAutomated Source Operations using cron\nYou can use cron to automatically run your source operations.\n\nnote\nAutomated source operations using cron requires to get an API token and install the CLI in your application container.\n\nOnce the CLI is installed in your application container and an API token has been configured, you can add a cron task to run your source operations once a day. We do not recommend triggering source operations on your master production environment, but rather on a dedicated environment which you can use for testing before deployment.\nThe example below synchronizes the update-dependencies environment with its parent before running the update source operation:\ncrons:\n    update:\n        # Run the 'update' source operation every day at midnight.\n        spec: '0 0 * * *'\n        cmd: |\n            set -e\n            if [ \"$PLATFORM_BRANCH\" = update-dependencies ]; then\n                platform environment:sync code data --no-wait --yes\n                platform source-operation:run update --no-wait --yes\n            fi\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "App", "title": "Multiple Applications", "url": "/configuration/app/multi-app.html", "documentId": "c7a6d715a6bde342490ffe4b139623ee9086ba4d", "text": "\n                        \n                            \n                                \n                                \n                                Multiple Applications\nPlatform.sh supports building multiple applications per project (for example RESTful web services with a front-end, or a main website and a blog).  For resource allocation reasons, however, that is not supported on Standard plan.\n\nNote\nThis page only applies to Grid projects.  Contact your sales representative for advanced Dedicated environment configurations.\n\nProject structure\nThere are multiple ways to structure such a project, depending on the way your source code is organized and what your goal is.  All of these approaches may be used within a single project simultaneously, although it is often easier to maintain if you settle on just one approach for a given project.\n\nDiscrete code bases\nIf your project consists of a discrete code base for each application, the most straightforward approach is to put both code bases into a single project repository in separate directories.  Each will have its own .platform.app.yaml file, which will define how that particular application gets built, using the code in that directory.\nFor example, if you have a Drupal back end with an AngularJS front end you could organize the repository like this:\n.git/\n.platform/\ndrupal/\n  .platform.app.yaml\n  ...\nangular/\n  .platform.app.yaml\n\nEach .platform.app.yaml file will define a single application container, and build code in that directory.  The .platform directory is outside of all of them and still defines additional services you require, as well as routes.\nNote that disk paths in the .platform.app.yaml file are relative to the directory where that file lives by default.\nThis is the recommended approach for most configurations.\nExplicit source.root\nAs an alternative, you may specify a source.root key in a .platform.app.yaml file to override the \"application root is where the file is\" logic.  The .platform.app.yaml file may then live anywhere in the repository but use code from another directory.  Two separate .platform.app.yaml files may refer to the same directory if desired.\nFor example:\n# .platform.app.yaml\n\nsource:\n    root: restapp\n\n.platform/\nmain/\n    .platform.app.yaml\n.platform.app.yaml\nrestapp/\n    # Your code here\n\nIn this case, the .platform.app.yaml file in main does not specify a source.root, and so will be built from the code in main.  The top-level .platform.app.yaml includes the YAML fragment above.  It will get built using the code in restapp, as if it were in that directory.\nNote that disk parameters in the .platform.app.yaml file will be relative to the source.root directory if specified.  The source.root path is relative to the repository root.\nThe primary use case for this configuration is if the source code is pulled in as a Git submodule or downloaded during the build phase.\napplications.yaml\nIt is possible to define an application in a .platform/applications.yaml file in addition to discrete .platform.app.yaml files.  The syntax is nearly identical, but the source.root key is required.  The applications.yaml file is then a YAML array of application definitions.\nFor example, the following .platform/applications.yaml file defines three applications:\n# .platform/applications.yaml\n-   name: api\n    type: golang:1.14\n    source:\n        root: apiapp\n    hooks:\n        build: |\n            go build -o bin/app\n    web:\n        upstream:\n            socket_family: tcp\n            protocol: http\n        commands:\n            start: ./bin/app\n        locations:\n            /:\n                allow: false\n                passthru: true\n\n-   name: main\n    type: \"php:7.4\"\n    source:\n        root: mainapp\n    web:\n        locations:\n            \"/\":\n                root: \"web\"\n                passthru: \"/index.php\"\n\n-   name: admin\n    type: \"php:7.4\"\n    size: S\n    source:\n        root: mainapp\n    web:\n        locations:\n            \"/\":\n                root: \"web\"\n                passthru: \"/admin.php\"\n\nIn this example, the apiapp directory will get built as a Go application while the mainapp directory will get built as two separate PHP applications, even though none of those directories has a .platform.app.yaml file.  The two PHP applications will use the same source code, but have different front controllers for the admin and main applications.  The admin instance will also be fixed at an S size container, while main will scale freely.\nThe primary use case for this configuration is defining multiple applications with different configuration off of the same source code, or when the source code is downloaded during the build phase.\nSubmodules\nPlatform.sh supports Git submodules, so each application can be in a separate repository.  However, there is currently a notable limitation: the .platform.app.yaml files must be in the top-level repository. That means the project must be structured like this:\n.git/\n.platform/\n    routes.yaml\n    services.yaml\napp1/\n    .platform.app.yaml\n    app1-submodule/\n        index.php\napp2/\n    .platform.app.yaml\n    app2-submodule/\n        index.php\nThis puts your applications' files at a different path relative to your .platform.app.yaml files.  The recommended way to handle that is to specify a source.root key in the .platform.app.yaml file and have it reference the submodule directory.\nMulti-app Routes\nEvery application, however it is defined, must have a unique name property.  The routes.yaml file may then refer to that application by name as an upstream for whatever route is appropriate.\nFor example, assuming this configuration from above:\n.git/\n.platform/\ndrupal/\n  .platform.app.yaml\n  ...\nangular/\n  .platform.app.yaml\n\nThe .platform/routes.yaml file can be structured like this:\n\"https://backend.{default}/\":\n    type: upstream\n    upstream: \"drupal:http\"\n\"https://{default}/\":\n    type: upstream\n    upstream: \"angular:http\"\n\n(This assumes your Drupal application is named drupal and your Angular front-end is named angular.)\nAssuming a domain name of example.com, that will result in:\n\nhttps://backend.example.com/ being served by the Drupal instance.\nhttps://example.com/ being served by the AngularJS instance.\n\nThere is no requirement that an application be web-accessible.  If it is not specified in routes.yaml then it will not be web-accessible at all.  However, if you are building a non-routable application off of the same code base as another application, you should probably consider defining it as a worker instead.  The net result is the same but it is much easier to manage.\nRelationships\nIn a multi-app configuration, applications by default cannot access each other.  However, they may declare a relationships block entry that references another application rather than a service.  In that case the endpoint is http.\nHowever, be aware that circular relationships are not supported.  That is, application A cannot have a relationship to application B if application B also has a relationship to application A.  Such circular relationships are usually a sign that the applications should be coordinating through a shared data store, like a database, RabbitMQ server, or similar.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "App", "title": "Cron and scheduled tasks", "url": "/configuration/app/cron.html", "documentId": "21968236915ffe86b2f2cab1e76ac354ac5b564f", "text": "\n                        \n                            \n                                \n                                \n                                Cron jobs\nCron jobs allow you to run scheduled tasks at specified times or intervals. The crons section of .platform.app.yaml describes these tasks and the schedule when they are triggered.  Each item in the list is a unique name identifying a separate cron job. Crons are started right after build phase.\nIt has a few subkeys listed below:\n\nspec: The cron specification. For example: */20 * * * * to run every 20 minutes.\ncmd: The command that is executed, for example cd public ; drush core-cron\n\nThe minimum interval between cron runs is 5 minutes, even if specified as less.  Additionally, a variable delay is added to each cron job in each project in order to prevent host overloading should every project try to run their nightly tasks at the same time.  Your crons will not run exactly at the time that you specify, but will be delayed by 0-300 seconds.\nA single application container may have any number of cron tasks configured, but only one may be running at a time.  That is, if a cron task fires while another cron task is still running it will pause and then continue normally once the first has completed.\nCron runs are executed using the dash shell, not the bash shell used by normal SSH logins. In most cases that makes no difference but may impact some more involved cron scripts.\nIf an application defines both a web instance and a worker instance, cron tasks will be run only on the web instance.\nHow do I setup Cron for a typical Drupal site?\nThe following example runs Drupal's normal cron hook every 20 minutes, using Drush.  It also sets up a second cron task to run Drupal's queue runner on the aggregator_feeds queue every 7 minutes.\ncrons:\n    # Run Drupal's cron tasks every 20 minutes.\n    drupal:\n        spec: '*/20 * * * *'\n        cmd: 'cd web ; drush core-cron'\n    # But also run pending queue tasks every 7 minutes.\n    # Use an odd number to avoid running at the same time as the `drupal` cron.\n    drush-queue:\n        spec: '*/7 * * * *'\n        cmd: 'cd web ; drush queue-run aggregator_feeds'\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "App", "title": "Worker configuration", "url": "/configuration/app/workers.html", "documentId": "7ffed43228919700e5adc050ea5d77e102618876", "text": "\n                        \n                            \n                                \n                                \n                                Workers\nEvery application may also define zero or more worker instances.  A worker instance runs as its own container independently of the web instance and has no Nginx instance running.  The router service cannot direct public requests to it, either, so running your own web server on a worker (using Node.js or Go) is not useful.\nA worker instance is the exact same code and compilation output as a web instance.  The container image is built only once, and then deployed multiple times if needed.  That is, the build hook and dependencies may not vary from one instance to another.  What may vary is how the container is then configured and how resources are allocated.\nWorker instances are well suited for background tasks such as queue workers, updating indexes, or for running periodic reporting tasks that are too long to make sense as a cron job.  (Although those should often be broken up into queue tasks.)\nA basic, common worker configuration could look like this:\nworkers:\n    queue:\n        size: S\n        commands:\n            start: |\n                php worker.php\n\nThat defines a single worker named queue, which will be a \"small\" container, and wil run the command php worker.php on startup.  If worker.php ever exits it will be automatically restarted.\nAny number of workers may be defined with their own distinct name, subject to available resources on your plan. For resource allocation reasons, using workers in your project requires a Medium plan or larger. \nAccessing the Worker Container\nLike with any other application container Platform.sh allows you to connect to the worker instance through SSH to inspect logs and interact with it.\nUsing the Platform CLI you would use the --worker switch, like so:\nplatform ssh --worker=queue\nTo output the SSH command you can use:\nplatform ssh --worker=queue --pipe\nYou will see the url is the name of the worker added to the user name after the application name part of the SSH url preceded by a double dash (--).\nFor example given a project with id 3seb7f2j6ogbm you would connect to its master environment for an app called app with a url such as:\nssh 3seb7f2j6ogbm-master-7rqtwti--app@ssh.us-2.platform.sh\nTo connect to a worker called queue (as in the example above) you would use an SSH url that would look as follows:\nssh 3seb7f2j6ogbm-master-7rqtwti--app--queue@ssh.us-2.platform.sh\nWorkers vs Cron\nBoth worker instances and cron tasks address similar use cases: They both address out-of-band work that an application needs to do but that should not or cannot be done as part of a normal web request.  They do so in different ways, however, and so are fit for different use cases.\nA Cron job is well suited for tasks when:\n\nThey need to happen on a fixed schedule, not continually.\nThe task itself is not especially long, as a running cron job will block a new deployment.\nOr it is long, but can be easily divided into many small queued tasks.\nA delay between when a task is registered and when it actually happens is acceptable.\n\nA dedicated worker instance is a better fit if:\n\nTasks should happen \"now\", but not block a web request.\nTasks are large enough that they risk blocking a deploy, even if they are subdivided.\nThe task in question is a continually running process rather than a stream of discrete units of work.\n\nThe appropriateness of one approach over the other also varies by language; single-threaded languages would benefit more from either cron or workers than a language with native multi-threading, for instance.  If a given task seems like it would run equally well as a worker or as a cron, cron will generally be more efficient as it does not require its own container.\nCommands\nThe commands key defines the command to launch the worker application.  For now there is only a single command, start, but more will be added in the future.  The commands.start property is required.\nThe start key specifies the command to use to launch your worker application.  It may be any valid shell command, although most often it will run a command in your application in the language of your application.  If the command specified by the start key terminates it will be restarted automatically.\nNote that deploy and post_deploy hooks, as well as cron commands, will run only on the web container, not on workers.\nInheritance\nAny top-level definitions for size, relationships, access, disk and mount, and variables will be inherited by every worker, unless overridden explicitly.\nThat means, for example, that the following two .platform.app.yaml definitions produce identical workers.\nname: app\n\ntype: python:3.5\n\ndisk: 256\nmounts:\n    test:\n        source: local\n        source_path: test\n\nrelationships:\n    database: 'mysqldb:mysql'\n\nworkers:\n    queue:\n        commands:\n            start: |\n                python queue-worker.py\n    mail:\n        commands:\n            start: |\n                python mail-worker.py\n\nname: app\n\ntype: python:3.5\n\nworkers:\n    queue:\n        commands:\n            start: |\n                python queue-worker.py\n        disk: 256\n        mounts:\n            test:\n                source: local\n                source_path: test\n        relationships:\n            database: 'mysqldb:mysql'\n    mail:\n        commands:\n            start: |\n                python mail-worker.py\n        disk: 256\n        mounts:\n            test:\n                source: local\n                source_path: test\n        relationships:\n            database: 'mysqldb:mysql'\n\nIn both cases, there will be two worker instances named queue and mail.  Both will have access to a MySQL/MariaDB service defined in services.yaml named mysqldb through the database relationship.  Both will also have their own separate, independent local disk mount at /app/test with 256 MB of allowed space.\nCustomizing a worker\nThe most common properties to set in a worker to override the top-level settings are size and variables.  size lets you allocate fewer resources to a container that will be running only a single background process (unlike the web site which will be handling many requests at once), while variables lets you instruct the application to run differently as a worker than as a web site.\nFor example, consider this .platform.app.yaml:\nname: app\n\ntype: \"python:3.7\"\n\ndisk: 2048\n\nhooks:\n   build: |\n       pip install -r requirements.txt\n       pip install -e .\n       pip install gunicorn\n\nrelationships:\n    database: 'mysqldb:mysql'\n    messages: 'rabbitqueue:rabbitmq'\n\nvariables:\n    env:\n        type: 'none'\n\nweb:\n    commands:\n        start: \"gunicorn -b $PORT project.wsgi:application\"\n    variables:\n        env:\n            type: 'web'\n    mounts:\n        uploads:\n            source: local\n            source_path: uploads\n    locations:\n         \"/\":\n             root: \"\"\n             passthru: true\n             allow: false\n         \"/static\":\n             root: \"static/\"\n             allow: true\n\nworkers:\n    queue:\n        size: 'M'\n        commands:\n            start: |\n                python queue-worker.py\n        variables:\n            env:\n                type: 'worker'\n        disk: 512\n        mounts:\n            scratch:\n                source: local\n                source_path: scratch\n\n\n\n    mail:\n        size: 'S'\n        commands:\n            start: |\n                python mail-worker.py\n        variables:\n            env:\n                type: 'worker'\n        disk: 256\n        mounts: {}\n        relationships:\n            emails: 'rabbitqueue:rabbitmq'\n\nThere's a lot going on here, but it's all reasonably straightforward.  This configuration will take a single Python 3.7 code base from your repository, download all dependencies in requirements.txt, and the install Gunicorn.  That artifact (your code plus the downloaded dependencies) will be deployed as three separate container instances, all running Python 3.7.\nThe web instance will start a gunicorn process to serve a web application.  \n\nIt will run the gunicorn process to serve web requests, defined by the project/wsgi.py file which contains an application definition.\nIt will have an environment variable named TYPE with value web.\nIt will have a writable mount at /app/uploads with a maximum space of 2048 MB.\nIt will have access to both a MySQL database and a RabbitMQ server, both of which are defined in services.yaml.\nPlatform.sh will automatically allocate resources to it as available on the plan, once all fixed-size containers are allocated.\n\nThe queue instance will be a worker that is not web-accessible.\n\nIt will run the queue-worker.py script, and restart it automatically if it ever terminates.\nIt will have an environment variable named TYPE with value worker.\nIt will have a writable mount at /app/scratch with a maximum space of 512 MB.\nIt will have access to both a MySQL database and a RabbitMQ server, both of which are defined in services.yaml (because it doesn't specify otherwise).\nIt will have \"Medium\" levels of CPU and RAM allocated to it, always.\n\nThe mail instance will be a worker that is not web-accessible.\n\nIt will run the mail-worker.py script, and restart it automatically if it ever terminates.\nIt will have an environment variable named TYPE with value worker.\nIt will have no writable file mounts at all.\nIt will have access only to the RabbitMQ server, through a different relationship name than on the web instance.  It will have no access to MySQL whatsoever.\nIt will have \"Small\" levels of CPU and RAM allocated to it, always.\n\nThis way, the web instance has a large upload space, the queue instance has a small amount of scratch space for temporary files, and the mail instance has no persistent writable disk space at all as it doesn't need it.  The mail instance also does not need any access to the SQL database so for security reasons it has none.  The workers have known fixed sizes, while web can scale to as large as the plan allows.  Each instance can also check the TYPE environment variable to detect how it's running and, if appropriate, vary its behavior accordingly.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "App", "title": "Web configuration", "url": "/configuration/app/web.html", "documentId": "c3f0bed848fd75a2811f01efd6eeefeafe60132e", "text": "\n                        \n                            \n                                \n                                \n                                Web\nThe web key defines a single web instance container running a single web server process (currently Nginx), behind which runs your application.  The web key configures the web server, including what requests should be served directly (such as static files) and which should be passed to your application.  The server is extremely flexible, which means that some configurations will be more involved than others.  Additionally, defaults may vary somewhat between different language base images (specified by the type key of .platform.app.yaml).\nThe first section on this page explains the various options the file supports.  If you prefer, the later sections include various example configurations to demonstrate common patterns and configurations.\nYou can also examine the .platform.app.yaml files of the provided project templates for various common Free Software applications.  See the various language pages for an index of available examples.\nThe web key defines how the application is exposed to the web (in HTTP). Here we tell the web application how to serve content, including static files, front-controller scripts, index files, index scripts, and so on. We support any directory structure, so the static files can be in a subdirectory and the index.php file can be further down.\nCommands\nThe commands key defines the command to launch the application.  For now there is only a single command, start, but more will be added in the future.\nThe start key specifies the command to use to launch your application.  That could be running a uwsgi command for a Python application or a unicorn command for a Ruby application, or simply running your compiled Go application.  If the command specified by the start key terminates it will be restarted automatically.\nweb:\n    commands:\n        start: 'uwsgi --ini conf/server.ini'\n\n\nNote\nNever \"background\" a start process using &amp;.  That will be interpreted as the command terminating and the supervisor process will start a second copy, creating an infinite loop until the container crashes.  Just run it as normal and allow the Platform.sh supervisor to manage it.\n\nOn PHP containers this value is optional and will default to starting PHP-FPM (i.e. /usr/sbin/php-fpm7.0 on PHP7 and /usr/sbin/php5-fpm on PHP5).  On all other containers it should be treated as required.  It can also be set explicitly on a PHP container in order to run a dedicated process such as React PHP or Amp.\nUpstream\nupstream specifies how the front server will connect to your application (the process started by commands.start above).  It has two keys:\n\nsocket_family:\n  Default: tcp. Describes whether your application will listen on a Unix socket (unix) or a TCP socket (tcp).\nprotocol:\n  Specifies whether your application is going to receive incoming requests over HTTP (http) or FastCGI (fastcgi). The default varies depending on which application runtime you're using. Other values will be supported in the future.\n\nOn a PHP container with FPM there is almost never a reason to set the upstream explicitly, as the defaults are already configured properly for PHP-FPM.  On all other containers the default is tcp and http.\nweb:\n    upstream:\n        socket_family: tcp\n        protocol: http\n\nThe above configuration (which is the default on non-PHP containers) will forward connections to the process started by commands.start as a raw HTTP request to a TCP port, as though the process were listening to the incoming request directly.\nSocket family\nIf the socket_family is set to tcp, then your application should listen on the port specified by the PORT environment variable.  (In practice it is almost always 8888, but checking the variable is preferred.)\nIf the socket_family is set to unix, then your application should open the unix socket file specified by the SOCKET environment variable.\nIf your application isn't listening at the same place that the runtime is sending requests, you'll see 502 Bad Gateway errors when you try to connect to your web site.\nLocations\nThe locations block is the most powerful, and potentially most involved, section of the .platform.app.yaml file.  It allows you to control how the application container responds to incoming requests at a very fine-grained level.  Common patterns also vary between language containers due to the way PHP-FPM handles incoming requests.\nEach entry of the locations block is an absolute URI path (with leading /) and its value includes the configuration directives for how the web server should handle matching requests.  That is, if your domain is example.com then '/' means \"requests for example.com/\", while '/admin' means \"requests for example.com/admin\".  If multiple blocks could match an incoming request then the most-specific will apply.\nweb:\n    locations:\n        '/':\n           # Rules for all requests that don't otherwise match.\n            ...\n        '/sites/default/files':\n            # Rules for any requests that begin with /sites/default/files.\n            ...\n\nThe simplest possible locations configuration is one that simply passes all requests on to your application unconditionally:\nweb:\n    locations:\n        '/':\n            passthru: true\n\nThat is, all requests to /* should be forwarded to the process started by web.commands.start above.  Note that for PHP containers the passthru key must specify what PHP file the request should be forwarded to, and must also specify a docroot under which the file lives.  For example:\nweb:\n    locations:\n        '/':\n            root: 'web'\n            passthru: '/app.php'\n\nThis block will serve requests to / from the web directory in the application, and if a file doesn't exist on disk then the request will be forwarded to the /app.php script.\nA full list of the possible subkeys for locations is below.\n\nroot: The folder from which to serve static assets for this location relative to the application root. The application root is the directory in which the .platform.app.yaml file is located.  Typical values for this property include public or web.  Setting it to '' is not recommended, and its behavior may vary depending on the type of application.  Absolute paths are not supported.\npassthru: Whether to forward disallowed and missing resources from this location to the application and can be true, false or an absolute URI path (with leading /). The default value is false. For non-PHP applications it will generally be just true or false.  In a PHP application this will typically be the front controller such as /index.php or /app.php.  This entry works similar to mod_rewrite under Apache.  Note: If the value of passthru does not begin with the same value as the location key it is under, the passthru may evaluate to another entry. That may be useful when you want different cache settings for different paths, for instance, but want missing files in all of them to map back to the same front controller. See the example block below.\nindex: The files to consider when serving a request for a directory: an array of file names or null. (typically ['index.html']). Note that in order for this to work, access to the static files named must be allowed by the allow or rules keys for this location.\nexpires: How long to allow static assets from this location to be cached (this enables the Cache-Control and Expires headers) and can be a time or -1 for no caching (default). Times can be suffixed with \"ms\" (milliseconds), \"s\" (seconds), \"m\" (minutes), \"h\" (hours), \"d\" (days), \"w\" (weeks), \"M\" (months, 30d) or \"y\" (years, 365d).\nscripts: Whether to allow loading scripts in that location (true or false).  This directive is only meaningful on PHP.\nallow: Whether to allow serving files which don't match a rule (true or false, default: true).\nheaders: Any additional headers to apply to static assets. This section is a mapping of header names to header values. Responses from the application aren't affected, to avoid overlap with the application's own ability to include custom headers in the response.\nrules: Specific overrides for a specific location. The key is a PCRE (regular expression) that is matched against the full request path.\nrequest_buffering: Most application servers do not support chunked requests (e.g. fpm, uwsgi), so Platform.sh enables request_buffering by default to handle them. That default configuration would look like this if it was present in .platform.app.yaml:\n  web:\n    locations:\n      '/':\n        passthru: true\n        request_buffering:\n          enabled: true\n          max_request_size: 250m\n  If the application server can already efficiently handle chunked requests, the request_buffering subkey can be modified to disable it entirely (enabled: false). Additionally, applications that frequently deal with uploads greater than 250MB in size can update the max_request_size key to the application's needs. Note that modifications to request_buffering will need to be specified at each location where it is desired.\n\n\nRules\nThe rules block warrants its own discussion as it allows overriding most other keys according to a regular expression. The key of each item under the rules block is a regular expression matching paths more specifically than the locations block entries.  If an incoming request matches the rule, then its handling will be overridden by the properties under the rule.  Note that it will override the entire rule in the case of a compound rule like headers.  (See example below.)\nFor example, the following file will serve dynamic requests from index.php in the public directory and disallow requests for static files anywhere.  Then it sets a rule to explicitly whitelist common image file formats, and sets a cache lifetime for them of 5 minutes.\nweb:\n    locations:\n        '/':\n            root: 'public'\n            passthru: '/index.php'\n            allow: false\n            rules:\n                # Allow common image files only.\n                '\\.(jpe?g|png|gif|svgz?|css|js|map|ico|bmp|eot|woff2?|otf|ttf)$':\n                    allow: true\n                    expires: 300s\n\nAs you can imagine the locations and rules blocks can be used to create highly involved and powerful configurations, but obeys Parker's Law.  (With great power comes great responsibility.)  The examples below demonstrate various common configurations and recommended defaults.\nHow do I setup a basic PHP application with front-controller?\nThe following web block is a reasonable starting point for a custom PHP application.  It sets the directory public as the docroot, and any missing files will get mapped to the /index.php file.  mp4 files are forbidden entirely.  Image files from the images URL (which will be served from the /public/images directory) will have an expiration time set, but non-image files will be disallowed.\nweb:\n    locations:\n        '/':\n            root: 'public'\n            passthru: '/index.php'\n            index:\n                - index.php\n            # No caching for static files.\n            # (Dynamic pages use whatever cache headers are generated by the program.)\n            expires: -1\n            scripts: true\n            allow: true\n            rules:\n                # Disallow .mp4 files specifically.\n                \\.mp4$:\n                    allow: false\n                    expires: -1\n        # Set a 5 min expiration time for static files here; a missing URL\n        # will passthru to the '/' location above and hit the application\n        # front-controller.\n        '/images':\n            expires: 300\n            passthru: true\n            allow: false\n            rules:\n                # Only allow static image files from the images directory.\n                '\\.(jpe?g|png|gif|svgz?|ico|bmp)$':\n                    allow: true\n\nHow can I serve a static-only site?\nAlthough most websites today have some dynamic component, static site generators are a valid way to build a site.  This documentation is built using a Node.js tool called GitBook, and served by Platform.sh as a static site.  You can see the entire repository on GitHub.  The .platform.app.yaml file it uses is listed below.  Note in particular the web.commands.start directive. There needs to be some background process so it's set to the sleep shell command, which will simply block forever (or some really long time, as computers don't know about forever) and restart if needed.  The file also runs the GitBook build process, and then whitelists the files to serve.\n# The name of this app. Must be unique within a project.\nname: app\n\n# There is no runtime requirement, but we want the latest Node at build time.\ntype: nodejs:10\n# Because there is no composer.json file we deactivate the build flavor.\nbuild:\n    flavor: none\n\n# A static site has only minimal CPU and memory needs, so don't ask for much.\nsize: S\n\n# The documentation is built using GitBook. Install the latest version\n# of it globally.\ndependencies:\n  nodejs:\n    gitbook-cli: \"2.3.0\"\n\n# Use a build hook to install the GitBook plugins specified by the book.json file,\n# then build the book. The generated files will be placed in the _book directory\n# by default.\nhooks:\n    build: |\n      set -e\n\n      # Install dependencies\n      npm install\n      npm run build\n\n      # Build example configuration files from the Registry so they can be pulled into docs.\n      node src/scripts/updateExampleConfigFiles.js\n\n      # Install and compile GitBook\n      gitbook install\n      gitbook build\n\n      # Gitbook doesn't allow for a custom favicon except through a theme plugin. So Plan B.\n      cp src/images/favicon.ico _book/gitbook/images/favicon.ico\n\n      # Gitbook doesn't copy over the generated example configuration files to the built book\n      #  automatically, so we have to do that manually.\n      cp -r src/registry/images/examples _book/registry/images\n      cp -r src/registry/images/tables _book/registry/images\n      cp -r src/registry/images/registry.yaml _book/registry/images/\n\n# There is no need for a writable file mount, so set it to the smallest possible size.\ndisk: 256\n\n# Configure the web server to serve our static site.\nweb:\n    commands:\n        # Run a no-op process that uses no CPU resources, since this is a static site.\n        start: sleep infinity\n    locations:\n        \"/\":\n            # The generated static site lives in _book, so make that the docroot.\n            root: \"_book\"\n            # Set an index file of \"index.html\" for any directory. We could also\n            # list multiple fallbacks here if desired.\n            index:\n                - \"index.html\"\n            # Since there is no backend application, return a 404 for a missing\n            # file rather than passing through to a non-existent application.\n            passthru: false\n            # Set a 5 minute cache lifetime on all static files.\n            expires: 300s\n            # Disable all scripts, since we don't have any anyway.\n            scripts: false\n            # By default do not allow any files to be served.\n            allow: false\n            # Whitelist common image file formats, plus HTML files, robots.txt, and\n            # The search_index.json file that is used by GitBook's search plugin.\n            # All other requests will be rejected.\n            rules:\n                \\.(css|js|gif|jpe?g|png|ttf|eot|woff2?|otf|cast|mp4|json|yaml|html|ico|svg?)$:\n                    allow: true\n                ^/robots\\.txt$:\n                    allow: true\n                search_plus_index\\.json$:\n                    allow: true\n\nHow can I control the headers sent with my files?\nThere are many use cases for setting custom headers on static content, such as custom content type headers, limiting cross-origin usage, etc.  Consider the following example:\nweb:\n    locations:\n        \"/\":\n            root: \"public\"\n            passthru: \"/index.php\"\n            index:\n                - index.php\n            headers:\n              X-Frame-Options: SAMEORIGIN\n            rules:\n                \\.mp4$:\n                    headers:\n                      X-Frame-Options: SAMEORIGIN\n                      Content-Type: video/mp4\n                \\.mp3$:\n                    headers:\n                      X-Specialness: low\n\nFirst, the headers directive sets the X-Frame-Options header to SAMEORIGIN for all static files.  That directive is then overriden by the two rules blocks.  For *.mp4 files, two custom headers will be sent: X-Frame-Options and Content-Type.  The repeated X-Frame-Options is necessary as the headers directive in the rule overrides the parent, rather than extending it.  Therefore, the rule for *.mp3 files will add only an X-Specialness header, and no X-Frame-Options header.\nThis example also demonstrates an effective way to set custom Content-Type headers for unusual file types using rules.\nNote that the headers directive applies only to static content.  Headers for responses generated by your application are unaffected.  If custom headers for certain file types or frame control are needed, set them from within the application.\nHow can I rewrite an incoming request without a redirect?\nRules blocks support regular expression capture groups that can be referenced in a passthru command.  For example, the following configuration will result in requests to /project/123 being seen by the application as a request to /index.php?projectid=123 without causing an HTTP redirect.  Note that query parameters present in the request are unaffected and will, unconditionally, appear in the request as seen by the application.\nweb:\n    locations:\n        '/':\n            root: 'public'\n            passthru: '/index.php'\n            index:\n                - index.php\n            scripts: true\n            allow: true\n            rules:\n                '^/project/(?&lt;projectid&gt;.*)$':\n                    passthru: '/index.php?projectid=$projectid'\n\nHow can I serve directories at different paths than in my application?\nAlthough it's common for the directories on disk to be served directly by the web server, that's not actually a requirement.  If desired it is quite possible to create a web URL structure that does not map 1:1 to the structure on disk.\nConsider the following example.  The git repository is structured like so:\n.platform/\n  services.yaml\n  routes.yaml\n.platform.app.yaml\napplication/\n  conf/\n    server.ini\n  application.py\ngitbook-src/\nold-docs/\n\nThe application directory contains a Python application.  The gitbook-src directory contains a GitBook project that is the public documentation for the application.  The old-docs directory contains a static HTML backup of legacy documentation for an older version of the application that is still needed.\nAssume that the GitBook source is compiled by the build process into the _book directory, as in the example above.  The following web block will:\n\nStart your Python application using uwsgi.\nRoute all requests to '/' to the Python application unconditionally, unless one of the following two rules apply.\nRoute requests to the /docs path to the _book directory, which contains our generated documentation, with a short cache lifetime.\nRoute requests to the /docs/legacy path to the old-docs directory, which contains plain old HTML, with a very long cache lifetime since those files should never change.\n\nweb:\n    commands:\n        start: 'uwsgi --ini application/conf/server.ini'\n    locations:\n        '/':\n            passthru: true\n        '/docs':\n            root: '_book'\n            index:\n                - \"index.html\"\n            expires: 300s\n            scripts: false\n            allow: true\n        '/docs/legacy':\n            root: 'old-docs'\n            index:\n                - \"index.html\"\n            expires: 4w\n            scripts: false\n            allow: true\n\nEven though the URL structure doesn't match the directory names or hierarchy on disk, that's no issue.  It also means the application can safely coexist with static files as if it were a single site hierarchy without the need to mix the static pages in with your Python code.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "App", "title": "Build and deploy tasks", "url": "/configuration/app/build.html", "documentId": "468b030e2a210d613b946499525965df229b9c2d", "text": "\n                        \n                            \n                                \n                                \n                                Build and deploy\nThe .platform.app.yaml file provides a number of ways to control how an application gets turned from a directory in Git into a running application.  There are three blocks that control different parts of the process: the build flavor, dependencies, and hooks.  The build process will run the build flavor, then install dependencies, then run the user-provided build hook.  The deploy process will run the deploy hook.\nBuild\nThe build defines what happens when building the application.  Its only property is flavor, which specifies a default set of build tasks to run. Flavors are language-specific.\nPHP (composer by default)\ncomposer will run composer --no-ansi --no-interaction install --no-progress --prefer-dist --optimize-autoloader if a composer.json file is detected.\ndrupal will run drush make automatically in one of a few different ways.  See the Drupal 7 documentation for more details. We recommend only using this build mode for Drupal 7.\nNode.js (default by default)\ndefault will run npm prune --userconfig .npmrc &amp;&amp; npm install --userconfig .npmrc if a package.json file is detected. Note that this also allows you to provide a custom .npmrc file in the root of your application (as a sibling of the .platform.app.yaml file.)\nIn all languages you can also specify a flavor of none (which is the default for any language other than PHP and Node.js); as the name suggests it will take no action at all. That is useful when you want complete control over your build steps, such as to run a custom Composer command or use an alternate Node.js package manager.\nbuild:\n    flavor: composer\n\nBuild dependencies\nIt is also possible to install additional system-level dependencies as part of the build process.  These can be installed before the build hook runs using the native package manager for several web-focused languages.\nPlatform.sh supports pulling any dependencies for the following languages:\n\nPHP (via Composer)\nPython 2 and 3 (via Pip)\nRuby (via Bundler)\nNode.js (via NPM)\nJava (via Apache Maven, Gradle, or Apache Ant)\n\nPython dependencies\nApplications can have both Python 2 and Python 3 dependencies, using the version of each that is packaged with the most recent Debian distribution. The format of Python dependencies complies with PEP 394. That is, specifying a dependency in a python or python2 block will use pip2 and Python 2, while specifying a dependency in a python3 block will use pip3 and Python 3.\nWe suggest that you specify your dependencies with the specific version of Python you wish to use (i.e. either python2 or python3), rather than with the generic python declaration, to ensure your application will function normally in the future if Debian's default version of Python changes.\nSpecifying dependencies\nBuild dependencies are independent of the eventual dependencies of your application and are available in the PATH during the build process and in the runtime environment of your application.  Note that in many cases a given package can be installed either as a global dependency or as part of your application's own dependencies.  In such cases it's up to you which one to use.\nYou can specify those dependencies as shown below:\ndependencies:\n  php: # Specify one Composer package per line.\n    drush/drush: '8.0.0'\n  python: # Specify one Python 2 package per line.\n    behave: '*'\n  python2: # Specify one Python 2 package per line.\n    requests: '*'\n  python3: # Specify one Python 3 package per line.\n    numpy: '*'\n  ruby: # Specify one Bundler package per line.\n    sass: '3.4.7'\n  nodejs: # Specify one NPM package per line.\n    grunt-cli: '~0.1.13'\n\nNote that the package name format for each language is defined by the package manager used; similarly, the version constraint string will be interpreted by the package manager.  Consult the appropriate package manager's documentation for the supported formats.\nHooks\nPlatform.sh supports three \"hooks\", or points in the deployment of a new version of an application that you can inject a custom script into.  Each runs at a different stage of the process.\nEach hook is executed as a single script, so they will be considered failed only if the final command in them fails. To cause them to fail on the first failed command, add set -e to the beginning of the hook.  If a build hook fails for any reason then the build is aborted and the deploy will not happen.\nThe \"home\" directory for each hook is the application root. If your scripts need to be run from the doc root of your application, you will need to cd to it first; e.g.: cd web.\nhooks:\n    build: |\n        set -e\n        cd web\n        cp some_file.php some_other_file.php\n    deploy: |\n        update_schema.sh\n    post_deploy: |\n        set -e\n        import_new_content.sh\n        clear_cache.sh\n\nThe | character tells YAML that the lines that follow should be interpreted literally as a newline-containing string rather than as multiple lines of YAML properties.\nHooks are executed using the dash shell, not the bash shell used by normal SSH logins. In most cases that makes no difference but may impact some more involved scripts.\nBuild hook\nThe build hook is run after the build flavor (if any).  At this point no services (such as a database) are available nor any persistent file mounts, as the application has not yet been deployed. Environment variables that exist only at runtime such as PLATFORM_BRANCH, PLATFORM_DOCUMENT_ROOT etc. are not available during this phase. The full list of build time and runtime variables is available on the variables page.  There are three writeable directories at this time:\n\n$PLATFORM_APP_DIR - This is where your code is checked out, and is the working directory when the build hook starts.  The contents of this directory after the build hook is what will be \"the application\" that gets deployed.  (This directory is always /app, but it's better to use the variable or rely on the working directory than to hard code that.)  Most of the time, this is the only directory you use.\n$PLATFORM_CACHE_DIR - This directory persists between builds, but is NOT deployed as part of your application.  It's a good place for temporary build artifacts, such as downloaded .tar.gz or .zip files, that can be reused between builds.  Note that it is shared by all builds on all branches, so if using the cache directory make sure your build code accounts for that.\n/tmp - The temp directory is also useful for writing files that are not needed in the final application, but it will be wiped between each build.\n\nThere are no constraints on what can be downloaded during your build hook except for the amount of disk available at that time. Independent of the mounted disk size you have allocated for deployment, build environments (the application plus the cache directory) and therefore application images are limited to 4 GB during the build phase. If you exceed this limit you will receive a No space left on device error. It is possible to increase this limit in certain situations, but it will be necessary to open a support ticket in order to do so. Consult the Troubleshooting guide for more information on this topic.\nDeploy hook\nThe deploy hook is run after the application container has been started, but before it has started accepting requests.  You can access other services at this stage (MySQL, Solr, Redis, etc.). The disk where the application lives is read-only at this point.  Note that the deploy hook will only run on a web instance, not on a worker instance.\nBe aware: The deploy hook blocks the site accepting new requests.  If your deploy hook is only a few seconds then incoming requests in that time are paused and will continue when the hook completes, effectively appearing as the site just took a few extra seconds to respond.  If it takes too long, however, requests cannot be held and will appear as dropped connections.  Only run tasks in your deploy hook that have to be run exclusively, such as database schema updates or some types of cache clear.  A post-deploy task that can safely run concurrently with new incoming requests should be run as a post_deploy hook instead.\nAfter a Git push, in addition to the log shown in the activity log, you can see the results of the deploy hook in the /var/log/deploy.log file when logged in to the environment via SSH. It contains the log of the execution of the deployment hook. For example:\n[2014-07-03 10:03:51.100476] Launching hook 'cd public ; drush -y updatedb'.\n\nMy_custom_profile  7001  Update 7001: Enable the Platform module.\nDo you wish to run all pending updates? (y/n): y\nPerformed update: my_custom_profile_update_7001\n'all' cache was cleared.\nFinished performing updates.\n\nPost-Deploy hook\nThe post_deploy hook functions exactly the same as the deploy hook, but after the container is accepting connections.  That is, it will run concurrently with normal incoming traffic.  That makes it well suited to any updates that do not require exclusive database access.\nWhat is \"safe\" to run in a post_deploy hook vs. in a deploy hook will vary by the application.  Often times content imports, some types of cache warmups, and other such tasks are good candidates for a post_deploy hook.\nThe post_deploy hook logs to its own file in addition to the activity log, /var/log/post-deploy.log.\nHow do I compile Sass files as part of a build?\nAs a good example of combining dependencies and hooks, you can compile your SASS files using Grunt.\nLet's assume that your application has Sass source files (Sass being a Ruby tool) in the web/styles directory.  That directory also contains a package.json file for npm and Gruntfile.js for Grunt (a Node.js tool).\nThe following blocks will download a specific version of Sass and Grunt pre-build, then during the build step will use them to install any Grunt dependencies and then run the grunt command.  This assumes that your Grunt command includes the Sass compile command.\ndependencies:\n  ruby:\n    sass: '3.4.7'\n  nodejs:\n    grunt-cli: '~0.1.13'\n\nhooks:\n  build: |\n    cd web/styles\n    npm install\n    grunt\n\nHow can I run certain commands only on certain environments?\nThe deploy and post_deploy hooks have access to all of the same environment variables as the application does normally, which makes it possible to vary those hooks based on the environment.  A common example is to enable certain modules only in non-production environments.  Because the hook is simply a shell script we have full access to all shell scripting capabilities, such as if/then directives.\nThe following Drupal example checks the $PLATFORM_BRANCH variable to see if we're in a production environment (the master branch) or not.  If so, it forces the devel module to be disabled.  If not, it forces the devel module to be enabled, and also uses the drush Drupal command line tool to strip user-specific information from the database.\nhooks:\n    deploy: |\n        if [ \"$PLATFORM_BRANCH\" = master ]; then\n            # Use Drush to disable the Devel module on the Master environment.\n            drush dis devel -y\n        else\n            # Use Drush to enable the Devel module on other environments.\n            drush en devel -y\n            # Sanitize your database and get rid of sensitive information from Master environment.\n            drush -y sql-sanitize --sanitize-email=user_%uid@example.com --sanitize-password=custompassword\n        fi\n        drush -y updatedb\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "App", "title": "Variables", "url": "/configuration/app/variables.html", "documentId": "ef118d7272f66f355ba70fc6febe1110986cc01f", "text": "\n                        \n                            \n                                \n                                \n                                Variables\nPlatform.sh provides a number of ways to set variables, either globally or specific to a single environment.  For values that should be consistent between different environments (because they're configuring the application or runtime itself, generally) the easiest way to control them is to set them in the .platform.app.yaml file.\nOnly prefixed variables may be set from the .platform.app.yaml file.  Some prefixes have specific meaning while others are only significant to a particular application.  Nested variables will be automatically converted into a nested array or list structure as appropriate to the language.\nFor example, the following section in .platform.app.yaml will set a single variable named env:AUTHOR to the value Juan.\nvariables:\n    env:\n        AUTHOR: 'Juan'\n\nThat will have the exact same runtime effect as setting a project variable via the CLI as follows, except it will be versioned along with the code:\n$ platform variable:create env:AUTHOR --level project --value Juan\n\nThe variable name may itself have punctuation in it.  For example, to set a Drupal 8 configuration override (assuming you're using the recommended settings.platformsh.php file) you can do the following:\nvariables:\n    d8config:\n        \"system.site:name\": 'My site rocks'\n\nThis will create a Platform.sh variable, that is, an item in the $PLATFORM_VARIABLES environment variable, named d8config:system.site:name with value \"My site rocks\".\nComplex values\nThe value for a variable may be more than just a string; it may also be a nested structure.  If the variable is in the env namespace, it will be mapped to a Unix environment variable as a JSON string.  If not, it will be included in the PLATFORM_VARIABLES environment variable.\nFor example, the following variable definitions:\nvariables:\n    env:\n        BASIC: \"a string\"\n        INGREDIENTS:\n            - 'peanut butter'\n            - 'jelly'\n        QUANTITIES:\n            \"milk\": \"1 liter\"\n            \"cookies\": \"1 kg\"\n    stuff:\n        STEPS: ['un', 'deux', 'trois']\n        COLORS:\n            red: '#FF0000'\n            green: '#00FF00'\n            blue: '#0000FF'\n\nWould appear this way in various languages:\nPHPPython&lt;?php\nvar_dump($_ENV['BASIC']);\n// string(8) \"a string\"\n\nvar_dump($_ENV['INGREDIENTS']);\n// string(26) \"[\"peanut butter\", \"jelly\"]\"\n\nvar_dump($_ENV['QUANTITIES']);\n// string(38) \"{\"milk\": \"1 liter\", \"cookies\": \"1 kg\"}\"\n\n$variables = json_decode(base64_decode($_ENV['PLATFORM_VARIABLES']), TRUE);\n\nprint_r($variables['stuff:STEPS']);\n/*\narray(3) {\n  [0]=&gt;\n  string(2) \"un\"\n  [1]=&gt;\n  string(4) \"deux\"\n  [2]=&gt;\n  string(5) \"trois\"\n}\n*/\n\nprint_r($variables['stuff:COLORS']);\n/*\narray(3) {\n  [\"red\"]=&gt;\n  string(7) \"#FF0000\"\n  [\"green\"]=&gt;\n  string(7) \"#00FF00\"\n  [\"blue\"]=&gt;\n  string(7) \"#0000FF\"\n}\n*/import os\nimport json\nimport base64\n\nprint os.getenv('BASIC')\n// a string\n\nprint os.getenv('INGREDIENTS')\n// [\"peanut butter\", \"jelly\"]\n\nprint os.getenv('QUANTITIES')\n// {\"milk\": \"1 liter\", \"cookies\": \"1 kg\"}\n\nvariables = json.loads(base64.b64decode(os.getenv('PLATFORM_VARIABLES')).decode('utf-8'))\n\nprint variables['stuff:STEPS']\n// [u'un', u'deux', u'trois']\nprint variables['stuff:COLORS']\n// {u'blue': u'#0000FF', u'green': u'#00FF00', u'red': u'#FF0000'}\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "App", "title": "Storage", "url": "/configuration/app/storage.html", "documentId": "eb8f645e770d952d5cf777ec2efdeb7977f0d105", "text": "\n                        \n                            \n                                \n                                \n                                Storage\nThe built file system image that results from your build process is mounted read-only.  That means it cannot be edited in production, even by accident.\nMany applications still require the ability to write and store files, however.  For that, applications can specify one or more mount points, that is, directories that will be mounted from a writable network file system cluster.  They may be mounted anywhere within the file system of your application.  If the specified directory already exists the contents of it will be masked by the writable mount and inaccessible at runtime.\nDisk\nYour plan storage size specifies the maximum total space available to all applications and services.\nWhen deploying your project, the sum of all disk keys defined in .platform.app.yaml and .platform/services.yaml \nmust be equal or less than the plan storage size. For example, if your plan storage size is 5 GB, you can assign:\n\n2 GB to your application, 3 GB to your database\n1 GB to your application, 4 GB to your database\n1 GB to your application, 1 GB to your database, 3 GB to your Elasticsearch service\netc.\n\nIf you receive an error on git push mentioning the total disk space configured for the application and its services exceeds the plan storage size, you need to either increase the disk space reserved for your project on the project setup page or lower the storage assigned to each service and the application.\nThe disk key is optional.  If set, it defines the size of the persistent disk of the application (in MB).  Its minimum value is 256 MB and a validation error will occur if you try to set it lower.\nMounts\nThe mounts key is an object whose keys are paths relative to the root of the application (that is, where the .platform.app.yaml file lives), and values are a 2-line mount definition.\nThis section is optional: if your application doesn't need writable local file storage, you can omit the mounts section and set disk to the minimum value of 256.\nNote that whether a mounted directory is web-accessible or not depends on the configuration of the web.locations block in .platform.app.yaml.  Depending on the application's needs, it's possible to publish files on writable mounts, leave them private, or have rules for different paths and file types as desired.\nBasic mounts\nThe following block defines a single writable directory, web/uploads:\nmounts:\n    'web/uploads':\n        source: local\n        source_path: uploads\n\nThe source specifies where the writable mount is.  source_path specifies the subdirectory from within the source that the mount should point at.  It is often easiest to have it match the name of the mount point itself but that is not required.\nlocal mounts\nThe local source indicates that the mount point will point to a local directory on the application container.  The source_path is then a subpath of that.  That means they may overlap.  local mounts are not shared between different application containers or workers.\nBe aware that the entire local space for a single app container is a common directory, and the directory is not wiped.  That means if you create a mount point with a source_path of \"uploads\", then write files there, then remove the mount point, the files will still exist on disk indefinitely until manually removed.\nLocal mounts require that the disk key be set.  If it is omitted there will be no storage space available at all.\nservice mounts\nA service mount refers to a network-storage service, as defined in services.yaml.  They function in essentially the same way as a local mount, with two important differences:\n1) The disk size of the service mount is controlled in services.yaml; it is separate from the value of the disk key in .platform.app.yaml.\n2) Multiple application containers may refer to the same service mount and share files.\nA service mount works like so:\nmounts:\n  'web/uploads':\n    source: service\n    service: files\n    source_path: uploads\n\nThis assumes that a network-storage service named files has already been defined.  See the Network Storage page for more details and examples.\nMulti-instance disk mounts\nIf you have multiple application instances defined (using both web and workers), each instance will have its own local disk mounts.  That's the case even if they are named the same, and even if there is only a single top-level mounts directive.  In that case, every instance will have an identical configuration, but separate, independent file spaces.\nIf you want to have multiple application instances share file storage, you will need to use a service mount.\nHow do I set up both public and private file uploads?\nThe following example sets up two file mounts.  One is mounted at /private within the application container, the other at /web/uploads.  The two file mounts together have a limit of 1024 MB of storage.\ndisk: 1024\n\nmounts:\n    'private':\n        source: local\n        source_path: private\n    'web/uploads':\n        source: local\n        source_path: uploads\n\nThen in the web.locations block, you'd specify that the web/uploads path is accessible.  For example, this fragment would specify the /web path as the docroot but provide a more locked-down access to the /web/uploads path.\nweb:\n    locations:\n        '/':\n            # The public directory of the application relative to its root.\n            root: 'web'\n            # The front-controller script which determines where to send\n            # non-static requests.\n            passthru: '/app.php'\n        # Allow uploaded files to be served, but do not run scripts.\n        '/web/uploads':\n            root: 'web/uploads'\n            expires: 300s\n            scripts: false\n            allow: true\n\nSee the web locations documentation for more details.\nWhy can't I mount a hidden folder?\nPlatform.sh ignores YAML keys that start with a dot. This causes a mount like .myhiddenfolder to be ignored. If you want to mount a hidden folder, you'll have to prepend it with a /:\nmounts:\n  '/.myhiddenfolder':\n    source: local\n    source_path: 'myhiddenfolder'\n\nHow do I setup overlapping mount paths?\nWhile not recommended it is possible to setup multiple mount points whose source paths overlap.  Consider the following example:\nmounts:\n    'private':\n        source: local\n        source_path: stuff\n    'secret':\n        source: local\n        source_path:  stuff/secret\n\nIn this configuration, there will be two mount points as seen from the application: ~/private and ~/secret.  However, the secret mount will point to a directory that is also under the mount point for private.  That is, the secret path and the private/secret path will be the exact same directory.\nAlthough this configuration won't cause any technical issues, it may be quite confusing so is generally not recommended.\nChecking the size of mounts\nYou can use standard commands such as df -ah to find the total disk usage of mounts (which are usually all on the same filesystem) and du -hs /path/to/dir to check the size of individual directories.\nThe CLI provides a command that combines these checks:\n$ platform mount:size\nChecking disk usage for all mounts of the application 'app'...\n+-------------------------+-----------+---------+-----------+-----------+----------+\n| Mount(s)                | Size(s)   | Disk    | Used      | Available | Capacity |\n+-------------------------+-----------+---------+-----------+-----------+----------+\n| private                 | 55.2 MiB  | 1.9 GiB | 301.5 MiB | 1.6 GiB   | 15.5%    |\n| tmp                     | 34.1 MiB  |         |           |           |          |\n| web/sites/default/files | 212.2 MiB |         |           |           |          |\n+-------------------------+-----------+---------+-----------+-----------+----------+\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "App", "title": "Access", "url": "/configuration/app/access.html", "documentId": "6b63e44fc02217590db524f6983a84cb052f7e39", "text": "\n                        \n                            \n                                \n                                \n                                Access\nThe access key in .platform.app.yaml defines the user roles who can log in via SSH to the environments they have permission to access.  The specified role is a minimum; anyone with an access level of this role or higher can access the container via SSH.\nPossible values are admin, contributor, and viewer.  The default is contributor.\nHow do I restrict SSH access only to project administrators?\nThe following block in .platform.app.yaml will restrict SSH access to just those users with \"admin\" privileges on the project or on the specific deployed environment.\naccess:\n  ssh: admin\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "App", "title": "Relationships", "url": "/configuration/app/relationships.html", "documentId": "175144213248e59aeaaaf5012d44b34a45526d0d", "text": "\n                        \n                            \n                                \n                                \n                                Relationships\nThe relationships block defines how services are mapped within your application.  By default, your application may not talk to any other container within a project.  To access another container you must define a relationship to it.\n\nEach relationship has an arbitrary name, although by convention the primary SQL database of an application (if any) is usually database.  That is the name by which the relationship will be known in the PLATFORM_RELATIONSHIPS environment variable, which will include credentials for accessing the service.\nThe relationship is specified in the form service_name:endpoint_name.  The \"service name\" is the name of the service given in .platform/services.yaml, or the name of another application in the same project (that is, the name property of the .platform.app.yaml file for that application).  The \"endpoint\" is the exposed functionality of the service to use.  For most services that is simply the same as the service type, but on MySQL, for example, could be different if the service is running multiple databases.\nSee the Services documentation for a full list of currently supported service types and service endpoints.\nHow do I get access to multiple services?\nIn the following example, there is a single MySQL service named mysqldb offering two databases, a Redis cache service named rediscache, and an Elasticsearch service named searchserver.\nrelationships:\n    database: 'mysqldb:db1'\n    database2: 'mysqldb:db2'\n    cache: 'rediscache:redis'\n    search: 'searchserver:elasticsearch'\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "App", "title": "Size", "url": "/configuration/app/size.html", "documentId": "19a5e992aad4a51d55ac13d3b32beb598548d282", "text": "\n                        \n                            \n                                \n                                \n                                Custom sizing\n\nNote\nThese are advanced settings and should only be used by experienced Platform.sh users.  99.9% of the time our default container sizes are the correct choice for best performance.\n\nBy default, Platform.sh will automatically select appropriate resource sizes (CPU and memory) for a container when it's deployed, based on the plan size and the number of other containers in the cluster.  The more containers in a project the fewer resources each one gets, and vice versa, with similar containers getting similar resources.\nUsually that's fine, but sometimes it's undesirable.  You may, for instance, want to have a queue worker container that you know has low memory and CPU needs, so it's helpful to give that one fewer resources and another container more.  Or a given service may be very heavily used in your architecture so it needs all the resources it can take.  In those cases you can provide sizing hints to the system on a per-service basis.\nEvery application container as well as every service in .platform/services.yaml supports a size key, which instructs the system how many resources to allocate to it.  The exact CPU and memory allocated will depend on the application or service type, and we may adjust these values over time to better optimize resource usage.\nLegal values for the size key are AUTO (the default), S, M, L, XL, 2XL, 4XL.\nNote that in a development environment this value is ignored and always set to S.  It will only take effect in a production deployment (a master branch with an associated domain).  If the total resources requested by all apps and services is larger than what the plan size allows then a production deployment will fail with an error.\nHow do I make a background processing container smaller to save resources?\nSimply set the size key to S to ensure that the container gets fewer resources, leaving more to be allocated to other containers.\nname: processing\n\ntype: nodejs:6.11\nsize: S\n\n...\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "App", "title": "Name", "url": "/configuration/app/name.html", "documentId": "ea0a8baebd7609043ddd0e6a39fe89a6d4d08254", "text": "\n                        \n                            \n                                \n                                \n                                Name\nThe name is the unique identifier of the application. Platform.sh supports multiple applications within a project, so each application must have a unique name within a project. The name may only be composed of lower case alpha-numeric characters (a-z0-9).\n\nWarning\nChanging the name of your application after it has been deployed will destroy all storage volumes and result in the loss of all persistent data.  This is typically a Very Bad Thing to do. It could be useful under certain circumstances in the early stages of development but you almost certainly don't want to change it on a live project.\n\nThis name is used in the .platform/routes.yaml file to define the HTTP upstream (by default php:http).  For instance, if you called your application app you will need to use app:http in the upstream field.\nYou can also use this name in multi-application relationships.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "App", "title": "Timezone", "url": "/configuration/app/timezone.html", "documentId": "25dd5606a960ab53412d576640daf2c3baf05259", "text": "\n                        \n                            \n                                \n                                \n                                Cron timezone\nAll Platform.sh containers default to running in UTC time.  Applications and application runtimes may elect to use a different timezone but the container itself runs in UTC.  That includes the spec parameter for cron tasks that are defined by the application.\nThat is generally fine but sometimes it's necessary to run cron tasks in a different timezone.\nSetting the system timezone for cron tasks\nThe timezone property sets the timezone for which the spec property of any cron tasks defined by the application will be interpreted.  Its value is one of the tz database region codes such as Europe/Paris or America/New_York.  This key will apply to all cron tasks defined in that file.\nThis entry is only meaningful on cron specs that specify a particular time of day, rather than a \"time past each hour\".  For example, 25 1 * * * would run every day at 1:25 am in the timezone specified.\nSetting an application runtime timezone\nThe application runtime timezone can also be set, although the mechanism varies a bit by the runtime.\n\nPHP runtime - You can change the timezone by providing a custom php.ini.\nNode.js runtime - You can change the timezone by starting the server with env TZ='&lt;timezone&gt;' node server.js.\nPython runtime - You can change the timezone by starting the server with env TZ='&lt;timezone&gt;' python server.py.\n\nSetting the application timezone will only affect the application itself, not system operations such as log files.\n\nnote\nIn the vast majority of cases it's best to leave all timezones in UTC and store user data with an associated timezone instead.\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "App", "title": "Type", "url": "/configuration/app/type.html", "documentId": "ce72d86e3ddeacd80e18661cf32ebead17316292", "text": "\n                        \n                            \n                                \n                                \n                                Type\nThe type key defines the base container image that will be used to run the application.  There is a separate base container image for each primary language for the application, often in multiple versions.  \nSupported types\nAvailable languages and their supported versions include:\n\n\n\nLanguage\nruntime\nSupported version\n\n\n\n\nC#/.Net Core\ndotnet\n2.0, 2.1, 2.2, 3.1\n\n\nGo\ngolang\n1.11, 1.12, 1.13, 1.14\n\n\nJava\njava\n11, 12, 8, 13\n\n\nLisp\nlisp\n1.5\n\n\nNode.js\nnodejs\n6, 8, 10, 12\n\n\nPHP\nphp\n7.2, 7.3, 7.4\n\n\nPython\npython\n2.7, 3.5, 3.6, 3.7, 3.8\n\n\nRuby\nruby\n2.3, 2.4, 2.5, 2.6, 2.7\n\n\n\nExample configuration\ntype: 'php:7.4'\nRuntime\nThe .platform.app.yaml file also supports a runtime key that allows selected customizations to the language runtime. As those possibilities vary by language, please see the appropriate language documentation.\n\nPHP\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Configuration", "title": "Apps (.platform.app.yaml)", "url": "/configuration/app-containers.html", "documentId": "d7ccccd74232b0e4e41522fadfd06a4d2774c723", "text": "\n                        \n                            \n                                \n                                \n                                Configure your Application\nYou control your application and the way it will be built and deployed on Platform.sh via a single configuration file, .platform.app.yaml, located at the root of your application folder inside your Git repository.\n\nThe .platform.app.yaml file is extremely flexible.  Depending on your needs it could be less than 10 lines long or over 100.  The only required keys are name, type, disk, and at least one \"instance definition\", either a web or worker block.  All others are optional.\nYour application code can generate one or more application instances. Web instances can be accessed from the outside world, while workers cannot and just run a persistent background process. Otherwise they are very similar.\nDifferent configuration properties can be applied to individual web and worker instances, or globally to all of them.  In the most typical case, with one web instance and no workers, it's common to just list each of the configuration directives below as a top-level property.  However, they can also be specified within the web or worker blocks to apply to just those instances.\nThe following properties apply only at the global level, and cannot be replicated inside an instance definition.\n\nname (required) - Sets the unique name of the application container.\ntype (required) - Sets the container base image to use, including application language.\ntimezone - Sets the timezone of cron tasks in application container.\nbuild, dependencies, and hooks - Control how the application gets compiled.  Note that this compilation happens before the application is copied into different instances, so any steps here will apply to all web and worker instances.\ncron - Defines scheduled tasks for the application.  Cron tasks will, technically, run as part of the web instance regardless of how many workers are defined.\nsource.root - This nested value specifies the path where all code for the application lives.  It defaults to the directory where the .platform.app.yaml file is defined.  It is rarely needed except in advanced configurations.\n\nThe following properties can be set at the top level of the .platform.app.yaml file and apply to all application instances, or set within a given instance definition and apply just to that one.  If set in both places then the instance-specific one will take precedence, and completely replace the global one.  That is, if you want to make a change to just one sub-property of one of the following keys you need to replicate the entire block.\n\nsize - Sets an explicit sizing hint for the application.\nrelationships - Defines connections to other services and applications.\naccess - Restricts SSH access with more granularity than the management console.\ndisk and mounts (required) - Defines writable file directories for the application.\nvariables - Sets environment variables that control application behavior.\n\nThe .platform.app.yaml file needs at least one of the following to define an instance, but may define both.\n\nweb - Controls how the web application is served.\nworker - Defines alternate copies of the application to run as background processes.\n\nAvailable resources\nEach web or worker instance is its own running container, which takes its own resources.  The size key allows some control over how many resources each container gets and if omitted the system will select one of a few fixed sizes for each container automatically.  All application and service containers are given resources out of a common pool defined by your plan size.  That means the more containers you define, the fewer resources each one will get and you may need to increase your plan size.\nCompression\nPlatform.sh does not compress any dynamic responses generated by your application due to a well known security issue.  While your application can compress its own response, doing so when the response includes any user-specific information, including a session cookie, opens up an attack vector over SSL/TLS connections.  For that reason we recommend against compressing any generated responses.\nRequests for static files that are served directly by Platform.sh are compressed automatically using either gzip or brotli compression if:\n\nThe request headers for the file support gzip or brotli.\nThe file is served directly from disk by Platform.sh, not passed through your application.\nThe file would be served with a cache expiration time in the future.\nThe file type is one of: html, javascript, json, pdf, postscript, svg, css, csv, plain text, or XML.\n\nAdditionally, if a file with a \".gz\" or \".br\" extension exists that will be served instead for the appropriate compression type regardless of the file type.  That is, a request for styles.css that accepts a gzipped file (according to the request headers) will automatically return the contents of styles.css.gz if it exists.  This approach supports any file type and offers some CPU optimization, especially if the cache lifetime is short.\nExample configuration\nAn example of a minimalist .platform.app.yaml file for PHP, heavily commented, is below:\n# .platform.app.yaml\n\n# The name of this application, which must be unique within a project.\nname: 'app'\n\n# The type key specifies the language and version for your application.\ntype: 'php:7.0'\n\n# On PHP, there are multiple build flavors available. Pretty much everyone\n# except Drupal 7 users will want the composer flavor.\nbuild:\n  flavor: composer\n\n# The relationships of the application with services or other applications.\n# The left-hand side is the name of the relationship as it will be exposed\n# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand\n# side is in the form `&lt;service name&gt;:&lt;endpoint name&gt;`.\nrelationships:\n    database: 'mysqldb:mysql'\n\n# The hooks that will be triggered when the package is deployed.\nhooks:\n    # Build hooks can modify the application files on disk but not access any services like databases.\n    build: |\n      rm web/app_dev.php\n    # Deploy hooks can access services but the file system is now read-only.\n    deploy: |\n      app/console --env=prod cache:clear\n\n\n# The size of the persistent disk of the application (in MB).\ndisk: 2048\n\n# The 'mounts' describe writable, persistent filesystem mounts in the application.\n# The keys are directory paths relative to the application root. The values are a\n# mount definition. In this case, `web-files` is just a unique name for the mount.\nmounts:\n    'web/files':\n        source: local\n        source_path: 'web-files'\n\n# The configuration of the application when it is exposed to the web.\nweb:\n    locations:\n        '/':\n            # The public directory of the application relative to its root.\n            root: 'web'\n            # The front-controller script which determines where to send\n            # non-static requests.\n            passthru: '/app.php'\n        # Allow uploaded files to be served, but do not run scripts.\n        # Missing files get mapped to the front controller above.\n        '/files':\n            root: 'web/files'\n            scripts: false\n            allow: true\n            passthru: '/app.php'\n\n\nNote\nThis configuration file is specific to one application. If you have multiple applications inside your Git repository (such as a RESTful web service and a front-end, or a main web site and a blog), you need .platform.app.yaml at the root of each application. See the Multi-app documentation.\n\nUpgrading from previous versions of the configuration file.\nAlthough we make an effort to always maintain backward compatibility in the .platform.app.yaml format, we do from time to time upgrade the file and encourage you to upgrade as well.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Services", "title": "Varnish", "url": "/configuration/services/varnish.html", "documentId": "70080339322d75ea1ba555863b863be1314730b0", "text": "\n                        \n                            \n                                \n                                \n                                Varnish HTTP Proxy\nVarnish is a popular HTTP proxy server, often used for caching.  It is usually not needed on Platform.sh, as each project's router provides an HTTP cache already and most more advanced use cases will use a CDN instead, both of which render Varnish redundant.\nHowever, it is possible to configure a Varnish instance as part of an application if Varnish-specific functionality is needed.\nSupported versions\n\n5.2\n6.0\n\nHow it works\nAll incoming requests still go through the environment's router first.  When using Varnish, a Varnish service sits between the router and the application server or servers.\nweb -&gt; router -&gt; varnish -&gt; application\n                         -&gt; application2\nConfiguration\nAdd a Varnish service\nAdd the following to your .platform/services.yaml file:\nproxy:\n    type: varnish:6.0\n    relationships:\n        application: 'app:http'\n    configuration:\n        vcl: !include\n            type: string\n            path: config.vcl\nIn the relationships block, define a relationship (application) to the application container (app) using the http endpoint.  That allows Varnish to talk to the application container.\nThe configuration block is required, and must reference a VCL file (here config.vcl).  The file name is relative to the .platform directory.\nCreate a VCL template file\nThe VCL file you provide has three specific requirements over and above the VCL syntax itself.\n\nYou MUST NOT define a vcl_init() function.  Platform.sh will auto-generate that function based on the relationships you define.  In particular, it will define a \"backend\" for each relationship defined in services.yaml, named the same as the relationship.\nYou MUST NOT include the preamble at the beginning of the file, specifying the VCL version.  That will be auto-generated as well. You CAN add imports, but not std and directors.\nYou MUST specify the backend to use in vcl_recv().  If you have a single app container/relationship/backend, it's just a single line.  If you want to split requests to different relationships/backends based on some rule then the logic for doing so should be incorporated into the vcl_recv() function.\n\nThe absolute bare minimum VCL file is:\nsub vcl_recv {\n    set req.backend_hint = application.backend();\n}\nWhere application is the name of the relationship defined in services.yaml.  (If the relationship was named differently, use that name instead.)\nIf you have multiple applications fronted by the same Varnish instance then you will need to include logic to determine to which application a request is forwarded.  For example:\nvarnish:\n    type: varnish:6.0\n    relationships:\n        blog: 'blog:http'\n        main: 'app:http'\n    configuration:\n        vcl: !include\n            type: string\n            path: config.vcl\n\n# config.vcl\nsub vcl_recv {\n    if (req.url ~ \"^/blog/\") {\n        set req.backend_hint = blog.backend();\n    } else {\n        set req.backend_hint = main.backend();\n    }\n}\nThis configuration will direct all requests to a URL beginning with a /blog/ path to the application on the relationship blog, and all other requests to the application on the relationship main.\nBesides that, the VCL file, including the vcl_recv() function, can be arbitrarily complex to suit the needs of the project.  That includes additional include directives if appropriate.  See the Varnish documentation for more details on the functionality offered by Varnish.\n\nnote\nA misconfigured VCL file can result in incorrect, often mysterious and confusing behavior.  Platform.sh does not provide support for VCL configuration options beyond the basic connection logic documented here.\n\nRoute incoming requests to Varnish\nTo enable Varnish now, edit the .platform/routes.yaml file to point to the Varnish service you just created.  You also need to disable the router cache as it is now entirely redundant with Varnish.\nFor example:\n\"https://{default}/\":\n    type: upstream\n    upstream: \"varnish:http\"\n    cache:\n        enabled: false\nThat will map all incoming requests to the Varnish service rather than the application.  Varnish will then, based on the VCL file, forward requests to the application as appropriate.\nModules\nPlatform.sh supports a number of optional modules you can include in your VCLs, namely:\n\ncookie\nheader\nsaintmode\nsoftpurge\ntcp\nvar\nvsthrottle\nxkey\n\nTo use in your VCL, add an import such as:\nimport xkey;\nCircular relationships\nAt this time Platform.sh does not support circular relationships between services or applications.  That means you cannot add a relationship in your .platform.app.yaml that points to the Varnish service.  If you do so then one of the relationships will be skipped and the connection will not work.  This limitation may be lifted in the future.\nStats endpoint\nThe Varnish service also offers an http+stats endpoint, which provides access to some Varnish analysis and debugging tools.  To access it, from a dedicated app container add the following to .platform.app.yaml:\nrelationships:\n    varnishstats: \"proxy:http+stats\"\nYou can then access the varnishstats relationship over HTTP at the following paths to get diagnostic information:\n\n/: returns the error if generating the VCL failed with an error\n/config: returns the generated VCL\n/stats: returns the output of varnishstat\n/logs: returns a streaming response of varnishlog\n\nNote that because of the circular relationship issue noted above this cannot be done on the application that Varnish is forwarding to.  It will need to be run on a separate application container.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Services", "title": "Solr", "url": "/configuration/services/solr.html", "documentId": "5acd66802f329f1bfa3ba60ea75f72c83c6eb209", "text": "\n                        \n                            \n                                \n                                \n                                Solr (Search Service)\nApache Solr is a scalable and fault-tolerant search index.\nSolr search with generic schemas provided, and a custom schema is also supported.\nSee the Solr documentation for more information.\nSupported versions\n\n3.6\n4.10\n6.3\n6.6\n7.6\n7.7\n8.0\n8.4\n\nRelationship\nThe format exposed in the $PLATFORM_RELATIONSHIPS environment variable:\n{\n    \"service\": \"solr\",\n    \"ip\": \"169.254.202.136\",\n    \"hostname\": \"7iug3likvszuk2vnf4y3dafara.solr.service._.eu-3.platformsh.site\",\n    \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\",\n    \"host\": \"solr.internal\",\n    \"rel\": \"solr\",\n    \"path\": \"solr\\/maincore\",\n    \"scheme\": \"solr\",\n    \"type\": \"solr:8.0\",\n    \"port\": 8080\n}\nUsage example\nIn your .platform/services.yaml:\nsearchsolr:\n    type: solr:8.4\n    disk: 256\nIn your .platform.app.yaml:\nrelationships:\n    solrsearch: \"searchsolr:solr\"\nYou can then use the service in a configuration file of your application with something like:\nGoJavaNode.jsPHPPythonpackage examples\n\nimport (\n\t\"fmt\"\n\tpsh \"github.com/platformsh/config-reader-go/v2\"\n\tgosolr \"github.com/platformsh/config-reader-go/v2/gosolr\"\n\tsolr \"github.com/rtt/Go-Solr\"\n)\n\nfunc UsageExampleSolr() string {\n\n\t// Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables.\n\t// You can alternatively use os.Getenv() yourself.\n\tconfig, err := psh.NewRuntimeConfig()\n\tcheckErr(err)\n\n\t// Get the credentials to connect to the Solr service.\n\tcredentials, err := config.Credentials(\"solr\")\n\tcheckErr(err)\n\n\t// Retrieve Solr formatted credentials.\n\tformatted, err := gosolr.FormattedCredentials(credentials)\n\tcheckErr(err)\n\n\t// Connect to Solr using the formatted credentials.\n\tconnection := &amp;solr.Connection{URL: formatted}\n\n\t// Add a document and commit the operation.\n\tdocAdd := map[string]interface{}{\n\t\t\"add\": []interface{}{\n\t\t\tmap[string]interface{}{\"id\": 123, \"name\": \"Valentina Tereshkova\"},\n\t\t},\n\t}\n\n\trespAdd, err := connection.Update(docAdd, true)\n\tcheckErr(err)\n\n\t// Select the document.\n\tq := &amp;solr.Query{\n\t\tParams: solr.URLParamMap{\n\t\t\t\"q\": []string{\"id:123\"},\n\t\t},\n\t}\n\n\tresSelect, err := connection.CustomSelect(q, \"query\")\n\tcheckErr(err)\n\n\t// Delete the document and commit the operation.\n\tdocDelete := map[string]interface{}{\n\t\t\"delete\": map[string]interface{}{\n\t\t\t\"id\": 123,\n\t\t},\n\t}\n\n\tresDel, err := connection.Update(docDelete, true)\n\tcheckErr(err)\n\n\tmessage := fmt.Sprintf(\\x60Adding one document - %s&lt;br&gt;\nSelecting document (1 expected): %d&lt;br&gt;\nDeleting document - %s&lt;br&gt;\n  \\x60, respAdd, resSelect.Results.NumFound, resDel)\n\n\treturn message\n}\npackage sh.platform.languages.sample;\n\nimport org.apache.solr.client.solrj.SolrQuery;\nimport org.apache.solr.client.solrj.SolrServerException;\nimport org.apache.solr.client.solrj.impl.HttpSolrClient;\nimport org.apache.solr.client.solrj.impl.XMLResponseParser;\nimport org.apache.solr.client.solrj.response.QueryResponse;\nimport org.apache.solr.client.solrj.response.UpdateResponse;\nimport org.apache.solr.common.SolrDocumentList;\nimport org.apache.solr.common.SolrInputDocument;\nimport sh.platform.config.Config;\nimport sh.platform.config.Solr;\n\nimport java.io.IOException;\nimport java.util.function.Supplier;\n\npublic class SolrSample implements Supplier&lt;String&gt; {\n\n    @Override\n    public String get() {\n\n        StringBuilder logger = new StringBuilder();\n\n        // Create a new config object to ease reading the Platform.sh environment variables.\n        // You can alternatively use getenv() yourself.\n        Config config = new Config();\n\n        Solr solr = config.getCredential(\"solr\", Solr::new);\n\n        try {\n\n            final HttpSolrClient solrClient = solr.get();\n            solrClient.setParser(new XMLResponseParser());\n\n            // Add a document\n            SolrInputDocument document = new SolrInputDocument();\n            final String id = \"123456\";\n            document.addField(\"id\", id);\n            document.addField(\"name\", \"Ada Lovelace\");\n            document.addField(\"city\", \"London\");\n            solrClient.add(document);\n            final UpdateResponse response = solrClient.commit();\n            logger.append(\"Adding one document. Status (0 is success): \")\n                    .append(response.getStatus()).append('\\n');\n\n            SolrQuery query = new SolrQuery();\n            query.set(\"q\", \"city:London\");\n            QueryResponse queryResponse = solrClient.query(query);\n\n            SolrDocumentList results = queryResponse.getResults();\n            logger.append(String.format(\"Selecting documents (1 expected):  %d \\n\", results.getNumFound()));\n\n            // Delete one document\n            solrClient.deleteById(id);\n\n            logger.append(String.format(\"Deleting one document. Status (0 is success):  %s \\n\",\n                    solrClient.commit().getStatus()));\n        } catch (SolrServerException | IOException exp) {\n            throw new RuntimeException(\"An error when execute Solr \", exp);\n        }\n\n        return logger.toString();\n    }\n}const solr = require('solr-node');\nconst config = require(\"platformsh-config\").config();\n\nexports.usageExample = async function() {\n\n    let client = new solr(config.formattedCredentials('solr', 'solr-node'));\n\n    let output = '';\n\n    // Add a document.\n    let addResult = await client.update({\n        id: 123,\n        name: 'Valentina Tereshkova',\n    });\n\n    output += \"Adding one document. Status (0 is success): \" + addResult.responseHeader.status +  \"&lt;br /&gt;\\n\";\n\n    // Flush writes so that we can query against them.\n    await client.softCommit();\n\n    // Select one document:\n    let strQuery = client.query().q();\n    let writeResult = await client.search(strQuery);\n    output += \"Selecting documents (1 expected): \" + writeResult.response.numFound + \"&lt;br /&gt;\\n\";\n\n    // Delete one document.\n    let deleteResult = await client.delete({id: 123});\n    output += \"Deleting one document. Status (0 is success): \" + deleteResult.responseHeader.status + \"&lt;br /&gt;\\n\";\n\n    return output;\n};\n&lt;?php\ndeclare(strict_types=1);\n\nuse Platformsh\\ConfigReader\\Config;\nuse Solarium\\Client;\n\n// Create a new config object to ease reading the Platform.sh environment variables.\n// You can alternatively use getenv() yourself.\n$config = new Config();\n\n// Get the credentials to connect to the Solr service.\n$credentials = $config-&gt;credentials('solr');\n\ntry {\n\n    $config = [\n        'endpoint' =&gt; [\n            'localhost' =&gt; [\n                'host' =&gt; $credentials['host'],\n                'port' =&gt; $credentials['port'],\n                'path' =&gt; \"/\" . $credentials['path'],\n            ]\n        ]\n    ];\n\n    $client = new Client($config);\n\n    // Add a document\n    $update = $client-&gt;createUpdate();\n\n    $doc1 = $update-&gt;createDocument();\n    $doc1-&gt;id = 123;\n    $doc1-&gt;name = 'Valentina Tereshkova';\n\n    $update-&gt;addDocuments(array($doc1));\n    $update-&gt;addCommit();\n\n    $result = $client-&gt;update($update);\n    print \"Adding one document. Status (0 is success): \" .$result-&gt;getStatus(). \"&lt;br /&gt;\\n\";\n\n    // Select one document\n    $query = $client-&gt;createQuery($client::QUERY_SELECT);\n    $resultset = $client-&gt;execute($query);\n    print  \"Selecting documents (1 expected): \" .$resultset-&gt;getNumFound() . \"&lt;br /&gt;\\n\";\n\n    // Delete one document\n    $update = $client-&gt;createUpdate();\n\n    $update-&gt;addDeleteById(123);\n    $update-&gt;addCommit();\n    $result = $client-&gt;update($update);\n    print \"Deleting one document. Status (0 is success): \" .$result-&gt;getStatus(). \"&lt;br /&gt;\\n\";\n\n} catch (Exception $e) {\n    print $e-&gt;getMessage();\n}\n\nimport pysolr\nfrom xml.etree import ElementTree as et\nfrom platformshconfig import Config\n\n\ndef usage_example():\n\n    # Create a new Config object to ease reading the Platform.sh environment variables.\n    # You can alternatively use os.environ yourself.\n    config = Config()\n\n    # Get the credentials to connect to the Solr service.\n    credentials = config.credentials('solr')\n\n    try:\n        formatted_url = config.formatted_credentials('solr', 'pysolr')\n\n        # Create a new Solr Client using config variables\n        client = pysolr.Solr(formatted_url)\n\n        # Add a document\n        message = ''\n        doc_1 = {\n            \"id\": 123,\n            \"name\": \"Valentina Tereshkova\"\n        }\n\n        result0 = client.add([doc_1])\n        client.commit()\n        message += 'Adding one document. Status (0 is success): {0} &lt;br /&gt;'.format(et.fromstring(result0)[0][0].text)\n\n        # Select one document\n        query = client.search('*:*')\n        message += '\\nSelecting documents (1 expected): {0} &lt;br /&gt;'.format(str(query.hits))\n\n        # Delete one document\n        result1 = client.delete(doc_1['id'])\n        client.commit()\n        message += '\\nDeleting one document. Status (0 is success): {0}'.format(et.fromstring(result1)[0][0].text)\n\n        return message\n\n    except Exception as e:\n        return e\n\nSolr 4\nFor Solr 4, Platform.sh supports only a single core per server called collection1.\nYou must provide your own Solr configuration via a core_config key in your .platform/services.yaml:\nsearch:\n    type: solr:4.10\n    disk: 1024\n    configuration:\n        core_config: !archive \"&lt;directory&gt;\"\n\nThe directory parameter points to a directory in the Git repository, in or below the .platform/ folder. This directory needs to contain everything that Solr needs to start a core. At the minimum, solrconfig.xml and schema.xml.  For example, place them in .platform/solr/conf/ such that the schema.xml file is located at .platform/solr/conf/schema.xml.   You can then reference that path like this -\nsearch:\n    type: solr:4.10\n    disk: 1024\n    configuration:\n        core_config: !archive \"solr/conf/\"\n\nSolr 6 and later\nFor Solr 6 and later Platform.sh supports multiple cores via different endpoints.  Cores and endpoints are defined separately, with endpoints referencing cores.  Each core may have its own configuration or share a configuration.  It is best illustrated with an example.\nsearch:\n    type: solr:8.4\n    disk: 1024\n    configuration:\n        cores:\n            mainindex:\n                conf_dir: !archive \"core1-conf\"\n            extraindex:\n                conf_dir: !archive \"core2-conf\"\n        endpoints:\n            main:\n                core: mainindex\n            extra:\n                core: extraindex\n\nThe above definition defines a single Solr 8.0 server.  That server has 2 cores defined: mainindex \u2014 the configuration for which is in the .platform/core1-conf directory \u2014 and extraindex \u2014 the configuration for which is in the .platform/core2-conf directory.\nIt then defines two endpoints: main is connected to the mainindex core while extra is connected to the extraindex core.  Two endpoints may be connected to the same core but at this time there would be no reason to do so.  Additional options may be defined in the future.\nEach endpoint is then available in the relationships definition in .platform.app.yaml.  For example, to allow an application to talk to both of the cores defined above its .platform.app.yaml file should contain the following:\nrelationships:\n    solrsearch1: 'search:main'\n    solrsearch2: 'search:extra'\n\nThat is, the application's environment would include a solr1 relationship that connects to the main endpoint, which is the mainindex core, and a solr2 relationship that connects to the extra endpoint, which is the extraindex core.\nThe relationships array would then look something like the following:\n{\n    \"solr1\": [\n        {\n            \"path\": \"solr/mainindex\",\n            \"host\": \"248.0.65.197\",\n            \"scheme\": \"solr\",\n            \"port\": 8080\n        }\n    ],\n    \"solr2\": [\n        {\n            \"path\": \"solr/extraindex\",\n            \"host\": \"248.0.65.197\",\n            \"scheme\": \"solr\",\n            \"port\": 8080\n        }\n    ]\n}\n\nConfigsets\nFor even more customizability, it's also possible to define Solr configsets.  For example, the following snippet would define one configset, which would be used by all cores.  Specific details can then be overriden by individual cores using core_properties, which is equivalent to the Solr core.properties file.\nsearch:\n    type: solr:8.4\n    disk: 1024\n    configuration:\n        configsets:\n            mainconfig: !archive \"configsets/solr8\"\n        cores:\n            english_index:\n                core_properties: |\n                    configSet=mainconfig\n                    schema=english/schema.xml\n            arabic_index:\n                core_properties: |\n                    configSet=mainconfig\n                    schema=arabic/schema.xml\n        endpoints:\n            english:\n                core: english_index\n            arabic:\n                core: arabic_index\n\nIn this example, the directory .platform/configsets/solr8 contains the configuration definition for multiple cores.  There are then two cores created: english_index uses the defined configset, but specifically the .platform/configsets/solr6/english/schema.xml file, while arabic_index is identical except for using the .platform/configsets/solr6/arabic/schema.xml file.  Each of those cores is then exposed as its own endpoint.\nNote that not all core.properties features make sense to specify in the core_properties. Some keys, such as name and dataDir, are not supported, and may result in a solrconfig that fails to work as intended, or at all.\nDefault configuration\nIf no configuration is specified, the default configuration is equivalent to:\nsearch:\n    type: solr:8.4\n    configuration:\n        cores:\n            collection1:\n                conf_dir: '{}'  # This will pick up the default Drupal 8 configuration\n        endpoints:\n            solr:\n                core: collection1\n\nThe default configuration is based on an older version of the Drupal 8 Search API Solr module that is no longer in use.  While it may work for generic cases defining your own custom configuration, core, and endpoint is strongly recommended.\nLimitations\nThe recommended maximum size for configuration directories (zipped) is 2MB. These need to be monitored to ensure they don't grow beyond that. If the zipped configuration directories grow beyond this, performance will decline and deploys will become longer. The directory archives will be compressed and string encoded. You could use this bash pipeline echo $(($(tar czf - . | base64 | wc -c )/(1024*1024))) Megabytes inside the directory to get an idea of the archive size.\nThe configuration directory is a collection of configuration data, like a data dictionary, e.g. small collections of key/value sets. The best way to keep the size small is to restrict the directory context to plain configurations. Including binary data like plugin .jar files will inflate the archive size, and is not recommended.\nAccessing the Solr server administrative interface\nBecause Solr uses HTTP for both its API and admin interface it's possible to access the admin interface over an SSH tunnel.\nplatform tunnel:open\nThat will open an SSH tunnel to all services on the current environment, and give an output similar to:\n\nSSH tunnel opened on port 30000 to relationship: solr\nSSH tunnel opened on port 30001 to relationship: database\nLogs are written to: /home/myuser/.platformsh/tunnels.log\n\nList tunnels with: platform tunnels\nView tunnel details with: platform tunnel:info\nClose tunnels with: platform tunnel:close\nIn this example, you can now open http://localhost:30000/solr/ in a browser to access the Solr admin interface.  Note that you cannot create indexes or users this way, but you can browse the existing indexes and manipulate the stored data.\n\nNote\nPlatform.sh Dedicated users can use ssh -L 8888:localhost:8983 &lt;user&gt;@&lt;cluster-name&gt;.ent.platform.sh to open a tunnel instead, after which the Solr server administrative interface will be available at http://localhost:8888/solr/.\n\nUpgrading\nThe Solr data format sometimes changes between versions in incompatible ways.  Solr does not include a data upgrade mechanism as it is expected that all indexes can be regenerated from stable data if needed.  To upgrade (or downgrade) Solr you will need to use a new service from scratch.\nThere are two ways of doing that.\nDestructive\nIn your services.yaml file, change the version of your Solr service and its name.  Then update the name in the .platform.app.yaml relationships block.\nWhen you push that to Platform.sh, the old service will be deleted and a new one with the name name created, with no data.  You can then have your application reindex data as appropriate.\nThis approach is simple but has the downside of temporarily having an empty Solr instance, which your application may or may not handle gracefully, and needing to rebuild your index afterward.  Depending on the size of your data that could take a while.\nTransitional\nFor a transitional approach you will temporarily have two Solr services.  Add a second Solr service with the new version a new name and give it a new relationship in .platform.app.yaml.  You can optionally run in that configuration for a while to allow your application to populate indexes in the new service as well.\nOnce you're ready to cut over, remove the old Solr service and relationship.  You may optionally have the new Solr service use the old relationship name if that's easier for your application to handle.  Your application is now using the new Solr service.\nThis approach has the benefit of never being without a working Solr instance.  On the downside, it requires two running Solr servers temporarily, each of which will consume resources and need adequate disk space.  Depending on the size of your data that may be a lot of disk space.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Services", "title": "Redis", "url": "/configuration/services/redis.html", "documentId": "1d0902b751c91c292da4597162156c4d332f17d3", "text": "\n                        \n                            \n                                \n                                \n                                Redis (Object cache)\nRedis is a high-performance in-memory object store, well-suited for application level caching.\nSee the Redis documentation for more information.\nPlatform.sh supports two different Redis configurations: One persistent (useful for key-value application data) and one ephemeral (in-memory only, useful for application caching).  Aside from that distinction they are identical.\nSupported versions\n\n3.2\n4.0\n5.0\n\nDeprecated versions\nThe following versions are available but are not receiving security updates from upstream, so their use is not recommended. They will be removed at some point in the future.\n\n2.8\n3.0\n\n\nnote\nVersions 3.0 and higher support up to 64 different databases per instance of the service, but Redis 2.8 is configured to support only a single database.\n\nEphemeral Redis\nThe redis service type is configured to serve as a LRU cache; its storage is not persistent.  It is not suitable for use except as a disposable cache.\nTo add an Ephemeral Redis service, specify it in your .platform/services.yaml file like so:\ncacheredis:\n    type: redis:5.0\nData in an Ephemeral Redis instance is stored only in memory, and thus requires no disk space.  When the service hits its memory limit it will automatically evict old cache items according to the configured eviction rule to make room for new ones.\nPersistent Redis\nThe redis-persistent service type is configured for persistent storage. That makes it a good choice for fast application-level key-value storage.\nTo add a Persistent Redis service, specify it in your .platform/services.yaml file like so:\ndata:\n    type: redis-persistent:5.0\n    disk: 256\nThe disk key is required for redis-persistent to tell Platform.sh how much disk space to reserve for Redis' persistent data.\nRelationship\nThe format exposed in the $PLATFORM_RELATIONSHIPS environment variable:\n{\n    \"username\": null,\n    \"scheme\": \"redis\",\n    \"service\": \"redis\",\n    \"fragment\": null,\n    \"ip\": \"169.254.0.86\",\n    \"hostname\": \"ftfs74bvoizcu4ua5eisskh7re.redis.service._.eu-3.platformsh.site\",\n    \"public\": false,\n    \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\",\n    \"host\": \"redis.internal\",\n    \"rel\": \"redis\",\n    \"query\": [],\n    \"path\": null,\n    \"password\": null,\n    \"type\": \"redis:5.0\",\n    \"port\": 6379,\n    \"host_mapped\": false\n}\nThe format is identical regardless of whether it's a persistent or ephemeral service.\nUsage example\nIn your .platform/services.yaml:\ncacheredis:\n    type: redis:5.0\nIf you are using PHP, configure a relationship and enable the PHP redis extension in your .platform.app.yaml.\nruntime:\n    extensions:\n        - redis\n\nrelationships:\n    rediscache: \"cache:redis\"\n\nYou can then use the service in a configuration file of your application with something like:\nJavaNode.jsPHPPythonpackage sh.platform.languages.sample;\n\nimport redis.clients.jedis.Jedis;\nimport redis.clients.jedis.JedisPool;\nimport sh.platform.config.Config;\nimport sh.platform.config.Redis;\n\nimport java.util.Set;\nimport java.util.function.Supplier;\n\npublic class RedisSample implements Supplier&lt;String&gt; {\n\n    @Override\n    public String get() {\n        StringBuilder logger = new StringBuilder();\n\n        // Create a new config object to ease reading the Platform.sh environment variables.\n        // You can alternatively use getenv() yourself.\n        Config config = new Config();\n\n        // The 'database' relationship is generally the name of primary database of an application.\n        // It could be anything, though, as in the case here here where it's called \"redis\".\n        Redis database = config.getCredential(\"redis\", Redis::new);\n        JedisPool dataSource = database.get();\n\n        // Get a Redis Client\n        final Jedis jedis = dataSource.getResource();\n\n        // Set a values\n        jedis.sadd(\"cities\", \"Salvador\");\n        jedis.sadd(\"cities\", \"London\");\n        jedis.sadd(\"cities\", \"S\u00e3o Paulo\");\n\n        // Read it back.\n        Set&lt;String&gt; cities = jedis.smembers(\"cities\");\n        logger.append(\"cities: \" + cities);\n        jedis.del(\"cities\");\n        return logger.toString();\n    }\n}\nconst redis = require('redis');\nconst config = require(\"platformsh-config\").config();\nconst { promisify } = require('util');\n\nexports.usageExample = async function() {\n\n    const credentials = config.credentials('redis');\n\n    var client = redis.createClient(credentials.port, credentials.host);\n\n    // The Redis client is not Promise-aware, so make it so.\n    const redisGet = promisify(client.get).bind(client);\n    const redisSet = promisify(client.set).bind(client);\n\n    let key = 'Deploy day';\n    let value = 'Friday';\n\n    // Set a value.\n    await redisSet(key, value);\n\n    // Read it back.\n    let test = await redisGet(key);\n\n    let output = `Found value &lt;strong&gt;${test}&lt;/strong&gt; for key &lt;strong&gt;${key}&lt;/strong&gt;.`;\n\n    return output;\n};\n&lt;?php\n\ndeclare(strict_types=1);\n\nuse Platformsh\\ConfigReader\\Config;\n\n// Create a new config object to ease reading the Platform.sh environment variables.\n// You can alternatively use getenv() yourself.\n$config = new Config();\n\n// Get the credentials to connect to the Redis service.\n$credentials = $config-&gt;credentials('redis');\n\ntry {\n    // Connecting to Redis server.\n    $redis = new Redis();\n    $redis-&gt;connect($credentials['host'], $credentials['port']);\n\n    $key = \"Deploy day\";\n    $value = \"Friday\";\n\n    // Set a value.\n    $redis-&gt;set($key, $value);\n\n    // Read it back.\n    $test = $redis-&gt;get($key);\n\n    printf('Found value &lt;strong&gt;%s&lt;/strong&gt; for key &lt;strong&gt;%s&lt;/strong&gt;.', $test, $key);\n\n} catch (Exception $e) {\n    print $e-&gt;getMessage();\n}\nfrom redis import Redis\nfrom platformshconfig import Config\n\n\ndef usage_example():\n\n    # Create a new config object to ease reading the Platform.sh environment variables.\n    # You can alternatively use os.environ yourself.\n    config = Config()\n\n    # Get the credentials to connect to the Redis service.\n    credentials = config.credentials('redis')\n\n    try:\n        redis = Redis(credentials['host'], credentials['port'])\n\n        key = \"Deploy day\"\n        value = \"Friday\"\n\n        # Set a value\n        redis.set(key, value)\n\n        # Read it back\n        test = redis.get(key)\n\n        return 'Found value &lt;strong&gt;{0}&lt;/strong&gt; for key &lt;strong&gt;{1}&lt;/strong&gt;.'.format(test.decode(\"utf-8\"), key)\n\n    except Exception as e:\n        return e\n\nMultiple databases\nRedis 3.0 and above are configured to support up to 64 databases.  Redis does not support distinct users for different databases so the same relationship connection gives access to all databases.  To use a particular database, use the Redis select command through your API library.  For instance, in PHP you could write:\n$redis-&gt;select(0);    // switch to DB 0\n$redis-&gt;set('x', '42');    // write 42 to x\n$redis-&gt;move('x', 1);    // move to DB 1\n$redis-&gt;select(1);    // switch to DB 1\n$redis-&gt;get('x');    // will return 42\n\nConsult the documentation for your connection library and Redis itself for further details.\nEviction policy\nOn the Ephemeral redis service it is also possible to select the key eviction policy.  That will control how Redis behaves when it runs out of memory for cached items and needs to clear old items to make room.\ncache:\n    type: redis:5.0\n    configuration:\n      maxmemory_policy: allkeys-lru\n\nThe default value if not specified is allkeys-lru, which will simply remove the oldest cache item first.  Legal values are:\n\nnoeviction\nallkeys-lru\nvolatile-lru\nallkeys-random\nvolatile-random\nvolatile-ttl\n\nSee the Redis documentation for a description of each option.\nUsing redis-cli to access your Redis service\nAssuming a Redis relationship named applicationcache defined in .platform.app.yaml\nrelationships:\n    rediscache: \"cacheredis:redis\"\nand services.yaml\ncacheredis:\n    type: redis:5.0\nThe host name and port number obtained from PLATFORM_RELATIONSHIPS would be applicationcache.internal and 6379. Open an SSH session and access the Redis server using the redis-cli tool as follows:\nredis-cli -h applicationcache.internal -p 6379\n\nUsing Redis as handler for native PHP sessions\nUsing the same configuration but with your Redis relationship named sessionstorage:\n.platform/services.yaml\ncacheredis:\n    type: redis:5.0\n.platform.app.yaml\nrelationships:\n  sessionstorage: \"cache:redis\"\n\n# .platform.app.yaml\nvariables:\n    php:\n        session.save_handler: redis\n        session.save_path: \"tcp://sessionstorage.internal:6379\"\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Services", "title": "RabbitMQ", "url": "/configuration/services/rabbitmq.html", "documentId": "dd0d522ca063fa538339346669d30c85f6adfe46", "text": "\n                        \n                            \n                                \n                                \n                                RabbitMQ (Message Queue service)\nRabbitMQ is an open source message broker software (sometimes called message-oriented middleware) that implements the Advanced Message Queuing Protocol (AMQP).\nSee the RabbitMQ documentation for more information.\nSupported versions\n\n3.5\n3.6\n3.7\n3.8\n\nRelationship\nThe format exposed in the $PLATFORM_RELATIONSHIPS environment variable:\n{\n    \"username\": \"guest\",\n    \"scheme\": \"amqp\",\n    \"service\": \"rabbitmq\",\n    \"ip\": \"169.254.178.95\",\n    \"hostname\": \"cefpddpigx4xs4alwihkji65fe.rabbitmq.service._.eu-3.platformsh.site\",\n    \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\",\n    \"host\": \"rabbitmq.internal\",\n    \"rel\": \"rabbitmq\",\n    \"password\": \"guest\",\n    \"type\": \"rabbitmq:3.7\",\n    \"port\": 5672\n}\nUsage example\nIn your .platform/services.yaml:\nqueuerabbit:\n    type: rabbitmq:3.8\n    disk: 256\nIn your .platform.app.yaml:\nrelationships:\n    rabbitmqqueue: \"queuerabbit:rabbitmq\"\nYou can then use the service in a configuration file of your application with something like:\nGoJavaPHPPythonpackage examples\n\nimport (\n\t\"fmt\"\n\tpsh \"github.com/platformsh/config-reader-go/v2\"\n\tamqpPsh \"github.com/platformsh/config-reader-go/v2/amqp\"\n\t\"github.com/streadway/amqp\"\n\t\"sync\"\n)\n\nfunc UsageExampleRabbitMQ() string {\n\n\t// Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables.\n\t// You can alternatively use os.Getenv() yourself.\n\tconfig, err := psh.NewRuntimeConfig()\n\tcheckErr(err)\n\n\t// Get the credentials to connect to RabbitMQ.\n\tcredentials, err := config.Credentials(\"rabbitmq\")\n\tcheckErr(err)\n\n\t// Use the amqp formatted credentials package.\n\tformatted, err := amqpPsh.FormattedCredentials(credentials)\n\tcheckErr(err)\n\n\t// Connect to the RabbitMQ server.\n\tconnection, err := amqp.Dial(formatted)\n\tcheckErr(err)\n\tdefer connection.Close()\n\n\t// Make a channel.\n\tchannel, err := connection.Channel()\n\tcheckErr(err)\n\tdefer channel.Close()\n\n\t// Create a queue.\n\tq, err := channel.QueueDeclare(\n\t\t\"deploy_days\", // name\n\t\tfalse,         // durable\n\t\tfalse,         // delete when unused\n\t\tfalse,         // exclusive\n\t\tfalse,         // no-wait\n\t\tnil,           // arguments\n\t)\n\n\tbody := \"Friday\"\n\tmsg := fmt.Sprintf(\"Deploying on %s\", body)\n\n\t// Publish a message.\n\terr = channel.Publish(\n\t\t\"\",     // exchange\n\t\tq.Name, // routing key\n\t\tfalse,  // mandatory\n\t\tfalse,  // immediate\n\t\tamqp.Publishing{\n\t\t\tContentType: \"text/plain\",\n\t\t\tBody:        []byte(msg),\n\t\t})\n\tcheckErr(err)\n\n\toutputMSG := fmt.Sprintf(\"[x] Sent '%s' &lt;br&gt;\", body)\n\n\t// Consume the message.\n\tmsgs, err := channel.Consume(\n\t\tq.Name, // queue\n\t\t\"\",     // consumer\n\t\ttrue,   // auto-ack\n\t\tfalse,  // exclusive\n\t\tfalse,  // no-local\n\t\tfalse,  // no-wait\n\t\tnil,    // args\n\t)\n\tcheckErr(err)\n\n\tvar received string\n\tvar wg sync.WaitGroup\n\twg.Add(1)\n\tgo func() {\n\t\tfor d := range msgs {\n\t\t\treceived = fmt.Sprintf(\"[x] Received message: '%s' &lt;br&gt;\", d.Body)\n\t\t\twg.Done()\n\t\t}\n\t}()\n\n\twg.Wait()\n\n\toutputMSG += received\n\n\treturn outputMSG\n}\npackage sh.platform.languages.sample;\n\nimport sh.platform.config.Config;\nimport sh.platform.config.RabbitMQ;\n\nimport javax.jms.Connection;\nimport javax.jms.ConnectionFactory;\nimport javax.jms.MessageConsumer;\nimport javax.jms.MessageProducer;\nimport javax.jms.Queue;\nimport javax.jms.Session;\nimport javax.jms.TextMessage;\nimport java.util.function.Supplier;\n\npublic class RabbitMQSample implements Supplier&lt;String&gt; {\n\n    @Override\n    public String get() {\n        StringBuilder logger = new StringBuilder();\n\n        // Create a new config object to ease reading the Platform.sh environment variables.\n        // You can alternatively use getenv() yourself.\n        Config config = new Config();\n        try {\n            // Get the credentials to connect to the RabbitMQ service.\n            final RabbitMQ credential = config.getCredential(\"rabbitmq\", RabbitMQ::new);\n            final ConnectionFactory connectionFactory = credential.get();\n\n            // Connect to the RabbitMQ server.\n            final Connection connection = connectionFactory.createConnection();\n            connection.start();\n            final Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);\n            Queue queue = session.createQueue(\"cloud\");\n            MessageConsumer consumer = session.createConsumer(queue);\n\n            // Sending a message into the queue.\n            TextMessage textMessage = session.createTextMessage(\"Platform.sh\");\n            textMessage.setJMSReplyTo(queue);\n            MessageProducer producer = session.createProducer(queue);\n            producer.send(textMessage);\n\n            // Receive the message.\n            TextMessage replyMsg = (TextMessage) consumer.receive(100);\n\n            logger.append(\"Message: \").append(replyMsg.getText());\n\n            // close connections.\n            producer.close();\n            consumer.close();\n            session.close();\n            connection.close();\n            return logger.toString();\n        } catch (Exception exp) {\n            throw new RuntimeException(\"An error when execute RabbitMQ\", exp);\n        }\n    }\n}\n&lt;?php\n\ndeclare(strict_types=1);\n\nuse Platformsh\\ConfigReader\\Config;\nuse PhpAmqpLib\\Connection\\AMQPStreamConnection;\nuse PhpAmqpLib\\Message\\AMQPMessage;\n\n// Create a new config object to ease reading the Platform.sh environment variables.\n// You can alternatively use getenv() yourself.\n$config = new Config();\n\n// Get the credentials to connect to the RabbitMQ service.\n$credentials = $config-&gt;credentials('rabbitmq');\n\ntry {\n\n    $queueName = 'deploy_days';\n\n    // Connect to the RabbitMQ server.\n    $connection = new AMQPStreamConnection($credentials['host'], $credentials['port'], $credentials['username'], $credentials['password']);\n    $channel = $connection-&gt;channel();\n\n    $channel-&gt;queue_declare($queueName, false, false, false, false);\n\n    $msg = new AMQPMessage('Friday');\n    $channel-&gt;basic_publish($msg, '', 'hello');\n\n    echo \"[x] Sent 'Friday'&lt;br/&gt;\\n\";\n\n    // In a real application you't put the following in a separate script in a loop.\n    $callback = function ($msg) {\n        printf(\"[x] Deploying on %s&lt;br /&gt;\\n\", $msg-&gt;body);\n    };\n\n    $channel-&gt;basic_consume($queueName, '', false, true, false, false, $callback);\n\n    // This blocks on waiting for an item from the queue, so comment it out in this demo script.\n    //$channel-&gt;wait();\n\n    $channel-&gt;close();\n    $connection-&gt;close();\n\n} catch (Exception $e) {\n    print $e-&gt;getMessage();\n}\n\nimport pika\nfrom platformshconfig import Config\n\n\ndef usage_example():\n    # Create a new Config object to ease reading the Platform.sh environment variables.\n    # You can alternatively use os.environ yourself.\n    config = Config()\n\n    # Get the credentials to connect to the RabbitMQ service.\n    credentials = config.credentials('rabbitmq')\n\n    try:\n        # Connect to the RabbitMQ server\n        creds = pika.PlainCredentials(credentials['username'], credentials['password'])\n        parameters = pika.ConnectionParameters(credentials['host'], credentials['port'], credentials=creds)\n\n        connection = pika.BlockingConnection(parameters)\n        channel = connection.channel()\n\n        # Check to make sure that the recipient queue exists\n        channel.queue_declare(queue='deploy_days')\n\n        # Try sending a message over the channel\n        channel.basic_publish(exchange='',\n                              routing_key='deploy_days',\n                              body='Friday!')\n\n        # Receive the message\n        def callback(ch, method, properties, body):\n            print(\" [x] Received {}\".format(body))\n\n        # Tell RabbitMQ that this particular function should receive messages from our 'hello' queue\n        channel.basic_consume('deploy_days',\n                              callback,\n                              auto_ack=False)\n\n        # This blocks on waiting for an item from the queue, so comment it out in this demo script.\n        # print(' [*] Waiting for messages. To exit press CTRL+C')\n        # channel.start_consuming()\n\n        connection.close()\n\n        return \" [x] Sent 'Friday!'&lt;br/&gt;\"\n\n    except Exception as e:\n        return e\n\n(The specific way to inject configuration into your application will vary. Consult your application or framework's documentation.)\nConnecting to RabbitMQ\nFrom your local development environment\nFor debugging purposes, it's sometimes useful to be able to directly connect to a service instance. You can do this using SSH tunneling. To open a tunnel, log into your application container like usual, but with an extra flag to enable local port forwarding:\nssh -L 5672:mq.internal:5672 &lt;projectid&gt;-&lt;branch_ID&gt;@ssh.eu.platform.sh\n\nWithin that SSH session, use the following command to pretty-print your relationships. This lets you see which username and password to use, and you can double check that the remote service's port is 5672.\nphp -r 'print_r(json_decode(base64_decode($_ENV[\"PLATFORM_RELATIONSHIPS\"])));'\n\nIf your service is running on a different port, you can re-open your SSH session with the correct port by modifying your -L flag: -L 5672:mq.internal:&lt;remote port&gt;.\nFinally, while the session is open, you can launch a RabbitMQ client of your choice from your local workstation, configured to connect to localhost:5672 using the username and password you found in the relationship variable.\nAccess the management plugin  (Web UI)\nIn case you want to access the browser-based UI, you have to use an SSH tunnel. To open a tunnel, log into your application container like usual, but with an extra flag to enable local port forwarding:\nssh -L 15672:mq.internal:15672 &lt;projectid&gt;-&lt;branch_ID&gt;@ssh.eu.platform.sh\n\nAfter you successfully established a connection, you should be able to open http://localhost:15672 in your browser. You'll find the credentials like mentioned above.\nFrom the application container\nThe application container currently doesn't include any useful utilities to connect to RabbitMQ with. However, you can install your own by adding a client as a dependency in your .platform.app.yaml file.\nFor example, you can use amqp-utils by adding this:\ndependencies:\n  ruby:\n    amqp-utils: \"0.5.1\"\n\nThen, when you SSH into your container, you can simply type any amqp- command available to manage your queues.\nConfiguration\nVirtual hosts\nYou can configure additional virtual hosts to a RabbitMQ service, which can be useful for separating resources, such as exchanges, queues, and bindings, to their own namespace. In your .platform/services.yaml file define the names of the virtual hosts under the configuration.vhosts attribute:\nrabbitmq:\n  type: rabbitmq:3.8\n  disk: 512\n  configuration:\n    vhosts:\n      - foo\n      - bar\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Services", "title": "Network Storage", "url": "/configuration/services/network-storage.html", "documentId": "b855619b3186057ac774846468345e3accdc79d7", "text": "\n                        \n                            \n                                \n                                \n                                Network Storage\nPlatform.sh supports internal \"storage as a service\" to provide a file store that can be shared between different application containers.\nThe network storage service enables a new kind of mount that refers to a shared service rather than to a local directory.  Any application can use both local and/or service mounts, or neither.\nSupported versions\n\n1.0\n\n(This is a reference to a version of our network storage implementation, not to a version of a 3rd party application.)\nSupported regions\nThe Network storage service is available on all regions except:\n\neu.platform.sh\nus.platform.sh\n\nIf you are on one of those and require the service we suggest you migrate your project to one of the newer regions (such as eu-2, us-2, ca, au, fr-1 or de-2).\nDefine the service\nFirst, declare a new service in the services.yaml file like so:\nfiles:\n    type: network-storage:1.0\n    disk: 256\nThis example creates a service named files that is of type network-storage, and gives it 256 MB of storage total.\nDeclare the mount\nSecond, add the following entry to your mounts list:\nmounts:\n    'my/files':\n        source: service\n        service: files\n        source_path: files\nThis block will declare a writeable mount on the application container at the path my/files, which will be provided by the files service defined above.  The source_path specifies the path within the network service that the mount points to.  It is often easiest to have it match the name of the mount point itself but that is not required.\nNote that you do not need to add a relationship to point to the files service.  That is handled automatically by the system.\nThe application container can now read from and write to the my/files path just as if it were a local writeable mount.\n\nnote\nThere is a small performance hit for using a network mount over a local mount.  In most cases it should not be noticeable.  However, high-volume sequential file creation (that is, creating a large number of small files in rapid succession) may see a more significant performance hit.  If that is something your application does regularly then a local mount will be more effective.\n\nMulti-application usage\nIf your project contains more than one application (that is, multiple directories with their own .platform.app.yaml files), they can all use the same network mounts if desired.  If the source_path is the same in both .platform.app.yaml files then the files will be shared between both applications, even if the mount location is different.\nIt is also possible to have one application mount a source_path that is a subdirectory of another application's mount.  For example:\napp1:\nmounts:\n    'my/files':\n        source: service\n        service: files\n        source_path: files\napp2:\nmounts:\n    'process':\n        source: service\n        service: files\n        source_path: uploads/incoming\n    'done':\n        source: service\n        service: files\n        source_path: uploads/done\n\nIn this example, app1 will have access to the entire uploads directory by writing to web/uploads.  app2, by contrast, will have two mounts that it can write to: process and done.  The process mount will refer to the same directory as the web/uploads/incoming directory does on app1, and the done mount will refer to the same directory as the web/uploads/done directory on app1.\nWorker instances\nWhen defining a Worker instance it is important to keep in mind what mount behavior is desired.  Unless the mounts block is defined within the web and workers sections separately, a top level mounts block will apply to both instances.  However, local mounts will be a separate storage area for each instance while service mounts will refer to the same file system.  For example:\nname: app\n\ntype: php:7.2\n\ndisk: 1024\n\nmounts:\n    'network_dir':\n        source: service\n        service: files\n        source_path: our_stuff\n\n    'local_dir':\n        source: local\n        source_path: my_stuff\n\nweb:\n    locations:\n        \"/\":\n            root: \"public\"\n            passthru: \"/index.php\"\n\nworkers:\n    queue:\n        commands:\n            start: |\n                php worker.php\n\nIn this case, both the web instance and the queue worker will have two mount points: network_dir and local_dir.  \n\nThe local_dir mount on each will be independent and not connected to each other at all, and they will each take 1024 MB of space.\nThe network_dir mount on each will point to the same network storage space on the files service.  They will both be able to read and write to it simultaneously.  The amount of space it has available will depend on the disk key specified in services.yaml.\n\nHow do I give my workers access to my main application's files?\nThe most common use case for network-storage is to allow a CMS-driven site to use a worker that has access to the same file mounts as the web-serving application.  For that case, all that is needed is to set the necessary file mounts as service mounts.\nFor example, the following .platform.app.yaml file (fragment) will keep Drupal files directories shared between web and worker instances while keeping the Drush backup directory web-only (as it has no need to be shared).  (This assumes a service named files has already been defined in services.yaml.)\nname: 'app'\ntype: 'php:7.2'\n\nrelationships:\n    database: 'db:mysql'\n\nhooks:\n  # ...\n\nweb:\n    locations:\n        '/':\n            # ...\n\ndisk: 1024\n\nmounts:\n    # The public and private files directories are\n    # network mounts shared by web and workers.\n    'web/sites/default/files':\n        source: service\n        service: files\n        source_path: files\n    'private':\n        source: service\n        service: files\n        source_path: private\n    # The backup, temp, and cache directories for\n    # Drupal's CLI tools don't need to be shared.\n    # It wouldn't hurt anything to make them network\n    # shares, however.\n    '/.drush':\n        source: local\n        source_path: drush\n    'tmp':\n        source: local\n        source_path: tmp\n    'drush-backups':\n        source: local\n        source_path: drush-backups\n    '/.console':\n        source: local\n        source_path: console\n\n# Crons run on the web container, so they have the\n# same mounts as the web container.\ncrons:\n    drupal:\n        spec: '*/20 * * * *'\n        cmd: 'cd web ; drush core-cron'\n\n# The worker defined here will also have the same 6 mounts;\n# 2 of them will be shared with the web container,\n# the other 4 will be local to the worker.\nworkers:\n    queue:\n        commands:\n            start: |\n                cd web &amp;&amp; drush queue-run myqueue\n\nHow can I migrate a local storage to a network storage?\nThere is no automated way of transferring data from one storage type to another.  However, the process is fundamentally \"just\" moving files around on disk, so it is reasonably straightforward.\nSuppose you have this mount configuration:\nmounts:\n    web/uploads:\n        source: local\n        source_path: uploads\n\nAnd want to move that to a network storage mount.  The following approximate steps will do so with a minimum of service interruption.\n1) Add a new network-storage service, named files, that has at least enough space for your existing files with some buffer.  You may need to increase your plan's disk size to accommodate it.\n2) Add a new mount to the network storage service on a non-public directory:\nmounts:\n    new-uploads:\n        source: service\n        service: files\n        source_path: uploads\n\n(Remember the source_path can be the same since they're on different storage services.)\n3) Deploy these changes.  Then use rsync to copy all files from the local mount to the network mount.  (Be careful of the trailing /.)\nrsync -avz web/uploads/* new-uploads/\n4) Reverse the mounts.  That is, point the web/uploads directory to the network mount instead:\nmounts:\n    web/uploads:\n        source: service\n        service: files\n        source_path: uploads\n    old-uploads:\n        source: local\n        source_path: uploads\n\nCommit and push that.  Test to make sure the network files are accessible.\n5) Cleanup.  First, run another rsync just to make sure any files uploaded during the transition are not lost.  (Note the command is different here.)\nrsync -avz old-uploads/* web/uploads/\nOnce you're confident all the files are accounted for, delete the entire contents of old-uploads.  If you do not, the files will remain on disk but inaccessible, just eating up disk space needlessly.\nOnce that's done you can remove the old-uploads mount and push again to finish the process.  You are also free to reduce the disk size in the .platform.app.yaml file if desired, but make sure to leave enough for any remaining local mounts.\nWhy do I get an invalid service type error with network storage?\nThe network-storage service is only available on our newer regions.  If you are running on the older us or eu regions and try to create a network-storage service you will receive this error.\nTo make use of network-storage you will need to migrate to the newer us-2 or eu-2 regions.  See our tutorial on how to migrate regions for more information.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Services", "title": "MySQL/MariaDB", "url": "/configuration/services/mysql.html", "documentId": "62a859b6ae1f2da76cf6dae83a3032a6e37f0ccf", "text": "\n                        \n                            \n                                \n                                \n                                MariaDB/MySQL (Database service)\nPlatform.sh supports both MariaDB and Oracle MySQL.  While there are some differences at the application level for developers, they function nearly identically from an infrastructure point of view.\nSee the MariaDB documentation or MySQL documentation for more information.\nSupported versions\nThe service types mariadb and mysql both refer to MariaDB for compatibility reasons. The service type oracle-mysql refers to MySQL as released by Oracle, Inc. Other than the type, MySQL and MariaDB are otherwise identical and the rest of this page refers to both equally.\n\nmariadb:10.0\nmariadb:10.1\nmariadb:10.2\nmariadb:10.3\nmariadb:10.4\n\n\nmysql:10.0\nmysql:10.1\nmysql:10.2\n\n\noracle-mysql:5.7\noracle-mysql:8.0\n\n\nnote\nDowngrades of MySQL or MariaDB are not supported. Both will update their own datafiles to a new version automatically but cannot downgrade them. If you want to experiment with a later version without committing to it use a non-master environment.\n\nDeprecated versions\nThe following versions are available but are not receiving security updates from upstream, so their use is not recommended. They will be removed at some point in the future.\n\nmysql:5.5\n\nRelationship\nThe format exposed in the $PLATFORM_RELATIONSHIPS environment variable:\n{\n    \"username\": \"user\",\n    \"scheme\": \"mysql\",\n    \"service\": \"mysql\",\n    \"fragment\": null,\n    \"ip\": \"169.254.4.210\",\n    \"hostname\": \"nha5q7m5ik526umqw6cwrcvpvi.mysql.service._.eu-3.platformsh.site\",\n    \"public\": false,\n    \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\",\n    \"host\": \"mysql.internal\",\n    \"rel\": \"mysql\",\n    \"query\": {\n        \"is_master\": true\n    },\n    \"path\": \"main\",\n    \"password\": \"\",\n    \"type\": \"mariadb:10.4\",\n    \"port\": 3306,\n    \"host_mapped\": false\n}\nUsage example\nFor MariaDB your .platform/services.yaml can use the mysql service type:\ndb:\n    type: mysql:10.4\n    disk: 256\nor the mariadb service type.\ndb:\n    type: mariadb:10.4\n    disk: 256\nOracle-mysql uses the oracle-mysql service type:\ndbmysql:\n    type: oracle-mysql:8.0\n    disk: 256\nNote that the minimum disk size for mysql/oracle-mysql is 256MB.\nDespite these service type differences, MariaDB and Oracle MySQL both use the mysql endpoint in their configuration.\nFor MariaDB, the endpoint does not change whether you used the mysql or mariadb service type:\nrelationships:\n    database: \"db:mysql\"\nThe same goes for using the oracle-mysql service type as well.\nrelationships:\n    mysqldatabase: \"dbmysql:mysql\"\nYou can then use the service in a configuration file of your application with something like:\nGoJavaNode.jsPHPPythonpackage examples\n\nimport (\n\t\"database/sql\"\n\t\"fmt\"\n\t_ \"github.com/go-sql-driver/mysql\"\n\tpsh \"github.com/platformsh/config-reader-go/v2\"\n\tsqldsn \"github.com/platformsh/config-reader-go/v2/sqldsn\"\n)\n\nfunc UsageExampleMySQL() string {\n\n\t// Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables.\n\t// You can alternatively use os.Getenv() yourself.\n\tconfig, err := psh.NewRuntimeConfig()\n\tcheckErr(err)\n\n\t// The 'database' relationship is generally the name of the primary SQL database of an application.\n\t// That's not required, but much of our default automation code assumes it.\n\tcredentials, err := config.Credentials(\"database\")\n\tcheckErr(err)\n\n\t// Using the sqldsn formatted credentials package.\n\tformatted, err := sqldsn.FormattedCredentials(credentials)\n\tcheckErr(err)\n\n\tdb, err := sql.Open(\"mysql\", formatted)\n\tcheckErr(err)\n\n\tdefer db.Close()\n\n\t// Force MySQL into modern mode.\n\tdb.Exec(\"SET NAMES=utf8\")\n\tdb.Exec(\\x60SET sql_mode = 'ANSI,STRICT_TRANS_TABLES,STRICT_ALL_TABLES,\n    NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,\n    NO_AUTO_CREATE_USER,ONLY_FULL_GROUP_BY'\\x60)\n\n\t// Creating a table.\n\tsqlCreate := \\x60\nCREATE TABLE IF NOT EXISTS PeopleGo (\nid SERIAL PRIMARY KEY,\nname VARCHAR(30) NOT NULL,\ncity VARCHAR(30) NOT NULL)\\x60\n\n\t_, err = db.Exec(sqlCreate)\n\tcheckErr(err)\n\n\t// Insert data.\n\tsqlInsert := \\x60\nINSERT INTO PeopleGo (name, city) VALUES\n('Neil Armstrong', 'Moon'),\n('Buzz Aldrin', 'Glen Ridge'),\n('Sally Ride', 'La Jolla');\\x60\n\n\t_, err = db.Exec(sqlInsert)\n\tcheckErr(err)\n\n\ttable := \\x60&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;City&lt;/th&gt;&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\\x60\n\n\tvar id int\n\tvar name string\n\tvar city string\n\n\trows, err := db.Query(\"SELECT * FROM PeopleGo\")\n\tif err != nil {\n\t\tpanic(err)\n\t} else {\n\t\tfor rows.Next() {\n\t\t\terr = rows.Scan(&amp;id, &amp;name, &amp;city)\n\t\t\tcheckErr(err)\n\t\t\ttable += fmt.Sprintf(\"&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td&gt;%s&lt;/td&gt;&lt;tr&gt;\\n\", name, city)\n\t\t}\n\t\ttable += \"&lt;/tbody&gt;\\n&lt;/table&gt;\\n\"\n\t}\n\n\t_, err = db.Exec(\"DROP TABLE PeopleGo;\")\n\tcheckErr(err)\n\n\treturn table\n}\npackage sh.platform.languages.sample;\n\nimport sh.platform.config.Config;\nimport sh.platform.config.MySQL;\n\nimport javax.sql.DataSource;\nimport java.sql.Connection;\nimport java.sql.ResultSet;\nimport java.sql.SQLException;\nimport java.sql.Statement;\nimport java.util.function.Supplier;\n\npublic class MySQLSample implements Supplier&lt;String&gt; {\n\n    @Override\n    public String get() {\n        StringBuilder logger = new StringBuilder();\n\n        // Create a new config object to ease reading the Platform.sh environment variables.\n        // You can alternatively use getenv() yourself.\n        Config config = new Config();\n\n        // The 'database' relationship is generally the name of primary SQL database of an application.\n        // That's not required, but much of our default automation code assumes it.\n        MySQL database = config.getCredential(\"database\", MySQL::new);\n        DataSource dataSource = database.get();\n\n        // Connect to the database\n        try (Connection connection = dataSource.getConnection()) {\n\n            // Creating a table.\n            String sql = \"CREATE TABLE JAVA_PEOPLE (\" +\n                    \" id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY,\" +\n                    \"name VARCHAR(30) NOT NULL,\" +\n                    \"city VARCHAR(30) NOT NULL)\";\n\n            final Statement statement = connection.createStatement();\n            statement.execute(sql);\n\n            // Insert data.\n            sql = \"INSERT INTO JAVA_PEOPLE (name, city) VALUES\" +\n                    \"('Neil Armstrong', 'Moon'),\" +\n                    \"('Buzz Aldrin', 'Glen Ridge'),\" +\n                    \"('Sally Ride', 'La Jolla')\";\n\n            statement.execute(sql);\n\n            // Show table.\n            sql = \"SELECT * FROM JAVA_PEOPLE\";\n            final ResultSet resultSet = statement.executeQuery(sql);\n            while (resultSet.next()) {\n                int id = resultSet.getInt(\"id\");\n                String name = resultSet.getString(\"name\");\n                String city = resultSet.getString(\"city\");\n                logger.append(String.format(\"the JAVA_PEOPLE id %d the name %s and city %s\", id, name, city));\n                logger.append('\\n');\n            }\n            statement.execute(\"DROP TABLE JAVA_PEOPLE\");\n            return logger.toString();\n        } catch (SQLException exp) {\n            throw new RuntimeException(\"An error when execute MySQL\", exp);\n        }\n    }\n}const mysql = require('mysql2/promise');\nconst config = require(\"platformsh-config\").config();\n\nexports.usageExample = async function() {\n\n    const credentials = config.credentials('database');\n\n    const connection = await mysql.createConnection({\n        host: credentials.host,\n        port: credentials.port,\n        user: credentials.username,\n        password: credentials.password,\n        database: credentials.path,\n    });\n\n    let sql = '';\n\n    // Creating a table.\n    sql = `CREATE TABLE IF NOT EXISTS People (\n        id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n            name VARCHAR(30) NOT NULL,\n            city VARCHAR(30) NOT NULL\n        )`;\n    await connection.query(sql);\n\n    // Insert data.\n    sql = `INSERT INTO People (name, city) VALUES\n    ('Neil Armstrong', 'Moon'),\n        ('Buzz Aldrin', 'Glen Ridge'),\n        ('Sally Ride', 'La Jolla');`;\n    await connection.query(sql);\n\n    // Show table.\n    sql = `SELECT * FROM People`;\n    let [rows] = await connection.query(sql);\n\n    let output = '';\n\n    if (rows.length &gt; 0) {\n        output +=`&lt;table&gt;\n            &lt;thead&gt;\n            &lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;City&lt;/th&gt;&lt;/tr&gt;\n            &lt;/thead&gt;\n            &lt;tbody&gt;`;\n\n        rows.forEach((row) =&gt; {\n            output += `&lt;tr&gt;&lt;td&gt;${row.name}&lt;/td&gt;&lt;td&gt;${row.city}&lt;/td&gt;&lt;/tr&gt;\\n`;\n        });\n\n        output += `&lt;/tbody&gt;\\n&lt;/table&gt;\\n`;\n    }\n\n    // Drop table.\n    sql = `DROP TABLE People`;\n    await connection.query(sql);\n\n    return output;\n};\n&lt;?php\n\ndeclare(strict_types=1);\n\nuse Platformsh\\ConfigReader\\Config;\n\n// Create a new config object to ease reading the Platform.sh environment variables.\n// You can alternatively use getenv() yourself.\n$config = new Config();\n\n// The 'database' relationship is generally the name of primary SQL database of an application.\n// That's not required, but much of our default automation code assumes it.\n$credentials = $config-&gt;credentials('database');\n\ntry {\n    // Connect to the database using PDO.  If using some other abstraction layer you would\n    // inject the values from $database into whatever your abstraction layer asks for.\n    $dsn = sprintf('mysql:host=%s;port=%d;dbname=%s', $credentials['host'], $credentials['port'], $credentials['path']);\n    $conn = new \\PDO($dsn, $credentials['username'], $credentials['password'], [\n        // Always use Exception error mode with PDO, as it's more reliable.\n        \\PDO::ATTR_ERRMODE =&gt; \\PDO::ERRMODE_EXCEPTION,\n        // So we don't have to mess around with cursors and unbuffered queries by default.\n        \\PDO::MYSQL_ATTR_USE_BUFFERED_QUERY =&gt; TRUE,\n        // Make sure MySQL returns all matched rows on update queries including\n        // rows that actually didn't have to be updated because the values didn't\n        // change. This matches common behavior among other database systems.\n        \\PDO::MYSQL_ATTR_FOUND_ROWS =&gt; TRUE,\n    ]);\n\n    // Creating a table.\n    $sql = \"CREATE TABLE People (\n      id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n      name VARCHAR(30) NOT NULL,\n      city VARCHAR(30) NOT NULL\n      )\";\n    $conn-&gt;query($sql);\n\n    // Insert data.\n    $sql = \"INSERT INTO People (name, city) VALUES \n        ('Neil Armstrong', 'Moon'), \n        ('Buzz Aldrin', 'Glen Ridge'), \n        ('Sally Ride', 'La Jolla');\";\n    $conn-&gt;query($sql);\n\n    // Show table.\n    $sql = \"SELECT * FROM People\";\n    $result = $conn-&gt;query($sql);\n    $result-&gt;setFetchMode(\\PDO::FETCH_OBJ);\n\n    if ($result) {\n        print &lt;&lt;&lt;TABLE\n&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;City&lt;/th&gt;&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\nTABLE;\n        foreach ($result as $record) {\n            printf(\"&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;\\n\", $record-&gt;name, $record-&gt;city);\n        }\n        print \"&lt;/tbody&gt;\\n&lt;/table&gt;\\n\";\n    }\n\n    // Drop table\n    $sql = \"DROP TABLE People\";\n    $conn-&gt;query($sql);\n\n} catch (\\Exception $e) {\n    print $e-&gt;getMessage();\n}\nimport pymysql\nfrom platformshconfig import Config\n\n\ndef usage_example():\n\n    # Create a new Config object to ease reading the Platform.sh environment variables.\n    # You can alternatively use os.environ yourself.\n    config = Config()\n\n    # The 'database' relationship is generally the name of primary SQL database of an application.\n    # That's not required, but much of our default automation code assumes it.'\n    credentials = config.credentials('database')\n\n    try:\n        # Connect to the database using PDO. If using some other abstraction layer you would inject the values\n        # from `database` into whatever your abstraction layer asks for.\n\n        conn = pymysql.connect(host=credentials['host'],\n                               port=credentials['port'],\n                               database=credentials['path'],\n                               user=credentials['username'],\n                               password=credentials['password'])\n\n        sql = '''\n                CREATE TABLE People (\n                id SERIAL PRIMARY KEY,\n                name VARCHAR(30) NOT NULL,\n                city VARCHAR(30) NOT NULL\n                )\n                '''\n\n        cur = conn.cursor()\n        cur.execute(sql)\n\n        sql = '''\n                INSERT INTO People (name, city) VALUES\n                ('Neil Armstrong', 'Moon'),\n                ('Buzz Aldrin', 'Glen Ridge'),\n                ('Sally Ride', 'La Jolla');\n                '''\n\n        cur.execute(sql)\n\n        # Show table.\n        sql = '''SELECT * FROM People'''\n        cur.execute(sql)\n        result = cur.fetchall()\n\n        table = '''&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;City&lt;/th&gt;&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;'''\n\n        if result:\n            for record in result:\n                table += '''&lt;tr&gt;&lt;td&gt;{0}&lt;/td&gt;&lt;td&gt;{1}&lt;/td&gt;&lt;tr&gt;\\n'''.format(record[1], record[2])\n            table += '''&lt;/tbody&gt;\\n&lt;/table&gt;\\n'''\n\n        # Drop table\n        sql = '''DROP TABLE People'''\n        cur.execute(sql)\n\n        # Close communication with the database\n        cur.close()\n        conn.close()\n\n        return table\n\n    except Exception as e:\n        return e\n\n\nnote\nMySQL schema names can not use system reserved namespace. (mysql, information_schema, etc)\n\nMultiple databases\nIf you are using version 10.0 or later of this service it is possible to define multiple databases as well as multiple users with different permissions.  To do so requires defining multiple endpoints.  Under the configuration key of your service there are two additional keys:\n\nschemas:  This is a YAML array listing the databases that should be created.  If not specified, a single database named main will be created.\nendpoints: This is a nested YAML array defining different credentials.  Each endpoint may have access to one or more schemas (databases), and may have different levels of permission on each.  The valid permission levels are:\nro: Using this endpoint only SELECT queries are allowed.\nrw: Using this endpoint SELECT queries as well INSERT/UPDATE/DELETE queries are allowed.\nadmin: Using this endpoint all queries are allowed, including DDL queries (CREATE TABLE, DROP TABLE, etc.).\n\n\n\nConsider the following illustrative example:\ndb:\n    type: mariadb:10.4\n    disk: 2048\n    configuration:\n        schemas:\n            - main\n            - legacy\n        endpoints:\n            admin:\n                default_schema: main\n                privileges:\n                    main: admin\n                    legacy: admin\n            reporter:\n                privileges:\n                    main: ro\n            importer:\n                default_schema: legacy\n                privileges:\n                    legacy: rw\n\nThis example creates a single MySQL/MariaDB service named mysqldb.  That server will have two databases, main and legacy.  There will be three endpoints created.  The first, named admin, will have full access to both databases.  The second, reporter, will have SELECT query access to the main DB but no access to legacy at all.  The importer user will have SELECT/INSERT/UPDATE/DELETE access (but not DDL access) to the legacy database but no access to main.\nIf a given endpoint has access to multiple databases you should also specify which will be listed by default in the relationships array.  If one isn't specified the path property of the relationship will be null.  While that may be acceptable for an application that knows the name of the database to connect to, it would mean that automated tools such as the Platform CLI will not be able to access the database on that relationship. For that reason the default_schema property is always recommended.\nOnce those endpoints are defined, you need to expose them to your application as a relationship.  Continuing with our example, this would be a possible corresponding block from .platform.app.yaml:\nrelationships:\n    database: \"db:admin\"\n    reports: \"db:reporter\"\n    imports: \"db:importer\"\n\nThis block defines three relationships, database, reports, and imports.  They'll be available in the PLATFORM_RELATIONSHIPS environment variable and all have the same structure documented above, but with different credentials.  You can use those to connect to the appropriate database with the specified restrictions using whatever the SQL access tools are for your language and application.\nIf no configuration block is specified at all, it is equivalent to the following default:\nconfiguration:\n    schemas:\n        - main\n    endpoints:\n        mysql:\n          default_schema: main\n          privileges:\n            main: admin\n\nIf either schemas or endpoints are defined, then no default will be applied and you must specify the full configuration.\nAdjusting database configuration\nFor MariaDB 10.1 and later Oracle MySQL 8.0 and later, a select few configuration properties from the my.cnf file are available for adjustment.\nPacket and connection sizing\nThis value defaults to 16 (in MB).  Legal values are from 1 to 100.\ndb:\n    type: mariadb:10.4\n    disk: 2048\n    configuration:\n        properties:\n            max_allowed_packet: 64\n\nThe above code will increase the maximum allowed packet size (the size of a query or response) to 64 MB.  However, increasing the size of the maximum packet will also automatically decrease the max_connections value.  The number of connections allowed will depend on the packet size and the memory available to the service.  In most cases leaving this value at the default is recommended.\nCharacter encoding\nFor services created prior to February 2020, the default character set and collation is latin1, which is the default in most MySQL/MariaDB.\nFor services created after February 2020, the default character set is utf8mb4 and the default collation is utf8mb4_unicode_ci.\nBoth values can be adjusted at the server level in services.yaml:\ndb:\n  type: mariadb:10.4\n  disk: 2048\n  configuration:\n    properties:\n      default_charset: utf8mb4\n      default_collation: utf8mb4_unicode_ci\n\nNote that the effect of this setting is to set the character set and collation of any tables created once those properties are set.  Tables created prior to when those settings are changed will be unaffected by changes to the services.yaml configuration.  However, you can change your own table's character set and collation through ALTER TABLE commands.  For example:\n# To change defaults when creating new tables:\nALTER DATABASE main CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\n\n# To change defaults when creating new columns:\nALTER TABLE table_name CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\n\n# To convert existing data:\nALTER TABLE table_name CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\nConsult the MySQL documentation for further details.\nAccess your MariaDB service\nAssuming your MariaDB relationship is named database, the host name and port number obtained from PLATFORM_RELATIONSHIPS would be database.internal and 3306. Open an SSH session and run the MySQL command line client.\nmysql -h database.internal -P 3306 -u user main\n\nOutside the application container, you can use Platform CLI platform sql.\nExporting data\nThe easiest way to download all data in a MariaDB instance is with the Platform.sh CLI.  If you have a single SQL database, the following command will export all data using the mysqldump command to a local file:\nplatform db:dump\n\nIf you have multiple SQL databases it will prompt you which one to export. You can also specify one by relationship name explicitly:\nplatform db:dump --relationship database\n\nBy default the file will be uncompressed. If you want to compress it, use the --gzip (-z) option:\nplatform db:dump --gzip\n\nYou can use the --stdout option to pipe the result to another command. For example, if you want to create a bzip2-compressed file, you can run:\nplatform db:dump --stdout | bzip2 &gt; dump.sql.bz2\n\nImporting data\nThe easiest way to load data into a database is to pipe an SQL dump through the platform sql command, like so:\nplatform sql &lt; my_database_backup.sql\n\nThat will run the database backup against the SQL database on Platform.sh.  That will work for any SQL file, so the usual caveats about importing an SQL dump apply (e.g., it's best to run against an empty database).  As with exporting, you can also specify a specific environment to use and a specific database relationship to use, if there are multiple.\nplatform sql --relationship database -e master &lt; my_database_backup.sql\n\n\nnote\nImporting a database backup is a destructive operation. It will overwrite data already in your database.\nTaking a backup or a database export before doing so is strongly recommended.\n\nTroubleshooting\n\nMySQL lock wait timeout\ndefiner/invoker of view lack rights to use them\nMySQL server has gone away\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Services", "title": "Kafka", "url": "/configuration/services/kafka.html", "documentId": "d25b05bc6a72ce45e972daeadc2bcf3b8b5e858e", "text": "\n                        \n                            \n                                \n                                \n                                Kafka (Message Queue service)\nApache Kafka is an open-source stream-processing software platform.  It is a framework for storing, reading and analysing streaming data.\nSee the Kafka documentation for more information.\nSupported versions\n\n2.1\n2.2\n2.3\n2.4\n\nRelationship\nThe format exposed in the $PLATFORM_RELATIONSHIPS environment variable:\n{\n    \"service\": \"kafka\",\n    \"ip\": \"169.254.252.225\",\n    \"hostname\": \"wsxz7kjwfkb3j6eh2sdzfubrry.kafka.service._.eu-3.platformsh.site\",\n    \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\",\n    \"host\": \"kafka.internal\",\n    \"rel\": \"kafka\",\n    \"scheme\": \"kafka\",\n    \"type\": \"kafka:2.2\",\n    \"port\": 9092\n}\nUsage example\nIn your .platform/services.yaml:\nqueuekafka:\n    type: kafka:2.4\n    disk: 512\nIn your .platform.app.yaml:\nrelationships:\n    kafkaqueue: \"queuekafka:kafka\"\nYou can then use the service in a configuration file of your application with something like:\nJavaPythonRubypackage sh.platform.languages.sample;\n\nimport org.apache.kafka.clients.consumer.Consumer;\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.producer.Producer;\nimport org.apache.kafka.clients.producer.ProducerConfig;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.clients.producer.RecordMetadata;\nimport sh.platform.config.Config;\nimport sh.platform.config.Kafka;\n\nimport java.time.Duration;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.function.Supplier;\n\npublic class KafkaSample implements Supplier&lt;String&gt; {\n\n    @Override\n    public String get() {\n        StringBuilder logger = new StringBuilder();\n\n        // Create a new config object to ease reading the Platform.sh environment variables.\n        // You can alternatively use getenv() yourself.\n        Config config = new Config();\n\n        try {\n            // Get the credentials to connect to the Kafka service.\n            final Kafka kafka = config.getCredential(\"kafka\", Kafka::new);\n            Map&lt;String, Object&gt; configProducer = new HashMap&lt;&gt;();\n            configProducer.putIfAbsent(ProducerConfig.CLIENT_ID_CONFIG, \"animals\");\n            final Producer&lt;Long, String&gt; producer = kafka.getProducer(configProducer);\n\n            // Sending data into the stream.\n            RecordMetadata metadata = producer.send(new ProducerRecord&lt;&gt;(\"animals\", \"lion\")).get();\n            logger.append(\"Record sent with to partition \").append(metadata.partition())\n                    .append(\" with offset \").append(metadata.offset()).append('\\n');\n\n            metadata = producer.send(new ProducerRecord&lt;&gt;(\"animals\", \"dog\")).get();\n            logger.append(\"Record sent with to partition \").append(metadata.partition())\n                    .append(\" with offset \").append(metadata.offset()).append('\\n');\n\n            metadata = producer.send(new ProducerRecord&lt;&gt;(\"animals\", \"cat\")).get();\n            logger.append(\"Record sent with to partition \").append(metadata.partition())\n                    .append(\" with offset \").append(metadata.offset()).append('\\n');\n\n            // Consumer, read data from the stream.\n            final HashMap&lt;String, Object&gt; configConsumer = new HashMap&lt;&gt;();\n            configConsumer.put(ConsumerConfig.GROUP_ID_CONFIG, \"consumerGroup1\");\n            configConsumer.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n\n            Consumer&lt;Long, String&gt; consumer = kafka.getConsumer(configConsumer, \"animals\");\n            ConsumerRecords&lt;Long, String&gt; consumerRecords = consumer.poll(Duration.ofSeconds(3));\n\n            // Print each record.\n            consumerRecords.forEach(record -&gt; {\n                logger.append(\"Record: Key \" + record.key());\n                logger.append(\" value \" + record.value());\n                logger.append(\" partition \" + record.partition());\n                logger.append(\" offset \" + record.offset()).append('\\n');\n            });\n\n            // Commits the offset of record to broker.\n            consumer.commitSync();\n\n            return logger.toString();\n        } catch (Exception exp) {\n            throw new RuntimeException(\"An error when execute Kafka\", exp);\n        }\n    }\n}from json import dumps\nfrom json import loads\nfrom kafka import KafkaConsumer, KafkaProducer\nfrom platformshconfig import Config\n\n\ndef usage_example():\n    # Create a new Config object to ease reading the Platform.sh environment variables.\n    # You can alternatively use os.environ yourself.\n    config = Config()\n    # Get the credentials to connect to the Kafka service.\n    credentials = config.credentials('kafka')\n    \n    try:\n        kafka_server = '{}:{}'.format(credentials['host'], credentials['port'])\n        \n        # Producer\n        producer = KafkaProducer(\n            bootstrap_servers=[kafka_server],\n            value_serializer=lambda x: dumps(x).encode('utf-8')\n        )\n        for e in range(10):\n            data = {'number' : e}\n            producer.send('numtest', value=data)\n        \n        # Consumer\n        consumer = KafkaConsumer(\n            bootstrap_servers=[kafka_server],\n            auto_offset_reset='earliest'\n        )\n        \n        consumer.subscribe(['numtest'])\n        \n        output = ''\n        # For demonstration purposes so it doesn't block.\n        for e in range(10):\n            message = next(consumer)\n            output += str(loads(message.value.decode('UTF-8'))[\"number\"]) + ', '\n\n        # What a real implementation would do instead.\n        # for message in consumer:\n        #     output += loads(message.value.decode('UTF-8'))[\"number\"]\n\n        return output\n    \n    except Exception as e:\n        return e\n## With the ruby-kafka gem\n\n# Producer\nrequire \"kafka\"\nkafka = Kafka.new([\"kafka.internal:9092\"], client_id: \"my-application\")\nkafka.deliver_message(\"Hello, World!\", topic: \"greetings\")\n\n# Consumer\nkafka.each_message(topic: \"greetings\") do |message|\n  puts message.offset, message.key, message.value\nend\n(The specific way to inject configuration into your application will vary. Consult your application or framework's documentation.)\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Services", "title": "Memcached", "url": "/configuration/services/memcached.html", "documentId": "5aa7db451b45ef8276a7745d26c71ad06bd8bfb2", "text": "\n                        \n                            \n                                \n                                \n                                Memcached (Object cache)\nMemcached is a simple in-memory object store well-suited for application level caching.\nSee the Memcached documentation for more information.\nBoth Memcached and Redis can be used for application caching.  As a general rule, Memcached is simpler and thus more widely supported while Redis is more robust.  Platform.sh recommends using Redis if possible but Memcached is fully supported if an application favors that cache service.\nSupported versions\n\n1.4\n1.5\n1.6\n\nRelationship\nThe format exposed in the $PLATFORM_RELATIONSHIPS environment variable:\n{\n    \"service\": \"memcached\",\n    \"ip\": \"169.254.235.192\",\n    \"hostname\": \"2fw7ykay5vo5cez7cgwccz5kqa.memcached.service._.eu-3.platformsh.site\",\n    \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\",\n    \"host\": \"memcached.internal\",\n    \"rel\": \"memcached\",\n    \"scheme\": \"memcached\",\n    \"type\": \"memcached:1.4\",\n    \"port\": 11211\n}\nUsage example\nIn your .platform/services.yaml:\ncachemc:\n    type: memcached:1.6\nNow add a relationship in your .platform.app.yaml file:\nrelationships:\n    memcachedcache: \"cachemc:memcached\"\nIf you are using PHP, configure the relationship and enable the PHP memcached extension in your .platform.app.yaml.  (Note that the memcached extension requires igbinary and msgpack as well, but those will be enabled automatically.)\nruntime:\n    extensions:\n        - memcached\n\nFor Python you will need to include a dependency for a Memcached library, either via your requirements.txt file or a global dependency.  As a global dependency you would add the following to .platform.app.yaml:\ndependencies:\n    python:\n       python-memcached: '*'\n\nYou can then use the service in a configuration file of your application with something like:\nGoJavaNode.jsPHPPythonpackage examples\n\nimport (\n\t\"fmt\"\n\t\"github.com/bradfitz/gomemcache/memcache\"\n\tpsh \"github.com/platformsh/config-reader-go/v2\"\n\tgomemcache \"github.com/platformsh/config-reader-go/v2/gomemcache\"\n)\n\nfunc UsageExampleMemcached() string {\n\n\t// Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables.\n\t// You can alternatively use os.Getenv() yourself.\n\tconfig, err := psh.NewRuntimeConfig()\n\tcheckErr(err)\n\n\t// Get the credentials to connect to the Solr service.\n\tcredentials, err := config.Credentials(\"memcached\")\n\tcheckErr(err)\n\n\t// Retrieve formatted credentials for gomemcache.\n\tformatted, err := gomemcache.FormattedCredentials(credentials)\n\tcheckErr(err)\n\n\t// Connect to Memcached.\n\tmc := memcache.New(formatted)\n\n\t// Set a value.\n\tkey := \"Deploy_day\"\n\tvalue := \"Friday\"\n\n\terr = mc.Set(&amp;memcache.Item{Key: key, Value: []byte(value)})\n\n\t// Read it back.\n\ttest, err := mc.Get(key)\n\n\treturn fmt.Sprintf(\"Found value &lt;strong&gt;%s&lt;/strong&gt; for key &lt;strong&gt;%s&lt;/strong&gt;.\", test.Value, key)\n}\npackage sh.platform.languages.sample;\n\nimport net.spy.memcached.MemcachedClient;\nimport sh.platform.config.Config;\n\nimport java.util.function.Supplier;\n\nimport sh.platform.config.Memcached;\n\npublic class MemcachedSample implements Supplier&lt;String&gt; {\n\n    @Override\n    public String get() {\n        StringBuilder logger = new StringBuilder();\n\n        // Create a new config object to ease reading the Platform.sh environment variables.\n        // You can alternatively use getenv() yourself.\n        Config config = new Config();\n\n        // Get the credentials to connect to the Memcached service.\n        Memcached memcached = config.getCredential(\"memcached\", Memcached::new);\n\n        final MemcachedClient client = memcached.get();\n\n        String key = \"cloud\";\n        String value = \"platformsh\";\n\n        // Set a value.\n        client.set(key, 0, value);\n\n        // Read it back.\n        Object test = client.get(key);\n\n        logger.append(String.format(\"Found value %s for key %s.\", test, key));\n\n        return logger.toString();\n    }\n}const Memcached = require('memcached');\nconst config = require(\"platformsh-config\").config();\nconst { promisify } = require('util');\n\nexports.usageExample = async function() {\n\n    const credentials = config.credentials('memcached');\n\n    let client = new Memcached(`${credentials.host}:${credentials.port}`);\n\n    // The MemcacheD client is not Promise-aware, so make it so.\n    const memcachedGet = promisify(client.get).bind(client);\n    const memcachedSet = promisify(client.set).bind(client);\n\n    let key = 'Deploy-day';\n    let value = 'Friday';\n\n    // Set a value.\n    await memcachedSet(key, value, 10);\n\n    // Read it back.\n    let test = await memcachedGet(key);\n\n    let output = `Found value &lt;strong&gt;${test}&lt;/strong&gt; for key &lt;strong&gt;${key}&lt;/strong&gt;.`;\n\n    return output;\n};\n&lt;?php\n\ndeclare(strict_types=1);\n\nuse Platformsh\\ConfigReader\\Config;\n\n// Create a new config object to ease reading the Platform.sh environment variables.\n// You can alternatively use getenv() yourself.\n$config = new Config();\n\n// Get the credentials to connect to the Memcached service.\n$credentials = $config-&gt;credentials('memcached');\n\ntry {\n    // Connecting to Memcached server.\n    $memcached = new Memcached();\n    $memcached-&gt;addServer($credentials['host'], $credentials['port']);\n    $memcached-&gt;setOption(Memcached::OPT_BINARY_PROTOCOL, true);\n\n    $key = \"Deploy day\";\n    $value = \"Friday\";\n\n    // Set a value.\n    $memcached-&gt;set($key, $value);\n\n    // Read it back.\n    $test = $memcached-&gt;get($key);\n\n    printf('Found value &lt;strong&gt;%s&lt;/strong&gt; for key &lt;strong&gt;%s&lt;/strong&gt;.', $test, $key);\n\n} catch (Exception $e) {\n    print $e-&gt;getMessage();\n}\n\nimport pymemcache\nfrom platformshconfig import Config\n\n\ndef usage_example():\n\n    # Create a new Config object to ease reading the Platform.sh environment variables.\n    # You can alternatively use os.environ yourself.\n    config = Config()\n\n    # Get the credentials to connect to the Memcached service.\n    credentials = config.credentials('memcached')\n\n    try:\n        # Try connecting to Memached server.\n        memcached = pymemcache.Client((credentials['host'], credentials['port']))\n        memcached.set('Memcached::OPT_BINARY_PROTOCOL', True)\n\n        key = \"Deploy_day\"\n        value = \"Friday\"\n\n        # Set a value.\n        memcached.set(key, value)\n\n        # Read it back.\n        test = memcached.get(key)\n\n        return 'Found value &lt;strong&gt;{0}&lt;/strong&gt; for key &lt;strong&gt;{1}&lt;/strong&gt;.'.format(test.decode(\"utf-8\"), key)\n\n    except Exception as e:\n        return e\n\nAccessing Memcached directly\nTo access the Memcached service directly you can simply use netcat as Memcached does not have a dedicated client tool.  Assuming your Memcached relationship is named cache, the host name and port number obtained from PLATFORM_RELATIONSHIPS would be cache.internal and 11211. Open an SSH session and access the Memcached server as follows:\nnetcat cache.internal 11211\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Services", "title": "PostgreSQL", "url": "/configuration/services/postgresql.html", "documentId": "a986c3a86def3796c1057f0e03a8f9b3767434d5", "text": "\n                        \n                            \n                                \n                                \n                                PostgreSQL (Database service)\nPostgreSQL is a high-performance, standards-compliant relational SQL database.\nSee the PostgreSQL documentation for more information.\nSupported versions\n\n9.6\n10\n11\n12\n\n\nnote\nUpgrading to PostgreSQL 12 using the postgis extension is not currently supported. Attempting to upgrade with this extension enabled will result in a failed deployment that will require support intervention to fix.\nSee the Upgrading to PostgreSQL 12 with postgis section below for more details.\n\nDeprecated versions\nThe following versions are available but are not receiving security updates from upstream, so their use is not recommended. They will be removed at some point in the future.\n\n9.3\n\nRelationship\nThe format exposed in the $PLATFORM_RELATIONSHIPS environment variable:\n{\n    \"username\": \"main\",\n    \"scheme\": \"pgsql\",\n    \"service\": \"postgresql\",\n    \"ip\": \"169.254.58.227\",\n    \"hostname\": \"2tcu2y75zg2ub6wzufaz2nxlcm.postgresql.service._.eu-3.platformsh.site\",\n    \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\",\n    \"host\": \"postgresql.internal\",\n    \"rel\": \"postgresql\",\n    \"path\": \"main\",\n    \"query\": {\n        \"is_master\": true\n    },\n    \"password\": \"main\",\n    \"type\": \"postgresql:11\",\n    \"port\": 5432\n}\nUsage example\nIn your .platform/services.yaml add:\ndbpostgres:\n    type: postgresql:12\n    disk: 256\nAdd a relationship to the service in your .platform.app.yaml:\nrelationships:\n    postgresdatabase: \"dbpostgres:postgresql\"\nFor PHP, in your .platform.app.yaml add:\nruntime:\n    extensions:\n        - pdo_pgsql\n\nYou can then use the service in a configuration file of your application with something like:\nGoJavaNode.jsPHPPythonpackage examples\n\nimport (\n\t\"database/sql\"\n\t\"fmt\"\n\t_ \"github.com/lib/pq\"\n\tpsh \"github.com/platformsh/config-reader-go/v2\"\n\tlibpq \"github.com/platformsh/config-reader-go/v2/libpq\"\n)\n\nfunc UsageExamplePostgreSQL() string {\n\n\t// Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables.\n\t// You can alternatively use os.Getenv() yourself.\n\tconfig, err := psh.NewRuntimeConfig()\n\tcheckErr(err)\n\n\t// The 'database' relationship is generally the name of the primary SQL database of an application.\n\t// It could be anything, though, as in the case here where it's called \"postgresql\".\n\tcredentials, err := config.Credentials(\"postgresql\")\n\tcheckErr(err)\n\n\t// Retrieve the formatted credentials.\n\tformatted, err := libpq.FormattedCredentials(credentials)\n\tcheckErr(err)\n\n\t// Connect.\n\tdb, err := sql.Open(\"postgres\", formatted)\n\tcheckErr(err)\n\n\tdefer db.Close()\n\n\t// Creating a table.\n\tsqlCreate := \\x60\nCREATE TABLE IF NOT EXISTS PeopleGo (\nid SERIAL PRIMARY KEY,\nname VARCHAR(30) NOT NULL,\ncity VARCHAR(30) NOT NULL);\\x60\n\n\t_, err = db.Exec(sqlCreate)\n\tcheckErr(err)\n\n\t// Insert data.\n\tsqlInsert := \\x60\nINSERT INTO PeopleGo(name, city) VALUES\n('Neil Armstrong', 'Moon'),\n('Buzz Aldrin', 'Glen Ridge'),\n('Sally Ride', 'La Jolla');\\x60\n\n\t_, err = db.Exec(sqlInsert)\n\tcheckErr(err)\n\n\ttable := \\x60&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;City&lt;/th&gt;&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\\x60\n\n\tvar id int\n\tvar name string\n\tvar city string\n\n\t// Read it back.\n\trows, err := db.Query(\"SELECT * FROM PeopleGo\")\n\tif err != nil {\n\t\tpanic(err)\n\t} else {\n\t\tfor rows.Next() {\n\t\t\terr = rows.Scan(&amp;id, &amp;name, &amp;city)\n\t\t\tcheckErr(err)\n\t\t\ttable += fmt.Sprintf(\"&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td&gt;%s&lt;/td&gt;&lt;tr&gt;\\n\", name, city)\n\t\t}\n\t\ttable += \"&lt;/tbody&gt;\\n&lt;/table&gt;\\n\"\n\t}\n\n\t_, err = db.Exec(\"DROP TABLE PeopleGo;\")\n\tcheckErr(err)\n\n\treturn table\n}\npackage sh.platform.languages.sample;\n\nimport sh.platform.config.Config;\nimport sh.platform.config.MySQL;\nimport sh.platform.config.PostgreSQL;\n\nimport javax.sql.DataSource;\nimport java.sql.Connection;\nimport java.sql.ResultSet;\nimport java.sql.SQLException;\nimport java.sql.Statement;\nimport java.util.function.Supplier;\n\npublic class PostgreSQLSample implements Supplier&lt;String&gt; {\n\n    @Override\n    public String get() {\n        StringBuilder logger = new StringBuilder();\n\n        // Create a new config object to ease reading the Platform.sh environment variables.\n        // You can alternatively use getenv() yourself.\n        Config config = new Config();\n\n        // The 'database' relationship is generally the name of primary SQL database of an application.\n        // It could be anything, though, as in the case here here where it's called \"postgresql\".\n        PostgreSQL database = config.getCredential(\"postgresql\", PostgreSQL::new);\n        DataSource dataSource = database.get();\n\n        // Connect to the database\n        try (Connection connection = dataSource.getConnection()) {\n\n            // Creating a table.\n            String sql = \"CREATE TABLE JAVA_FRAMEWORKS (\" +\n                    \" id SERIAL PRIMARY KEY,\" +\n                    \"name VARCHAR(30) NOT NULL)\";\n\n            final Statement statement = connection.createStatement();\n            statement.execute(sql);\n\n            // Insert data.\n            sql = \"INSERT INTO JAVA_FRAMEWORKS (name) VALUES\" +\n                    \"('Spring'),\" +\n                    \"('Jakarta EE'),\" +\n                    \"('Eclipse JNoSQL')\";\n\n            statement.execute(sql);\n\n            // Show table.\n            sql = \"SELECT * FROM JAVA_FRAMEWORKS\";\n            final ResultSet resultSet = statement.executeQuery(sql);\n            while (resultSet.next()) {\n                int id = resultSet.getInt(\"id\");\n                String name = resultSet.getString(\"name\");\n                logger.append(String.format(\"the JAVA_FRAMEWORKS id %d the name %s \", id, name));\n                logger.append('\\n');\n            }\n            statement.execute(\"DROP TABLE JAVA_FRAMEWORKS\");\n            return logger.toString();\n        } catch (SQLException exp) {\n            throw new RuntimeException(\"An error when execute PostgreSQL\", exp);\n        }\n    }\n}const pg = require('pg');\nconst config = require(\"platformsh-config\").config();\n\nexports.usageExample = async function() {\n\n    const credentials = config.credentials('postgresql');\n\n    const client = new pg.Client({\n        host: credentials.host,\n        port: credentials.port,\n        user: credentials.username,\n        password: credentials.password,\n        database: credentials.path,\n    });\n\n    client.connect();\n\n    let sql = '';\n\n    // Creating a table.\n    sql = `CREATE TABLE IF NOT EXISTS People (\n      id SERIAL PRIMARY KEY,\n      name VARCHAR(30) NOT NULL,\n      city VARCHAR(30) NOT NULL\n      )`;\n    await client.query(sql);\n\n    // Insert data.\n    sql = `INSERT INTO People (name, city) VALUES \n        ('Neil Armstrong', 'Moon'), \n        ('Buzz Aldrin', 'Glen Ridge'), \n        ('Sally Ride', 'La Jolla');`;\n    await client.query(sql);\n\n    // Show table.\n    sql = `SELECT * FROM People`;\n    let result = await client.query(sql);\n\n    let output = '';\n\n    if (result.rows.length &gt; 0) {\n        output +=`&lt;table&gt;\n            &lt;thead&gt;\n            &lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;City&lt;/th&gt;&lt;/tr&gt;\n            &lt;/thead&gt;\n            &lt;tbody&gt;`;\n\n        result.rows.forEach((row) =&gt; {\n            output += `&lt;tr&gt;&lt;td&gt;${row.name}&lt;/td&gt;&lt;td&gt;${row.city}&lt;/td&gt;&lt;/tr&gt;\\n`;\n        });\n\n        output += `&lt;/tbody&gt;\\n&lt;/table&gt;\\n`;\n    }\n\n    // Drop table.\n    sql = `DROP TABLE People`;\n    await client.query(sql);\n\n    return output;\n};\n&lt;?php\n\ndeclare(strict_types=1);\n\nuse Platformsh\\ConfigReader\\Config;\n\n// Create a new config object to ease reading the Platform.sh environment variables.\n// You can alternatively use getenv() yourself.\n$config = new Config();\n\n// The 'database' relationship is generally the name of primary SQL database of an application.\n// It could be anything, though, as in the case here here where it's called \"postgresql\".\n$credentials = $config-&gt;credentials('postgresql');\n\ntry {\n    // Connect to the database using PDO.  If using some other abstraction layer you would\n    // inject the values from $database into whatever your abstraction layer asks for.\n    $dsn = sprintf('pgsql:host=%s;port=%d;dbname=%s', $credentials['host'], $credentials['port'], $credentials['path']);\n    $conn = new \\PDO($dsn, $credentials['username'], $credentials['password'], [\n        // Always use Exception error mode with PDO, as it's more reliable.\n        \\PDO::ATTR_ERRMODE =&gt; \\PDO::ERRMODE_EXCEPTION,\n        // So we don't have to mess around with cursors and unbuffered queries by default.\n    ]);\n\n    $conn-&gt;query(\"DROP TABLE IF EXISTS People\");\n\n    // Creating a table.\n    $sql = \"CREATE TABLE IF NOT EXISTS People (\n      id SERIAL PRIMARY KEY,\n      name VARCHAR(30) NOT NULL,\n      city VARCHAR(30) NOT NULL\n      )\";\n    $conn-&gt;query($sql);\n\n    // Insert data.\n    $sql = \"INSERT INTO People (name, city) VALUES\n        ('Neil Armstrong', 'Moon'),\n        ('Buzz Aldrin', 'Glen Ridge'),\n        ('Sally Ride', 'La Jolla');\";\n    $conn-&gt;query($sql);\n\n    // Show table.\n    $sql = \"SELECT * FROM People\";\n    $result = $conn-&gt;query($sql);\n    $result-&gt;setFetchMode(\\PDO::FETCH_OBJ);\n\n    if ($result) {\n        print &lt;&lt;&lt;TABLE\n&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;City&lt;/th&gt;&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\nTABLE;\n        foreach ($result as $record) {\n            printf(\"&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;\\n\", $record-&gt;name, $record-&gt;city);\n        }\n        print \"&lt;/tbody&gt;\\n&lt;/table&gt;\\n\";\n    }\n\n    // Drop table.\n    $sql = \"DROP TABLE People\";\n    $conn-&gt;query($sql);\n\n} catch (\\Exception $e) {\n    print $e-&gt;getMessage();\n}\nimport psycopg2\nfrom platformshconfig import Config\n\n\ndef usage_example():\n    # Create a new Config object to ease reading the Platform.sh environment variables.\n    # You can alternatively use os.environ yourself.\n    config = Config()\n\n    # The 'database' relationship is generally the name of primary SQL database of an application.\n    # That's not required, but much of our default automation code assumes it.' \\\n    database = config.credentials('postgresql')\n\n    try:\n        # Connect to the database.\n        conn_params = {\n            'host': database['host'],\n            'port': database['port'],\n            'dbname': database['path'],\n            'user': database['username'],\n            'password': database['password']\n        }\n\n        conn = psycopg2.connect(**conn_params)\n\n        # Open a cursor to perform database operations.\n        cur = conn.cursor()\n\n        cur.execute(\"DROP TABLE IF EXISTS People\")\n\n        # Creating a table.\n        sql = '''\n                CREATE TABLE IF NOT EXISTS People (\n                id SERIAL PRIMARY KEY,\n                name VARCHAR(30) NOT NULL,\n                city VARCHAR(30) NOT NULL\n                )\n                '''\n\n        cur.execute(sql)\n\n        # Insert data.\n        sql = '''\n                INSERT INTO People (name, city) VALUES\n                ('Neil Armstrong', 'Moon'),\n                ('Buzz Aldrin', 'Glen Ridge'),\n                ('Sally Ride', 'La Jolla');\n                '''\n\n        cur.execute(sql)\n\n        # Show table.\n        sql = '''SELECT * FROM People'''\n        cur.execute(sql)\n        result = cur.fetchall()\n\n        table = '''&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;City&lt;/th&gt;&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;'''\n\n        if result:\n            for record in result:\n                table += '''&lt;tr&gt;&lt;td&gt;{0}&lt;/td&gt;&lt;td&gt;{1}&lt;/td&gt;&lt;tr&gt;\\n'''.format(record[1], record[2])\n            table += '''&lt;/tbody&gt;\\n&lt;/table&gt;\\n'''\n\n        # Drop table\n        sql = \"DROP TABLE People\"\n        cur.execute(sql)\n\n        # Close communication with the database\n        cur.close()\n        conn.close()\n\n        return table\n\n    except Exception as e:\n        return e\n\nExporting data\nThe easiest way to download all data in a PostgreSQL instance is with the Platform CLI.  If you have a single SQL database, the following command will export all data using the pg_dump command to a local file:\nplatform db:dump\n\nIf you have multiple SQL databases it will prompt you which one to export. You can also specify one by relationship name explicitly:\nplatform db:dump --relationship database\n\nBy default the file will be uncompressed. If you want to compress it, use the --gzip (-z) option:\nplatform db:dump --gzip\n\nYou can use the --stdout option to pipe the result to another command. For example, if you want to create a bzip2-compressed file, you can run:\nplatform db:dump --stdout | bzip2 &gt; dump.sql.bz2\n\nImporting data\nThe easiest way to load data into a database is to pipe an SQL dump through the platform sql command, like so:\nplatform sql &lt; my_database_backup.sql\n\nThat will run the database backup against the SQL database on Platform.sh.  That will work for any SQL file, so the usual caveats about importing an SQL dump apply (e.g., it's best to run against an empty database).  As with exporting, you can also specify a specific environment to use and a specific database relationship to use, if there are multiple.\nplatform sql --relationship database -e master &lt; my_database_backup.sql\n\n\nnote\nImporting a database backup is a destructive operation. It will overwrite data already in your database.\nTaking a backup or a database export before doing so is strongly recommended.\n\nExtensions\nPlatform.sh supports a number of PostgreSQL extensions.  To enable them, list them under the configuration.extensions key in your services.yaml file, like so:\ndb:\n    type: postgresql:12\n    disk: 1025\n    configuration:\n        extensions:\n            - pg_trgm\n            - hstore\n\nIn this case you will have pg_trgm installed, providing functions to determine the similarity of text based on trigram matching, and hstore providing a key-value store.\nAvailable extensions\nThe following is the extensive list of supported extensions. Note that you cannot currently add custom\nextensions not listed here.\n\naddress_standardizer - Used to parse an address into constituent elements. Generally used to support geocoding address normalization step.\naddress_standardizer_data_us - Address Standardizer US dataset example\nadminpack - administrative functions for PostgreSQL\nautoinc - functions for autoincrementing fields\nbloom - bloom access method - signature file based index (requires 9.6 or higher)\nbtree_gin - support for indexing common datatypes in GIN\nbtree_gist - support for indexing common datatypes in GiST\nchkpass - data type for auto-encrypted passwords\ncitext - data type for case-insensitive character strings\ncube - data type for multidimensional cubes\ndblink - connect to other PostgreSQL databases from within a database\ndict_int - text search dictionary template for integers\ndict_xsyn - text search dictionary template for extended synonym processing\nearthdistance - calculate great-circle distances on the surface of the Earth\nfile_fdw - foreign-data wrapper for flat file access\nfuzzystrmatch - determine similarities and distance between strings\nhstore - data type for storing sets of (key, value) pairs\ninsert_username - functions for tracking who changed a table\nintagg - integer aggregator and enumerator (obsolete)\nintarray - functions, operators, and index support for 1-D arrays of integers\nisn - data types for international product numbering standards\nlo - Large Object maintenance\nltree - data type for hierarchical tree-like structures\nmoddatetime - functions for tracking last modification time\npageinspect - inspect the contents of database pages at a low level\npg_buffercache - examine the shared buffer cache\npg_freespacemap - examine the free space map (FSM)\npg_prewarm - prewarm relation data (requires 9.6 or higher)\npg_stat_statements - track execution statistics of all SQL statements executed\npg_trgm - text similarity measurement and index searching based on trigrams\npg_visibility - examine the visibility map (VM) and page-level visibility info (requires 9.6 or higher)\npgcrypto - cryptographic functions\npgrouting - pgRouting Extension (requires 9.6 or higher)\npgrowlocks - show row-level locking information\npgstattuple - show tuple-level statistics\nplpgsql - PL/pgSQL procedural language\npostgis - PostGIS geometry, geography, and raster spatial types and functions\npostgis_sfcgal - PostGIS SFCGAL functions\npostgis_tiger_geocoder - PostGIS tiger geocoder and reverse geocoder\npostgis_topology - PostGIS topology spatial types and functions\npostgres_fdw - foreign-data wrapper for remote PostgreSQL servers\nrefint - functions for implementing referential integrity (obsolete)\nseg - data type for representing line segments or floating-point intervals\nsslinfo - information about SSL certificates\ntablefunc - functions that manipulate whole tables, including crosstab\ntcn - Triggered change notifications\ntimetravel - functions for implementing time travel\ntsearch2 - compatibility package for pre-8.3 text search functions (obsolete, only available for 9.6 and 9.3)\ntsm_system_rows - TABLESAMPLE method which accepts number of rows as a limit (requires 9.6 or higher)\ntsm_system_time - TABLESAMPLE method which accepts time in milliseconds as a limit (requires 9.6 or higher)\nunaccent - text search dictionary that removes accents\nuuid-ossp - generate universally unique identifiers (UUIDs)\nxml2 - XPath querying and XSLT\n\n\nnote\nUpgrading to PostgreSQL 12 using the postgis extension is not currently supported. Attempting to upgrade with this extension enabled will result in a failed deployment that will require support intervention to fix.\nSee the Upgrading to PostgreSQL 12 with postgis section below for more details.\n\nNotes\nCould not find driver\nIf you see this error: Fatal error: Uncaught exception 'PDOException' with message 'could not find driver', this means you are missing the pdo_pgsql PHP extension. You simply need to enable it in your .platform.app.yaml (see above).\nUpgrading\nPostgreSQL 10 and later include an upgrade utility that can convert databases from previous versions to version 10 or 11.  If you upgrade your service from a previous version of PostgreSQL to version 10 or above (by modifying the services.yaml file) the upgrader will run automatically.\nThe upgrader does not work to upgrade to PostgreSQL 9 versions, so upgrades from PostgreSQL 9.3 to 9.6 are not supported.  Upgrade straight to version 11 instead.\n\nWarning: Make sure you first test your migration on a separate branch\nWarning: be sure to take a backup of your master environment before you merge this change.\n\nDowngrading is not supported. If you want, for whatever reason, to downgrade you should dump to SQL, remove the service, recreate the service, and import your dump.\nUpgrading to PostgreSQL 12 with the postgis extension\nUpgrading to PostgreSQL 12 using the postgis extension is not currently supported. Attempting to upgrade with this extension enabled will result in a failed deployment that will require support intervention to fix.\nIf you need to upgrade, you should follow the same steps recommended for performing downgrades: dump the database, remove the service, recreate the service with PostgreSQL 12, and then import the dump to that service.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Services", "title": "MongoDB", "url": "/configuration/services/mongodb.html", "documentId": "09cb7c78a88c13180e10b6f9c82cc9359ffde13b", "text": "\n                        \n                            \n                                \n                                \n                                MongoDB (Database service)\nMongoDB is a cross-platform, document-oriented database.\nFor more information on using MongoDB, see MongoDB's own documentation.\nSupported versions\n\n3.0\n3.2\n3.4\n3.6\n\n\nnote\nDowngrades of MongoDB are not supported. MongoDB will update its own datafiles to a new version automatically but cannot downgrade them. If you want to experiment with a later version without committing to it use a non-master environment.\n\nRelationship\nThe format exposed in the $PLATFORM_RELATIONSHIPS environment variable:\n{\n    \"username\": \"main\",\n    \"scheme\": \"mongodb\",\n    \"service\": \"mongodb\",\n    \"ip\": \"169.254.117.167\",\n    \"hostname\": \"ldh423mk2e7o6qto2syljqbg5u.mongodb.service._.eu-3.platformsh.site\",\n    \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\",\n    \"host\": \"mongodb.internal\",\n    \"rel\": \"mongodb\",\n    \"path\": \"main\",\n    \"query\": {\n        \"is_master\": true\n    },\n    \"password\": \"main\",\n    \"type\": \"mongodb:3.6\",\n    \"port\": 27017\n}\nUsage example\nIn your .platform/services.yaml:\ndbmongo:\n    type: mongodb:3.6\n    disk: 512\nThe minimum disk size for MongoDB is 512 (MB).\nrelationships:\n    mongodatabase: \"dbmongo:mongodb\"\nFor PHP, in your .platform.app.yaml add:\nruntime:\n    extensions:\n        - mongodb\n\n(Before PHP 7, use mongo instead.)\nYou can then use the service in a configuration file of your application with something like:\nGoJavaNode.jsPHPPythonpackage examples\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\tpsh \"github.com/platformsh/config-reader-go/v2\"\n\tmongoPsh \"github.com/platformsh/config-reader-go/v2/mongo\"\n\t\"go.mongodb.org/mongo-driver/bson\"\n\t\"go.mongodb.org/mongo-driver/mongo\"\n\t\"go.mongodb.org/mongo-driver/mongo/options\"\n\t\"time\"\n)\n\nfunc UsageExampleMongoDB() string {\n\n\t// Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables.\n\t// You can alternatively use os.Getenv() yourself.\n\tconfig, err := psh.NewRuntimeConfig()\n\tcheckErr(err)\n\n\t// Get the credentials to connect to the Solr service.\n\tcredentials, err := config.Credentials(\"mongodb\")\n\tcheckErr(err)\n\n\t// Retrieve the formatted credentials for mongo-driver.\n\tformatted, err := mongoPsh.FormattedCredentials(credentials)\n\tcheckErr(err)\n\n\t// Connect to MongoDB using the formatted credentials.\n\tctx, _ := context.WithTimeout(context.Background(), 10*time.Second)\n\tclient, err := mongo.Connect(ctx, options.Client().ApplyURI(formatted))\n\tcheckErr(err)\n\n\t// Create a new collection.\n\tcollection := client.Database(\"main\").Collection(\"starwars\")\n\n\t// Clean up after ourselves.\n\terr = collection.Drop(context.Background())\n\tcheckErr(err)\n\n\t// Create an entry.\n\tres, err := collection.InsertOne(ctx, bson.M{\"name\": \"Rey\", \"occupation\": \"Jedi\"})\n\tcheckErr(err)\n\n\tid := res.InsertedID\n\n\t// Read it back.\n\tcursor, err := collection.Find(context.Background(), bson.M{\"_id\": id})\n\tcheckErr(err)\n\n\tvar name string\n\tvar occupation string\n\n\tfor cursor.Next(context.Background()) {\n\t\tdocument := struct {\n\t\t\tName       string\n\t\t\tOccupation string\n\t\t}{}\n\t\terr := cursor.Decode(&amp;document)\n\t\tcheckErr(err)\n\n\t\tname = document.Name\n\t\toccupation = document.Occupation\n\t}\n\n\treturn fmt.Sprintf(\"Found %s (%s)\", name, occupation)\n}\npackage sh.platform.languages.sample;\n\nimport com.mongodb.MongoClient;\nimport com.mongodb.client.MongoCollection;\nimport com.mongodb.client.MongoDatabase;\nimport org.bson.Document;\nimport sh.platform.config.Config;\nimport sh.platform.config.MongoDB;\n\nimport java.util.function.Supplier;\n\nimport static com.mongodb.client.model.Filters.eq;\n\npublic class MongoDBSample implements Supplier&lt;String&gt; {\n\n    @Override\n    public String get() {\n        StringBuilder logger = new StringBuilder();\n\n        // Create a new config object to ease reading the Platform.sh environment variables.\n        // You can alternatively use getenv() yourself.\n        Config config = new Config();\n\n        // The 'database' relationship is generally the name of primary database of an application.\n        // It could be anything, though, as in the case here here where it's called \"mongodb\".\n        MongoDB database = config.getCredential(\"mongodb\", MongoDB::new);\n        MongoClient mongoClient = database.get();\n        final MongoDatabase mongoDatabase = mongoClient.getDatabase(database.getDatabase());\n        MongoCollection&lt;Document&gt; collection = mongoDatabase.getCollection(\"scientist\");\n        Document doc = new Document(\"name\", \"Ada Lovelace\")\n                .append(\"city\", \"London\");\n\n        collection.insertOne(doc);\n        Document myDoc = collection.find(eq(\"_id\", doc.get(\"_id\"))).first();\n        logger.append(myDoc.toJson()).append('\\n');\n        logger.append(collection.deleteOne(eq(\"_id\", doc.get(\"_id\"))));\n        return logger.toString();\n    }\n}const mongodb = require('mongodb');\nconst config = require(\"platformsh-config\").config();\n\nexports.usageExample = async function() {\n    const credentials = config.credentials('mongodb');\n    const MongoClient = mongodb.MongoClient;\n\n    var client = await MongoClient.connect(config.formattedCredentials('mongodb', 'mongodb'));\n\n    let db = client.db(credentials[\"path\"]);\n\n    let collection = db.collection(\"startrek\");\n\n    const documents = [\n        {'name': 'James Kirk', 'rank': 'Admiral'},\n        {'name': 'Jean-Luc Picard', 'rank': 'Captain'},\n        {'name': 'Benjamin Sisko', 'rank': 'Prophet'},\n        {'name': 'Katheryn Janeway', 'rank': 'Captain'},\n    ];\n\n    await collection.insert(documents, {w: 1});\n\n    let result = await collection.find({rank:\"Captain\"}).toArray();\n\n    let output = '';\n\n    output += `&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Rank&lt;/th&gt;&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;`;\n\n    Object.keys(result).forEach((key) =&gt; {\n        output += `&lt;tr&gt;&lt;td&gt;${result[key].name}&lt;/td&gt;&lt;td&gt;${result[key].rank}&lt;/td&gt;&lt;/tr&gt;\\n`;\n    });\n\n    output += `&lt;/tbody&gt;\\n&lt;/table&gt;\\n`;\n\n    // Clean up after ourselves.\n    collection.remove();\n\n    return output;\n};\n&lt;?php\n\ndeclare(strict_types=1);\n\nuse Platformsh\\ConfigReader\\Config;\nuse MongoDB\\Client;\n\n// Create a new config object to ease reading the Platform.sh environment variables.\n// You can alternatively use getenv() yourself.\n$config = new Config();\n\n// The 'database' relationship is generally the name of primary database of an application.\n// It could be anything, though, as in the case here here where it's called \"mongodb\".\n$credentials = $config-&gt;credentials('mongodb');\n\ntry {\n\n    $server = sprintf('%s://%s:%s@%s:%d/%s',\n        $credentials['scheme'],\n        $credentials['username'],\n        $credentials['password'],\n        $credentials['host'],\n        $credentials['port'],\n        $credentials['path']\n    );\n\n    $client = new Client($server);\n    $collection = $client-&gt;main-&gt;starwars;\n\n    $result = $collection-&gt;insertOne([\n        'name' =&gt; 'Rey',\n        'occupation' =&gt; 'Jedi',\n    ]);\n\n    $id = $result-&gt;getInsertedId();\n\n    $document = $collection-&gt;findOne([\n        '_id' =&gt; $id,\n    ]);\n\n    // Clean up after ourselves.\n    $collection-&gt;drop();\n\n    printf(\"Found %s (%s)&lt;br /&gt;\\n\", $document-&gt;name, $document-&gt;occupation);\n\n} catch (\\Exception $e) {\n    print $e-&gt;getMessage();\n}\nfrom pymongo import MongoClient\nfrom platformshconfig import Config\n\n\ndef usage_example():\n\n    # Create a new Config object to ease reading the Platform.sh environment variables.\n    # You can alternatively use os.environ yourself.\n    config = Config()\n\n    # The 'database' relationship is generally the name of primary SQL database of an application.\n    # It could be anything, though, as in the case here here where it's called \"mongodb\".\n    credentials = config.credentials('mongodb')\n\n    try:\n        formatted = config.formatted_credentials('mongodb', 'pymongo')\n\n        server = '{0}://{1}:{2}@{3}'.format(\n            credentials['scheme'],\n            credentials['username'],\n            credentials['password'],\n            formatted\n        )\n\n        client = MongoClient(server)\n\n        collection = client.main.starwars\n\n        post = {\n            \"name\": \"Rey\",\n            \"occupation\": \"Jedi\"\n        }\n\n        post_id = collection.insert_one(post).inserted_id\n\n        document = collection.find_one(\n            {\"_id\": post_id}\n        )\n\n        # Clean up after ourselves.\n        collection.drop()\n\n        return 'Found {0} ({1})&lt;br /&gt;'.format(document['name'], document['occupation'])\n\n    except Exception as e:\n        return e\n\nExporting data\nThe most straightforward way to export data from a MongoDB database is to open an SSH tunnel to it and simply export the data directly using MongoDB's tools.  \nFirst, open an SSH tunnel with the Platform.sh CLI:\nplatform tunnel:open\n\nThat will open an SSH tunnel to all services on your current environment, and produce output something like the following:\nSSH tunnel opened on port 30000 to relationship: database\nSSH tunnel opened on port 30001 to relationship: redis\n\nThe port may vary in your case.  You will also need to obtain the user, password, and database name from the relationships array, as above.\nThen, simply connect to that port locally using mongodump (or your favorite MongoDB tools) to export all data in that server:\nmongodump --port 30000 -u main -p main --authenticationDatabase main --db main\n\n(If necessary, vary the -u, -p, --authenticationDatabase and --db flags.)\nAs with any other shell command it can be piped to another command to compress the output or redirect it to a specific file.\nFor further references please see the official mongodump documentation.\nUpgrading\nTo upgrade to 3.6 from a version earlier than 3.4, you must successively upgrade major releases until you have upgraded to 3.4. For example, if you are running a 3.0 image, you must upgrade first to 3.2 and then upgrade to 3.4 before you can upgrade to 3.6.\nFor more details on upgrading and how to handle potential application backward compatibility issues, please see Release Notes for MongoDB.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Services", "title": "InfluxDB", "url": "/configuration/services/influxdb.html", "documentId": "d8c2c41ed069fdcae43deeef07b90ba0aaa373b9", "text": "\n                        \n                            \n                                \n                                \n                                InfluxDB (Database service)\nInfluxDB is a time series database optimized for high-write-volume use cases such as logs, sensor data, and real-time analytics.  It exposes an HTTP API for client interaction.\nSee the InfluxDB documentation for more information.\nSupported versions\n\n1.2\n1.3\n1.7\n\nRelationship\nThe format exposed in the $PLATFORM_RELATIONSHIPS environment variable:\n{\n    \"service\": \"influxdb\",\n    \"ip\": \"169.254.180.153\",\n    \"hostname\": \"mycuti5glfqyt322jjhcfahrpi.influxdb.service._.eu-3.platformsh.site\",\n    \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\",\n    \"host\": \"influxdb.internal\",\n    \"rel\": \"influxdb\",\n    \"scheme\": \"http\",\n    \"type\": \"influxdb:1.7\",\n    \"port\": 8086\n}\nUsage example\nIn your .platform/services.yaml:\ntimedb:\n    type: influxdb:1.7\n    disk: 256\nIn your .platform.app.yaml:\nrelationships:\n    influxtimedb: \"timedb:influxdb\"\nYou can then use the service in a configuration file of your application with something like:\n&lt;?php\n// This assumes a fictional application with an array named $settings.\nif (getenv('PLATFORM_RELATIONSHIPS')) {\n    $relationships = json_decode(base64_decode($relationships), TRUE);\n\n    // For a relationship named 'influxtimedb' referring to one endpoint.\n    if (!empty($relationships['influxtimedb'])) {\n        foreach ($relationships['influxtimedb'] as $endpoint) {\n            $settings['influxdb_host'] = $endpoint['host'];\n            $settings['influxdb_port'] = $endpoint['port'];\n            break;\n        }\n    }\n}\n\nExporting data\nInfluxDB includes its own export mechanism.  To gain access to the server from your local machine open an SSH tunnel with the Platform.sh CLI:\nplatform tunnel:open\n\nThat will open an SSH tunnel to all services on your current environment, and produce output something like the following:\nSSH tunnel opened on port 30000 to relationship: influxtimedb\n\nThe port may vary in your case.  Then, simply run InfluxDB's export commands as desired.\ninflux_inspect export -compress\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Services", "title": "Elasticsearch", "url": "/configuration/services/elasticsearch.html", "documentId": "149265ecd7b1d7bb5aa7ec0d02cc291893e5725a", "text": "\n                        \n                            \n                                \n                                \n                                Elasticsearch (Search Service)\nElasticsearch is a distributed RESTful search engine built for the cloud.\nSee the Elasticsearch documentation for more information.\nSupported versions\n\n6.5\n7.2\n\nDeprecated versions\nThe following versions are available but are not receiving security updates from upstream, so their use is not recommended. They will be removed at some point in the future.\n\n0.90\n1.4\n1.7\n2.4\n5.2\n5.4\n\nRelationship\nThe format exposed in the $PLATFORM_RELATIONSHIPS environment variable:\n{\n    \"username\": null,\n    \"scheme\": \"http\",\n    \"service\": \"elasticsearch\",\n    \"fragment\": null,\n    \"ip\": \"169.254.252.137\",\n    \"hostname\": \"gnenv2b23ltik7mrvu3fyrlybq.elasticsearch.service._.eu-3.platformsh.site\",\n    \"public\": false,\n    \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\",\n    \"host\": \"elasticsearch.internal\",\n    \"rel\": \"elasticsearch\",\n    \"query\": [],\n    \"path\": null,\n    \"password\": null,\n    \"type\": \"elasticsearch:7.2\",\n    \"port\": 9200,\n    \"host_mapped\": false\n}\nUsage example\nIn your .platform/services.yaml:\nsearchelastic:\n    type: elasticsearch:7.2\n    disk: 256\nIn your .platform.app.yaml:\nrelationships:\n    essearch: \"searchelastic:elasticsearch\"\nYou can then use the service in a configuration file of your application with something like:\nJavaNode.jsPHPPythonpackage sh.platform.languages.sample;\n\nimport org.elasticsearch.action.admin.indices.refresh.RefreshRequest;\nimport org.elasticsearch.action.admin.indices.refresh.RefreshResponse;\nimport org.elasticsearch.action.delete.DeleteRequest;\nimport org.elasticsearch.action.index.IndexRequest;\nimport org.elasticsearch.action.search.SearchRequest;\nimport org.elasticsearch.action.search.SearchResponse;\nimport org.elasticsearch.client.RequestOptions;\nimport org.elasticsearch.client.RestHighLevelClient;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.search.SearchHit;\nimport org.elasticsearch.search.builder.SearchSourceBuilder;\nimport sh.platform.config.Config;\nimport sh.platform.config.Elasticsearch;\n\nimport java.io.IOException;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.function.Supplier;\n\nimport static java.util.concurrent.ThreadLocalRandom.current;\n\npublic class ElasticsearchSample implements Supplier&lt;String&gt; {\n\n    @Override\n    public String get() {\n        StringBuilder logger = new StringBuilder();\n\n        // Create a new config object to ease reading the Platform.sh environment variables.\n        // You can alternatively use getenv() yourself.\n        Config config = new Config();\n\n        Elasticsearch elasticsearch = config.getCredential(\"elasticsearch\", Elasticsearch::new);\n\n        // Create an Elasticsearch client object.\n        RestHighLevelClient client = elasticsearch.get();\n\n        try {\n\n            String index = \"animals\";\n            String type = \"mammals\";\n            // Index a few document.\n            final List&lt;String&gt; animals = Arrays.asList(\"dog\", \"cat\", \"monkey\", \"horse\");\n            for (String animal : animals) {\n                Map&lt;String, Object&gt; jsonMap = new HashMap&lt;&gt;();\n                jsonMap.put(\"name\", animal);\n                jsonMap.put(\"age\", current().nextInt(1, 10));\n                jsonMap.put(\"is_cute\", current().nextBoolean());\n\n                IndexRequest indexRequest = new IndexRequest(index, type)\n                        .id(animal).source(jsonMap);\n                client.index(indexRequest, RequestOptions.DEFAULT);\n            }\n\n            RefreshRequest refresh = new RefreshRequest(index);\n\n            // Force just-added items to be indexed\n            RefreshResponse refreshResponse = client.indices().refresh(refresh, RequestOptions.DEFAULT);\n\n            // Search for documents.\n            SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n            sourceBuilder.query(QueryBuilders.termQuery(\"name\", \"dog\"));\n            SearchRequest searchRequest = new SearchRequest();\n            searchRequest.indices(index);\n            searchRequest.source(sourceBuilder);\n\n            SearchResponse search = client.search(searchRequest, RequestOptions.DEFAULT);\n\n            for (SearchHit hit : search.getHits()) {\n                String id = hit.getId();\n                final Map&lt;String, Object&gt; source = hit.getSourceAsMap();\n                logger.append(String.format(\"result id %s source: %s\", id, source)).append('\\n');\n            }\n\n            // Delete documents.\n            for (String animal : animals) {\n                client.delete(new DeleteRequest(index, type, animal), RequestOptions.DEFAULT);\n            }\n        } catch (IOException exp) {\n            throw new RuntimeException(\"An error when execute Elasticsearch: \" + exp.getMessage());\n        }\n        return logger.toString();\n    }\n}const elasticsearch = require('elasticsearch');\nconst config = require(\"platformsh-config\").config();\n\nexports.usageExample = async function() {\n\n    const credentials = config.credentials('elasticsearch');\n\n    var client = new elasticsearch.Client({\n        host: `${credentials.host}:${credentials.port}`,\n    });\n\n\n    let index = 'my_index';\n    let type = 'People';\n\n    // Index a few document.\n    let names = ['Ada Lovelace', 'Alonzo Church', 'Barbara Liskov'];\n\n    let message = {\n        refresh: \"wait_for\",\n        body: []\n    };\n    names.forEach((name) =&gt; {\n        message.body.push({index: {_index: index, _type: type}});\n        message.body.push({name: name});\n    });\n    await client.bulk(message);\n\n    // Search for documents.\n    const response = await client.search({\n        index: index,\n        q: 'name:Barbara Liskov'\n    });\n\n    let output = '';\n\n    if(response.hits.total.value &gt; 0) {\n        output += `&lt;table&gt;\n        &lt;thead&gt;\n        &lt;tr&gt;&lt;th&gt;ID&lt;/th&gt;&lt;th&gt;Name&lt;/th&gt;&lt;/tr&gt;\n        &lt;/thead&gt;\n        &lt;tbody&gt;`;\n        response.hits.hits.forEach((record) =&gt; {\n            output += `&lt;tr&gt;&lt;td&gt;${record._id}&lt;/td&gt;&lt;td&gt;${record._source.name}&lt;/td&gt;&lt;/tr&gt;\\n`;\n        });\n        output += \"&lt;/tbody&gt;\\n&lt;/table&gt;\\n\";\n    }\n    else {\n        output = \"No records found.\";\n    }\n\n    // Clean up after ourselves.\n    response.hits.hits.forEach((record) =&gt; {\n        client.delete({\n            index: index,\n            type: type,\n            id: record._id,\n        });\n    });\n\n    return output;\n};\n&lt;?php\n\ndeclare(strict_types=1);\n\nuse Elasticsearch\\ClientBuilder;\nuse Platformsh\\ConfigReader\\Config;\n\n// Create a new config object to ease reading the Platform.sh environment variables.\n// You can alternatively use getenv() yourself.\n$config = new Config();\n\n// Get the credentials to connect to the Elasticsearch service.\n$credentials = $config-&gt;credentials('elasticsearch');\n\ntry {\n    // The Elasticsearch library lets you connect to multiple hosts.\n    // On Platform.sh Standard there is only a single host so just\n    // register that.\n    $hosts = [\n        [\n            'scheme' =&gt; $credentials['scheme'],\n            'host' =&gt; $credentials['host'],\n            'port' =&gt; $credentials['port'],\n        ]\n    ];\n\n    // Create an Elasticsearch client object.\n    $builder = ClientBuilder::create();\n    $builder-&gt;setHosts($hosts);\n    $client = $builder-&gt;build();\n\n    $index = 'my_index';\n    $type = 'People';\n\n    // Index a few document.\n    $params = [\n        'index' =&gt; $index,\n        'type' =&gt; $type,\n    ];\n\n    $names = ['Ada Lovelace', 'Alonzo Church', 'Barbara Liskov'];\n\n    foreach ($names as $name) {\n        $params['body']['name'] = $name;\n        $client-&gt;index($params);\n    }\n\n    // Force just-added items to be indexed.\n    $client-&gt;indices()-&gt;refresh(array('index' =&gt; $index));\n\n\n    // Search for documents.\n    $result = $client-&gt;search([\n        'index' =&gt; $index,\n        'type' =&gt; $type,\n        'body' =&gt; [\n            'query' =&gt; [\n                'match' =&gt; [\n                    'name' =&gt; 'Barbara Liskov',\n                ],\n            ],\n        ],\n    ]);\n\n    if (isset($result['hits']['hits'])) {\n        print &lt;&lt;&lt;TABLE\n&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;&lt;th&gt;ID&lt;/th&gt;&lt;th&gt;Name&lt;/th&gt;&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\nTABLE;\n        foreach ($result['hits']['hits'] as $record) {\n            printf(\"&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;\\n\", $record['_id'], $record['_source']['name']);\n        }\n        print \"&lt;/tbody&gt;\\n&lt;/table&gt;\\n\";\n    }\n\n    // Delete documents.\n    $params = [\n        'index' =&gt; $index,\n        'type' =&gt; $type,\n    ];\n\n    $ids = array_map(function($row) {\n        return $row['_id'];\n    }, $result['hits']['hits']);\n\n    foreach ($ids as $id) {\n        $params['id'] = $id;\n        $client-&gt;delete($params);\n    }\n\n} catch (Exception $e) {\n    print $e-&gt;getMessage();\n}\nimport elasticsearch\nfrom platformshconfig import Config\n\ndef usage_example():\n\n    # Create a new Config object to ease reading the Platform.sh environment variables.\n    # You can alternatively use os.environ yourself.\n    config = Config()\n\n    # Get the credentials to connect to the Elasticsearch service.\n    credentials = config.credentials('elasticsearch')\n\n    try:\n        # The Elasticsearch library lets you connect to multiple hosts.\n        # On Platform.sh Standard there is only a single host so just register that.\n        hosts = {\n            \"scheme\": credentials['scheme'],\n            \"host\": credentials['host'],\n            \"port\": credentials['port']\n        }\n\n        # Create an Elasticsearch client object.\n        client = elasticsearch.Elasticsearch([hosts])\n\n        # Index a few documents\n        es_index = 'my_index'\n        es_type = 'People'\n\n        params = {\n            \"index\": es_index,\n            \"type\": es_type,\n            \"body\": {\"name\": ''}\n        }\n\n        names = ['Ada Lovelace', 'Alonzo Church', 'Barbara Liskov']\n\n        ids = {}\n\n        for name in names:\n            params['body']['name'] = name\n            ids[name] = client.index(index=params[\"index\"], doc_type=params[\"type\"], body=params['body'])\n\n        # Force just-added items to be indexed.\n        client.indices.refresh(index=es_index)\n\n        # Search for documents.\n        result = client.search(index=es_index, body={\n            'query': {\n                'match': {\n                    'name': 'Barbara Liskov'\n                }\n            }\n        })\n\n        table = '''&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;&lt;th&gt;ID&lt;/th&gt;&lt;th&gt;Name&lt;/th&gt;&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;'''\n\n        if result['hits']['hits']:\n            for record in result['hits']['hits']:\n                table += '''&lt;tr&gt;&lt;td&gt;{0}&lt;/td&gt;&lt;td&gt;{1}&lt;/td&gt;&lt;tr&gt;\\n'''.format(record['_id'], record['_source']['name'])\n            table += '''&lt;/tbody&gt;\\n&lt;/table&gt;\\n'''\n\n        # Delete documents.\n        params = {\n            \"index\": es_index,\n            \"type\": es_type,\n        }\n\n        for name in names:\n            client.delete(index=params['index'], doc_type=params['type'], id=ids[name]['_id'])\n\n        return table\n\n    except Exception as e:\n        return e\n\n\nnote\nWhen you create an index on Elasticsearch, you should not specify number_of_shards and number_of_replicas settings in your Elasticsearch API call. These values will be set automatically based on available resources.\n\nAuthentication\nBy default, Elasticsearch has no authentication.  No username or password is required to connect to it.\nStarting with Elasticsearch 7.2 you may optionally enable HTTP Basic authentication.  To do so, include the following in your services.yaml1 configuration:\nsearch:\n    type: elasticsearch:7.2\n    disk: 2048\n    configuration:\n        authentication:\n            enabled: true\n\nThat will enable mandatory HTTP Basic auth on all requests.  The credentials will be available in any relationships that point at that service, in the username and password properties, respectively.\nThis functionality is generally not required if Elasticsearch is not exposed on it own public HTTP route.  However, certain applications may require it, or it allows you to safely expose Elasticsearch directly to the web.  To do so, add a route to routes.yaml that has search:http as its upstream (where search is whatever you named the service in services.yaml).\nPlugins\nThe Elasticsearch 2.4 and later services offer a number of plugins.  To enable them, list them under the configuration.plugins key in your services.yaml file, like so:\nsearch:\n    type: \"elasticsearch:7.2\"\n    disk: 1024\n    configuration:\n        plugins:\n            - analysis-icu\n            - lang-python\n\nIn this example you'd have the ICU analysis plugin and Python script support plugin.\nIf there is a publicly available plugin you need that is not listed here, please contact our support team.\nAvailable plugins\nThis is the complete list of official Elasticsearch plugins that can be enabled:\n\n\n\nPlugin\nDescription\n2.4\n5.2\n5.4\n6.5\n7.2\n\n\n\n\nanalysis-icu\nSupport ICU Unicode text analysis\n*\n*\n*\n*\n*\n\n\nanalysis-nori\nIntegrates Lucene nori analysis module into Elasticsearch\n\n\n\n*\n*\n\n\nanalysis-kuromoji\nJapanese language support\n*\n*\n*\n*\n*\n\n\nanalysis-smartcn\nSmart Chinese Analysis Plugins\n*\n*\n*\n*\n*\n\n\nanalysis-stempel\nStempel Polish Analysis Plugin\n*\n*\n*\n*\n*\n\n\nanalysis-phonetic\nPhonetic analysis\n*\n*\n*\n*\n*\n\n\nanalysis-ukrainian\nUkrainian language support\n\n*\n*\n*\n*\n\n\ncloud-aws\nAWS Cloud plugin, allows storing indices on AWS S3\n*\n\n\n\n\n\n\ndelete-by-query\nSupport for deleting documents matching a given query\n*\n\n\n\n\n\n\ndiscovery-multicast\nAbility to form a cluster using TCP/IP multicast messages\n*\n\n\n\n\n\n\ningest-attachment\nExtract file attachments in common formats (such as PPT, XLS, and PDF)\n\n*\n*\n*\n*\n\n\ningest-user-agent\nExtracts details from the user agent string a browser sends with its web requests\n\n*\n*\n*\n\n\n\nlang-javascript\nJavascript language plugin, allows the use of Javascript in Elasticsearch scripts\n\n*\n*\n\n\n\n\nlang-python\nPython language plugin, allows the use of Python in Elasticsearch scripts\n*\n*\n*\n\n\n\n\nmapper-annotated-text\nAdds support for text fields with markup used to inject annotation tokens into the index\n\n\n\n*\n*\n\n\nmapper-attachments\nMapper attachments plugin for indexing common file types\n*\n*\n*\n\n\n\n\nmapper-murmur3\nMurmur3 mapper plugin for computing hashes at index-time\n*\n*\n*\n*\n*\n\n\nmapper-size\nSize mapper plugin, enables the _size meta field\n*\n*\n*\n*\n*\n\n\nrepository-s3\nSupport for using S3 as a repository for Snapshot/Restore\n\n*\n*\n*\n*\n\n\n\nUpgrading\nThe Elasticsearch data format sometimes changes between versions in incompatible ways.  Elasticsearch does not include a data upgrade mechanism as it is expected that all indexes can be regenerated from stable data if needed.  To upgrade (or downgrade) Elasticsearch you will need to use a new service from scratch.\nThere are two ways of doing that.\nDestructive\nIn your services.yaml file, change the version of your Elasticsearch service and its name.  Then update the name in the .platform.app.yaml relationships block.\nWhen you push that to Platform.sh, the old service will be deleted and a new one with the name name created, with no data.  You can then have your application reindex data as appropriate.\nThis approach is simple but has the downside of temporarily having an empty Elasticsearch instance, which your application may or may not handle gracefully, and needing to rebuild your index afterward.  Depending on the size of your data that could take a while.\nTransitional\nFor a transitional approach you will temporarily have two Elasticsearch services.  Add a second Elasticsearch service with the new version a new name and give it a new relationship in .platform.app.yaml.  You can optionally run in that configuration for a while to allow your application to populate indexes in the new service as well.\nOnce you're ready to cut over, remove the old Elasticsearch service and relationship.  You may optionally have the new Elasticsearch service use the old relationship name if that's easier for your application to handle.  Your application is now using the new Elasticsearch service.\nThis approach has the benefit of never being without a working Elasticsearch instance.  On the downside, it requires two running Elasticsearch servers temporarily, each of which will consume resources and need adequate disk space.  Depending on the size of your data that may be a lot of disk space.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Services", "title": "Headless Chrome", "url": "/configuration/services/headless-chrome.html", "documentId": "0f737c72d5009ca7b115bcab82ff97754632f346", "text": "\n                        \n                            \n                                \n                                \n                                Headless Chrome\nHeadless Chrome is a headless browser that can be configured on projects like any other service on Platform.sh. You can interact with the headless-chrome service container using Puppeteer, a Node library that provides an API to control Chrome over the DevTools Protocol.\nPuppeteer can be used to generate PDFs and screenshots of web pages, automate form submission, and test your project's UI. You can find out more information about using Puppeteer on GitHub or in their documentation.\nSupported versions\n\n73\n\nRelationship\nThe format exposed in the $PLATFORM_RELATIONSHIPS environment variable:\n{\n    \"service\": \"headless\",\n    \"ip\": \"169.254.73.96\",\n    \"hostname\": \"3rxha4e2w4yv36lqlypy7qlkza.headless.service._.eu-3.platformsh.site\",\n    \"cluster\": \"moqwtrvgc63mo-master-7rqtwti\",\n    \"host\": \"headless.internal\",\n    \"rel\": \"http\",\n    \"scheme\": \"http\",\n    \"type\": \"chrome-headless:73\",\n    \"port\": 9222\n}\n\nRequirements\nPuppeteer requires at least Node.js version 6.4.0, while using the async and await examples below requires Node 7.6.0 or greater.\nUsing the Platform.sh Config Reader library requires Node.js 10 or later.\nOther languages\nIt will be necessary to upgrade the version of Node.js in other language containers before using Puppeteer. You can use Node Version Manager or NVM to change or update the version available in your application container by following the instructions in the Alternate Node.js install documentation.\nUsage example\nIn your .platform/services.yaml:\nheadlessbrowser:\n    type: chrome-headless:73\nIn your .platform.app.yaml:\nrelationships:\n    chromeheadlessbrowser: \"headlessbrowser:http\"\nAfter configuration, include Puppeteer as a dependency in your package.json:\n\"puppeteer\": \"^1.14.0\"\n\nUsing the Node.js Config Reader library, you can retrieve formatted credentials for connecting to headless Chrome with Puppeteer:\nconst platformsh = require('platformsh-config');\n\nlet config = platformsh.config();\nconst credentials = config.credentials('chromeheadlessbrowser');\nand use them to define the browserURL parameter of puppeteer.connect() within an async function:\nexports.takeScreenshot = async function (url) {\n    try {\n        // Connect to chrome-headless using pre-formatted puppeteer credentials\n        const formattedURL = config.formattedCredentials('chromeheadlessbrowser', 'puppeteer');\n        const browser = await puppeteer.connect({browserURL: formattedURL});\n\n        ...\n\n        return browser\n\n    } catch (e) {\n        return Promise.reject(e);\n    }\n};\nPuppeteer allows your application to create screenshots, emulate a mobile device, generate PDFs, and much more.\nYou can find some useful examples of using headless Chrome and Puppeteer on Platform.sh on the Community Portal:\n\nHow to take screenshots using Puppeteer and Headless Chrome\nHow to generate PDFs using Puppeteer and Headless Chrome\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Configuration", "title": "Services (services.yaml)", "url": "/configuration/services.html", "documentId": "dbd3b3d0776e85530ba52a84a397a513d64e0ede", "text": "\n                        \n                            \n                                \n                                \n                                Configure Services\nPlatform.sh allows you to completely define and configure the topology and services you want to use on your project.\nUnlike other PaaS services, Platform.sh is batteries included which means that you don't need to subscribe to an external service to get a cache or a search engine. And that those services are managed. When you back up your project, all of the services are backed-up.\nServices are configured through the .platform/services.yaml file you will need to commit to your Git repository. This section describes specifics you might want to know about for each service.\n\nIf you don't have a .platform folder, you need to create one:\nmkdir .platform\ntouch .platform/services.yaml\n\nHere is an example of a services.yaml file:\ndatabase1:\n  type: mysql:10.1\n  disk: 2048\n\ndatabase2:\n  type: postgresql:9.6\n  disk: 1024\n\nName\nThe name you want to give to your service. You are free to name each service as you wish (lowercase alphanumeric only).\nWARNING: Because we support multiple services of the same type (you can have 3 different MySQL instances), changing the name of the service in services.yaml will be interpreted as destroying the existing service and creating a new one. This will make all the data in that service disappear forever. Remember to always back up your environment in which you have important data before modifying this file.\nType\nThe type of your service. It's using the format type:version.\nIf you specify a version number which is not available, you'll see this error when pushing your changes:\nValidating configuration files.\nE: Error parsing configuration files:\n    - services.mysql.type: 'mysql:5.6' is not a valid service type.\n\nService types and their supported versions include:\n\n\n\nService\ntype\nSupported version\n\n\n\n\nHeadless Chrome\nchrome-headless\n73\n\n\nElasticsearch\nelasticsearch\n6.5, 7.2\n\n\nInfluxDB\ninfluxdb\n1.2, 1.3, 1.7\n\n\nKafka\nkafka\n2.1, 2.2, 2.3, 2.4\n\n\nMariaDB\nmariadb\n10.0, 10.1, 10.2, 10.3, 10.4\n\n\nMemcached\nmemcached\n1.4, 1.5, 1.6\n\n\nMongoDB\nmongodb\n3.0, 3.2, 3.4, 3.6\n\n\nNetwork Storage\nnetwork-storage\n1.0\n\n\nOracle MySQL\noracle-mysql\n5.7, 8.0\n\n\nPostgreSQL\npostgresql\n9.6, 10, 11, 12\n\n\nRabbitMQ\nrabbitmq\n3.5, 3.6, 3.7, 3.8\n\n\nRedis\nredis\n3.2, 4.0, 5.0\n\n\nSolr\nsolr\n3.6, 4.1, 6.3, 6.6, 7.6, 7.7, 8.0, 8.4\n\n\nVarnish\nvarnish\n5.6, 6.0\n\n\n\nDisk\nThe disk attribute is the size of the persistent disk (in MB) allocated to the service.\nFor example, the current default storage amount per project is 5GB (meaning 5120MB) which you can distribute between your application (as defined in .platform.app.yaml) and each of its services.  For memory-resident-only services such as memcache or redis, the disk key is not available and will generate an error if present.\n\nnotes\nCurrently we do not support downsizing the persistent disk of a service.\n\nSize\nBy default, Platform.sh will allocate CPU and memory resources to each container automatically.  Some services are optimized for high CPU load, some for high memory load.  By default, Platform.sh will try to allocate the largest \"fair\" size possible to all services, given the available resources on the plan.  That is not always optimal, however, and you can customize that behavior on any service or on any application container.  See the application sizing page for more details.\nService timezones\nAll services have their system timezone set to UTC by default.  In most cases that is the best option.  For some applications it's possible to change the application timezone, which will affect only the running application itself.\n\nMySQL - You can change the per-connection timezone by running SQL SET time_zone = &lt;timezone&gt;;.\nPostgreSQL - You can change the timezone of current session by running SQL SET TIME ZONE &lt;timezone&gt;;.\n\nUsing the services\nIn order for a service to be available to an application in your project (Platform.sh supports not only multiple backends but also multiple applications in each project) you will need to refer to it in the .platform.app.yaml file which configures the relationships between applications and services.\nEndpoints\nAll services offer one or more endpoints.  An endpoint is simply a named set of credentials that can be used to access the service from other applications or services in your project.  Only some services support multiple user-defined endpoints.  If you do not specify one then one will be created with a standard defined name, generally the name of the service type (e.g., mysql or solr).  An application container, defined by a .platform.app.yaml file, always exposes and endpoint named http to allow the router to forward requests to it.\nWhen defining relationships in a configuration file you will always address a service as &lt;servicename&gt;:&lt;endpoint&gt;.  See the appropriate service page for details on how to configure multiple endpoints for each service that supports it.\nConnecting to a service\nOnce a service is running and exposed as a relationship, its appropriate credentials (host name, username if appropriate, etc.) will be exposed through the PLATFORM_RELATIONSHIPS environment variable.  The structure of each is documented on the appropriate service's page, along with sample code for how to connect to it from your application. Note that different applications manage configuration differently so the exact code will vary from one application to another.\nBe aware that the keys in the PLATFORM_RELATIONSHIPS structure are fixed but the values they hold may change on any deployment or restart.  Never hard-code connection credentials for a service into your application.  You should re-check the environment variable every time your script or application starts.\nAccess to the database or other services is only available from within the cluster.  For security reasons they cannot be accessed directly.  However, they can be accessed over an SSH tunnel.  There are two ways to do so.  (The example here uses MariaDB but the process is largely identical for any service.)\nObtaining service credentials\nIn either case, you will also need the service credentials.  For that, run platform relationships.  That will give output similar to the following:\nredis:\n    -\n        service: rediscache\n        ip: 246.0.82.19\n        cluster: jyu7waly36ncj-master-7rqtwti\n        host: redis.internal\n        rel: redis\n        scheme: redis\n        port: 6379\ndatabase:\n    -\n        username: user\n        scheme: mysql\n        service: mysqldb\n        ip: 246.0.80.37\n        cluster: jyu7waly36ncj-master-7rqtwti\n        host: database.internal\n        rel: mysql\n        path: main\n        query:\n            is_master: true\n        password: ''\n        port: 3306\n\nThat indicates that the database relationship can be accessed at host database.internal, user user, and an empty password.  The path key contains the database name, main.  The other values can be ignored.\n\nnote\nWhen using the default endpoint on MySQL/MariaDB, the password is usually empty. It will be filled in if you define any custom endpoints. As there is only the one user and port access is tightly restricted anyway the lack of a password does not create a security risk.\n\nOpen an SSH tunnel directly\nThe first option is to open an SSH tunnel for all of your services.  You can do so with the Platform.sh CLI, like so:\n$ platform tunnel:open\nSSH tunnel opened on port 30000 to relationship: redis\nSSH tunnel opened on port 30001 to relationship: database\nLogs are written to: ~/.platformsh/tunnels.log\n\nList tunnels with: platform tunnels\nView tunnel details with: platform tunnel:info\nClose tunnels with: platform tunnel:close\n\nThe tunnel:open command will connect all relationships defined in the .platform.app.yaml file to local ports, starting at 30000.  You can then connect to those ports on localhost using the program of your choice.\nIn this example, we would connect to localhost:30001, database name main, with username user and an empty password.\nThe platform tunnels command will list all open tunnels:\n+-------+---------------+-------------+-----------+--------------+\n| Port  | Project       | Environment | App       | Relationship |\n+-------+---------------+-------------+-----------+--------------+\n| 30000 | a43m75zns6k4c | master      | [default] | redis        |\n| 30001 | a43m75zns6k4c | master      | [default] | database     |\n+-------+---------------+-------------+-----------+--------------+\n\nUsing an application tunnel\nAlternatively, many database applications (such as MySQL Workbench and similar tools) support establishing their own SSH tunnel.  Consult the documentation for your application for how to enter SSH credentials, including telling it where your SSH private key is.  (Platform.sh does not support password-based SSH authentication.)\nTo get the values to use, the easiest way is to run platform ssh --pipe.  That will return a command line that can be used to connect over SSH, from which you can pull the appropriate information.  For example:\njyu7waly36ncj-master-7rqtwti--app@ssh.us.platform.sh\nIn this case, the username is jyu7waly36ncj-master-7rqtwti--app and the host is ssh.us.platform.sh.  Note that the host will vary per region, and the username will vary per-environment.\nIn this example, we would configure our database application to setup a tunnel to ssh.us.platform.sh as user jyu7waly36ncj-master-7rqtwti--app, and then connect to the database on host database.internal, username user, empty password, and database name main.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Routes", "title": "Server Side Includes", "url": "/configuration/routes/ssi.html", "documentId": "51705bac1092e843c932fd059dfcbb9809225211", "text": "\n                        \n                            \n                                \n                                \n                                Server Side Includes\nServer side includes is a powerful mechanism by which you can at the same time leverage caching and serve dynamic content.\nYou can activate or deactivate SSI on a per-route basis in your .platform/routes.yaml for example:\n\"https://{default}/\":\n    type: upstream\n    upstream: \"app:http\"\n    cache:\n      enabled: false\n    ssi:\n        enabled: true\n\"https://{default}/time.php\":\n    type: upstream\n    upstream: \"app:http\"\n    cache:\n      enabled: true\n\nIt allows you to include in your HTML response directives that will make the server \"fill-in\" parts of the HTML respecting the caching you setup.\nFor example you could in a dynamic non-cached page include a block that would have been cached for example in the /index.php page we would have:\n&lt;?php\necho date(DATE_RFC2822);\n?&gt;\n&lt;!--#include virtual=\"time.php\" --&gt;\n\nand in time.php we had\n&lt;?php\nheader(\"Cache-Control: max-age=600\");\necho date(DATE_RFC2822);\n\nAnd you visit the home page you will see, as you refresh the page, the time on the top will continue to change, while the one on the bottom will only change every 600 seconds.\nFor more on SSI functionality see the nginx documentation.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Routes", "title": "Redirects", "url": "/configuration/routes/redirects.html", "documentId": "11180499f9ac73e55132c7b394ad9a6108c7bf86", "text": "\n                        \n                            \n                                \n                                \n                                Redirects\nManaging redirection rules is a common requirement for web applications, especially in cases where you do not want to lose incoming links that have changed or been removed over time. You can manage redirection rules on your Platform.sh projects in two different ways, which we describe here. If neither of these options satisfy your redirection needs, you can still implement redirects directly from within your application, which if implemented with the appropriate caching headers would be almost as efficient as using the configuration options provided by Platform.sh.\nWhole-route redirects\nUsing whole-route redirects, you can define very basic routes in your .platform/routes.yaml file whose sole purpose is to redirect. A typical use case for this type of route is adding or removing a www. prefix to your domain, as the following example shows:\nhttps://{default}/:\n    type: redirect\n    to: https://www.{default}/\n\nPartial redirects\nIn the .platform/routes.yaml file you can also add partial redirect rules to existing routes:\nhttps://{default}/:\n  # [...]\n  redirects:\n    expires: 1d\n    paths:\n      '/from':\n        to: 'https://example.com/'\n      '^/foo/(.*)/bar':\n        to: 'https://example.com/$1'\n        regexp: true\n\nThis format is more rich and works with any type of route, including routes served directly by the application.\nTwo keys are available under redirects:\n\nexpires: optional, the duration the redirect will be cached. Examples of valid values include 3600s, 1d, 2w, 3m.\npaths: the paths to apply redirections to.\n\nEach rule under paths is defined by its key describing the expression to match against the request path and a value object describing both the destination to redirect to with detail on how to handle the redirection. The value object is defined with the following keys:\n\nto: required, a relative URL - '/destination', or absolute URL - 'https://example.com/'.\nregexp: optional, defaults to false. Specifies whether the path key should be interpreted as a PCRE regular expression. In the following example, a request to https://example.com/foo/a/b/c/bar would redirect to https://example.com/a/b/c:\nhttps://{default}/:\n  type: upstream\n  redirects:\n    paths:\n      '^/foo/(.*)/bar':\n         to: 'https://example.com/$1'\n         regexp: true\n\nNote that special arguments in the to statement are also valid when regexp is set to true:\n\n$is_args will evaluate to ? or empty string\n$args will evaluate to the full query string if any\n$arg_foo will evaluate to the value of the query parameter foo\n$uri will evaluate to the full URI of the request.\n\n\nprefix: optional, specifies whether we should redirect both the path and all its children or just the path itself. Defaults to true, but not supported if regexp is true. For example,\nhttps://{default}/:\n  type: upstream\n  redirects:\n    paths:\n      '/from':\n         to: 'https://{default}/to'\n         prefix: true\n\nwith prefix set to true, /from will redirect to /to and /from/another/path will redirect to /to/another/path.\nIf prefix is set to false then /from will trigger a redirect, but /from/another/path will not.\n\nappend_suffix: optional, determines if the suffix is carried over with the redirect. Defaults to true, but not supported if regexp is true or if prefix is false.\nIf we redirect with append_suffix set to false, for example, then the following\nhttps://{default}/:\n  type: upstream\n  redirects:\n    paths:\n      '/from':\n         to: 'https://{default}/to'\n         append_suffix: false\n\nwould result in /from/path/suffix redirecting to just /to. If append_suffix was left on its default value of true, then /from/path/suffix would have redirected to /to/path/suffix.\n\ncode: optional, HTTP status code. Valid status codes are 301, 302, 307, and 308. Defaults to 302.\nhttps://{default}/:\n  type: upstream\n  redirects:\n    paths:\n      '/from':\n        to: 'https://example.com/'\n        code: 308\n      '/here':\n        to: 'https://example.com/there'\n\nIn this example, redirects from /from would use a 308 HTTP status code, but redirects from /here would default to 302.\n\nexpires: optional, the duration the redirect will be cached for. Defaults to the expires value defined directly under the redirects key, but at this level we can fine-tune the expiration of individual partial redirects:\nhttps://{default}/:\n  type: upstream\n  redirects:\n    expires: 1d\n    paths:\n      '/from':\n        to: 'https://example.com/'\n      '/here':\n        to: 'https://example.com/there'\n        expires: 2w\n\nIn this example, redirects from /from would be set to expire in one day, but redirects from /here would expire in two weeks.\n\n\nApplication-driven redirects\nIf neither of the above options satisfy your redirection needs, you can still implement redirects directly in your application. If sent with the appropriate caching headers, this is nearly as efficient as implementing the redirect through one of the two configurations described above. Implementing application-driven redirects depends on your own code or framework and is beyond the scope of this documentation.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Routes", "title": "HTTP Cache", "url": "/configuration/routes/cache.html", "documentId": "ffc6984f6c1357dd79ed7ba73af9febdcdeb4b23", "text": "\n                        \n                            \n                                \n                                \n                                HTTP cache\nPlatform.sh supports HTTP caching at the server level. Caching is enabled by default, but is only applied to GET and HEAD requests.\nThe cache can be controlled using the cache key in your .platform/routes.yaml file. \nIf a request is cacheable, Platform.sh builds a cache key from several request properties and stores the response associated with this key. When a request comes with the same cache key, the cached response is reused.\nWhen caching is on...\n\nyou can configure cache behaviour for different location blocks in your .platform.app.yaml;\nthe router will respect whatever cache headers are sent by the application;\ncookies will bypass the cache;\nresponses with the Cache-Control header set to Private, No-Cache, or No-Store are not cached. \n\nBasic usage\nThe HTTP cache is enabled by default, however you may wish to override this behaviour.\nTo configure the HTTP cache, add a cache key to your route in .platform/routes.yaml. You may like to start with the defaults:\nhttps://{default}/:\n    type: upstream\n    upstream: app:http\n    cache:\n        enabled: true\n        default_ttl: 0\n        cookies: ['*']\n        headers: ['Accept', 'Accept-Language']\n\nExample\nIn this example, requests will be cached based on the URI, the Accept header, Accept-Language header, and X-Language-Locale header; Any response that lacks a Cache-Control header will be cached for 60 seconds; and the presence of any cookie in the request will disable caching of that response.\nhttps://{default}/:\n    type: upstream\n    upstream: app:http\n    cache:\n        enabled: true\n        headers: ['Accept', 'Accept-Language', 'X-Language-Locale']\n        cookies: ['*']\n        default_ttl: 60\n\nHow it works\nThe cache key\nIf a request is cacheable, Platform.sh builds a cache key from several request properties and stores the response associated with this key. When a request comes with the same cache key, the cached response is reused.\nThere are two parameters that let you control this key: headers and cookies.\nThe default value for these keys are the following:\ncache:\n    enabled: true\n    cookies: ['*']\n    headers: ['Accept', 'Accept-Language']\n\nDuration\nThe cache duration is decided based on the Cache-Control response header value. If no Cache-Control header is in the response, then the value of default_ttl key is used.\nConditional requests\nConditional requests using If-Modified-Since and If-None-Match are both supported. Our web server does not honor the Pragma request header. \nCache revalidation\nWhen the cache is expired (indicated by Last-Modified header in the response) the web server will send a request to your application with If-Modified-Since header.\nIf the If-None-Match header is sent in the conditional request when Etag header is set in the cached response, your application can extend the validity of the cache by replying HTTP 304 Not Modified.\nFlushing\nThe HTTP cache does not support a complete cache flush, however, you can invalidate the cache by setting cache:false.\nCache configuration properties\nenabled\nTurns the cache on or off for a route.\n\nType: Boolean\nRequired: Yes\nValues\n\ntrue: enable the cache for this route [default, but only if the cache key is not actually specified]\nfalse: disable the cache for this route \n\n\nheaders\nAdds specific header fields to the cache key, enabling caching of separate responses for those headers.\nFor example, if the headers key is the following, Platform.sh will cache a different response for each value of the Accept HTTP request header only:\ncache:\n  enabled: true\n  headers: [\"Accept\"]\n\n\nType: List\nValues:\n\n['Accept', 'Accept-Language']: Cache on Accept &amp; Accept-Language [default]\n\n\nHeader behaviors\nThe cache is only applied to GET and HEAD requests. Some headers trigger specific behaviours in the cache.\n\n\n\nHeader field\nCache behavior\n\n\n\n\nCache-Control\nResponses with the Cache-Control header set to Private, No-Cache, or No-Store are not cached. All other values override default_ttl.\n\n\nVary\nA list of header fields to be taken into account when constructing the cache key. Multiple header fields can be listed, separted by commas. The Cache key is the union of the values of the Header fields listed in Vary header, and whatever's listed in the routes.yaml file.\n\n\nSet-Cookie\nNot cached\n\n\nAccept-Encoding, Connection, Proxy-Authorization, TE, Upgrade\nNot allowed, and will throw an error\n\n\nCookie\nNot allowed, and will throw an error. Use the cookies value, instead.\n\n\nPragma\nIgnored\n\n\n\nA full list of HTTP headers is available on Wikipedia.\ncookies\nA whitelist of cookie names to include values for in the cache key. \nAll cookies will bypass the cache when using the default (['*']) or if the Set-Cookie header is present. \nFor example, for the cache key to depend on the value of the foo cookie in the request.  Other cookies will be ignored.\ncache:\n  enabled: true\n  cookies: [\"foo\"]\n\n\nType: List\nValues:\n\n['*']: any request with a cookie will bypass the cache [default]\n[]: Ignore all cookies\n['cookie_1','cookie_2']: A whitelist of cookies to include in the cache key. All other cookies are ignored.\n\n\nA cookie value may also be a regular expression.  An entry that begins and ends with a / will be interpreted as a PCRE regular expression to match the cookie name.  For example:\ncache:\n  enabled: true\n  cookies: ['/^SS?ESS/']\n\nWill cause all cookies beginning with SESS or SSESS to be part of the cache key, as a single value.  Other cookies will be ignored for caching.  If your site uses a session cookie as well as 3rd party cookies, say from an analytics service, this is the recommended approach.\ndefault_ttl\nDefines the default time-to-live for the cache, in seconds, for non-static responses, when the response does not specify one. \nThe cache duration is decided based on the Cache-Control response header value. If no Cache-Control header is in the response, then the value of default_ttl is used. If the application code returns a Cache-Control header or if your .platform.app.yaml file is configured to set a cache lifetime, then this value is ignored in favor of the application headers.\nThe default_ttl only applies to non-static responses, that is, those generated your application. \nTo set a cache lifetime for static resources configure that in your .platform.app.yaml file. All static assets will have a Cache-Control header with a max age defaulting to 0 (which is the default for expires in the .platform.app.yaml).\n\nType: integer\nValues:\n\n0: Do not cache [default]. This prevents caching, unless the response specifies a Cache-Control header value.\n\n\nDebugging\nPlatform.sh adds an X-Platform-Cache header to each request which show whether your request is a cache HIT, MISS or BYPASS. This can be useful when trying to determine whether it is your application, the HTTP cache, or another proxy or CDN which is not behaving as expected.\nIf in doubt, disable the cache using cache:false.\nAdvanced caching strategies\nCache per route\nIf you need fine-grained caching, you can set up caching rules for several routes separately:\nhttps://{default}/:\n  type: upstream\n  upstream: app:http\n  cache:\n    enabled: true\n\nhttps://{default}/foo/:\n  type: upstream\n  upstream: app:http\n  cache:\n    enabled: false\n\nhttps://{default}/foo/bar/:\n  type: upstream\n  upstream: app:http\n  cache:\n    enabled: true\n\nWith this configuration, the following routes are cached:\n\nhttps://{default}/\nhttps://{default}/foo/bar/\nhttps://{default}/foo/bar/baz/\n\nAnd the following routes are not cached:\n\nhttps://{default}/foo/\nhttps://{default}/foo/baz/\n\n\nnote\nRegular expressions in routes are not supported.\n\nAllowing only specific cookies\nSome applications use cookies to invalidate cache responses, but expect other cookies to be ignored. This is a simple case of allowing only a subset of cookies to invalidate the cache.\ncache:\n  enabled: true\n  cookies: [\"MYCOOKIE\"]\n\nCache HTTP and HTTPS separately using the Vary header\nSet the Vary header to X-Forwarded-Proto custom request header to render content based on the request protocol (i.e. HTTP or HTTPS). By adding Vary: X-Forwarded-Proto to the response header, HTTP and HTTPS content would be cached separately.\nCache zipped content separately\nUse Vary: Accept-Encoding to serve different content depending on the encoding. Useful for ensuring that gzipped content is not served to clients that can't read it.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Configuration", "title": "Routes (routes.yaml)", "url": "/configuration/routes.html", "documentId": "b7cd2de297656972c733d29464956ae2173a163d", "text": "\n                        \n                            \n                                \n                                \n                                Configure Routes\nPlatform.sh allows you to define the routes used in your environments.\nA route describes how an incoming HTTP request is going to be processed by Platform.sh. The routes are defined using .platform/routes.yaml file in your Git repository.\nIf you don't have one, use the commands below to create it:\n$ mkdir .platform\n$ touch .platform/routes.yaml\n\n\nRoute templates\nThe YAML file is composed of a list of routes and their configuration. A route can either be an absolute URL or a URL template that looks like: http://www.{default}/ or https://{default}/blog where {default} will be substituted by the default fully qualified domain name configured in the project. So if your default domain is example.com, these routes will be resolved to http://www.example.com/ and https://example.com/blog in the master environment.\nPlatform.sh will also generate a domain for every active development environment.  It will receive a domain name based on the region, project ID, branch name, and a per-environment random string. The domain name itself is not guaranteed stable, although the pattern is consistent.\n\nnote\nPlatform.sh supports running multiple applications per environment. The .platform/routes.yaml file defines how to route requests to different applications.\n\nRoute configuration\nEach route can be configured separately. It has the following properties\n\ntype can be:\nupstream serves an application\nIt will then also have an upstream property which will be the name of the application (as defined in .platform.app.yaml), followed by \":http\" (see examples below).\n\n\nredirect redirects to another route\nIt will then be followed by a to property, this defines a HTTP 301 redirect to any URL or another route (see examples below).\n\n\n\n\ncache controls caching behavior of the route.\nssi controls whether Server Side Includes are enabled. For more information: see SSI.\nredirects controls redirect rules associated with the route.\n\n\n\nnote\nFor the moment, the value of upstream is always in the form: &lt;application-name&gt;:http.  &lt;application-name&gt; is the name defined in .platform.app.yaml file.  :php is a deprecated application endpoint; use :http instead.  In the future, Platform.sh will support multiple endpoints per application.\n\nRoute limits\nAlthough there is no fixed limit on the number of routes that can be defined, there is a cap on the size of generated route information.\nThis limitation comes from the Linux kernel, which caps the size of environment variables.\nThe kernel limit on environment variables is 32 pages. Each page is 4k on x86 processors, resulting in a maximum environment variable length of 131072 bytes.\nIf your routes.yaml file would result in too large of a route information value it will be rejected.\nThe full list of generated route information is often much larger than what is literally specified in the routes.yaml file.  For example, by default all HTTPS routes will be duplicated to create an HTTP redirect route.  Also, the {all} placeholder will create two routes (one HTTP, one HTTPS) for each domain that is configured.\nAs a general rule we recommend keeping the defined routes under 100.  Should you find your routes.yaml file rejected due to an excessive size the best alternative is to move any redirect routes to the application rather than relying on the router, or collapsing them into a regular expression-based redirect within a route definition.\nLet's Encrypt also limits an environment to 100 configured domains.  If you try to add more than that some of them will fail to get an SSL certificate.\nRoutes examples\nHere is an example of a basic .platform/routes.yaml file:\n\"https://{default}/\":\n  type: upstream\n  upstream: \"app:http\"\n\"https://www.{default}/\":\n  type: redirect\n  to: \"https://{default}/\"\n\nIn this example, we will route both the apex domain and the www subdomain to an application called \"app\", the www subdomain being redirected to the apex domain using an HTTP 301 Moved Permanently response.\nIn the following example, we are not redirecting from the www subdomain to the apex domain but serving from both:\n\"https://{default}/\":\n    type: upstream\n    upstream: \"app:http\"\n\n\"https://www.{default}/\":\n    type: upstream\n    upstream: \"app:http\"\n\nRoute placeholders\nYou can configure any number of domains on a project when you are ready to make it live.  Only one of them may be set as the \"default\" domain.  In the routes.yaml file a route can be defined either literally or using one of two special placeholders: {default} and {all}.\nThe magic value {default} will be replaced with the production domain name configured as the default on your account in the production branch.  In a non-production branch it will be replaced with the project ID and environment ID so that it is always unique.\nThe magic value {all} will be replaced by all of the domain names configured on the account. That is, if two domains example1.com and example2.com are configured, then a route named https://www.{all}/ will result in two routes in production: https://www.example1.com and https://www.example2.com.  That can be useful in cases when a single application is serving two different websites simultaneously.  In a non-production branch it will be replaced with the project ID and environment ID for each domain, in the same fashion as a static route below.\nIf there are no domains defined on a project (as is typical in development before launch) then the {all} placeholder will behave exactly like the {default} placeholder.\nIt's also entirely possible to use an absolute URL in the route. In that case, it will be used as-is in a production environment.  On a development environment it will be mangled to include the project ID and environment name.  For example:\n\"https://www.example.com/\":\n    type: upstream\n    upstream: \"app:http\"\n\"https://blog.example.com/\":\n    type: upstream\n    upstream: \"blog:http\"\n\nIn this case, there are two application containers app and blog.  In a production environment, they would be accessible at www.example.com and blog.example.com, respectively.  On a development branch named sprint, however, they would be accessible at URLs something like:\nhttps://www.example.com.sprint-7onpvba-tvh56f275i3um.eu-2.platformsh.site/\nhttps://blog.example.com.sprint-7onpvba-tvh56f275i3um.eu-2.platformsh.site/\n\nIf your project involves only a single apex domain with one app or multiple apps under subdomains, it's generally best to use the {default} placeholder.  If you are running multiple applications on different apex domains then you will need to use a static domain for all but one of them.\nPlease note that when there are two routes sharing the same HTTP scheme, domain, and path, where the first route is using the {default} placeholder and the other is using the {all} placeholder, the route using {default} takes precedence.\nRoute identifiers\nAll routes defined for an environment are available to the application in its PLATFORM_ROUTES environment variable, which contains a base64-encoded JSON object. This object can be parsed by the language of your choice to give your application access to the generated routes.\nWhen routes are generated, all placeholders will be replaced with appropriate domains names, and depending on your configuration, additional route entries may be generated (e.g. automatic HTTP to HTTPS redirects). To make it easier to locate routes in a standardized fashion, you may specify an id key on each route which remains stable across environments. You may also specify a single route as primary, which will cause it to be highlighted in the web management console but will have no impact on the runtime environment.\nConsider this routes.yaml configuration example:\n\"https://site1.{default}/\":\n    type: upstream\n    upstream: 'site1:http'\n\n\"https://site2.{default}/\":\n    type: upstream\n    id: 'the-second'\n    upstream: 'site2:http'\n\nThis example defines two routes, on two separate subdomains, pointing at two separate app containers. (However, they could just as easily be pointing at the same container for our purposes).  On a branch named test, the route array in PHP would look like this:\nArray\n(\n    [https://site1.test-t6dnbai-abcdef1234567.us-2.platformsh.site/] =&gt; Array\n        (\n            [primary] =&gt; 1\n            [id] =&gt;\n            [type] =&gt; upstream\n            [upstream] =&gt; site1\n            [original_url] =&gt; https://site1.{default}/\n            // ...\n        )\n\n    [https://site2.test-t6dnbai-abcdef1234567.us-2.platformsh.site/] =&gt; Array\n        (\n            [primary] =&gt;\n            [id] =&gt; the-second\n            [type] =&gt; upstream\n            [upstream] =&gt; site2\n            [original_url] =&gt; https://site2.{default}/\n            // ...\n        )\n    [http://site1.test-t6dnbai-abcdef1234567.us-2.platformsh.site/] =&gt; Array\n        (\n            [to] =&gt; https://site1.test-t6dnbai-abcdef1234567.us-2.platformsh.site/\n            [original_url] =&gt; http://site1.{default}/\n            [type] =&gt; redirect\n            [primary] =&gt;\n            [id] =&gt;\n        )\n\n    [http://site2.test-t6dnbai-abcdef1234567.us-2.platformsh.site/] =&gt; Array\n        (\n            [to] =&gt; https://site2.test-t6dnbai-abcdef1234567.us-2.platformsh.site/\n            [original_url] =&gt; http://site2.{default}/\n            [type] =&gt; redirect\n            [primary] =&gt;\n            [id] =&gt;\n        )\n)\n\n(Some keys omitted for space.)  Note that the site2 HTTPS route has an id specified as the-second while other routes have no ID. Futhermore, because we did not specify a primary route, the first non-redirect route defined is marked as the primary route by default. In each case, the original_url specified in the configuration file is accessible if desired.\nThat makes it straightforward to look up the domain of a particular route, regardless of what branch it's on, from within application code.  For example, the following PHP function will retrieve the domain for a specific route id, regardless of the branch it's on.\nfunction get_domain_for(string $id) {\n  foreach ($routes as $domain =&gt; $route) {\n    if ($route['id'] == $id) {\n      return $domain;\n    }\n  }\n}\n\nThat can be used, for example, for inbound request whitelisting, a feature of many PHP frameworks.\nRoute attributes\nRoute attributes are an arbitrary key/value pair attached to a route.  This metadata does not have any impact on Platform.sh, but will be available in the route definition structure in $PLATFORM_ROUTES.\n\"http://{default}/\":\n    type: upstream\n    upstream: \"app:http\"\n    attributes:\n        \"foo\": \"bar\"\nAttributes will appear in the routes data for PHP like so:\n[https://site1.test-t6dnbai-abcdef1234567.us-2.platformsh.site/] =&gt; Array\n    (\n        [primary] =&gt; 1\n        [id] =&gt;\n        [type] =&gt; upstream\n        [upstream] =&gt; site1\n        [original_url] =&gt; https://site1.{default}/\n        [attributes] =&gt; Array\n            (\n                [foo] =&gt; bar\n            )\n        // ...\n    )\n\nThese extra attributes may be used to \"tag\" routes in more complex scenarios that can then be read by your application.\nConfiguring routes on the management console\nRoutes can also be configured using the management console in the routes section of the environment settings. If you have edited the routes via the management console, you will have to git pull the updated .platform/routes.yaml file from us.\nCLI Access\nYou can get a list of the configured routes of an environment by running platform environment:routes.\n\nIf you need to see more detailed info, such as cache and ssi, use platform route:get\nWildcard routes\nPlatform.sh supports wildcard routes, so you can map multiple subdomains to the same application. This works both for redirect and upstream routes. You can simply prefix the route with a star (*), for example *.example.com, and HTTP request to www.example.com, blog.example.com, us.example.com will all get routed to the same endpoint.\nFor your master environment, this would function as a catch-all domain once you added the parent domain to the project settings.\nFor development environments, we will also be able to handle this. Here is how:\nLet's say we have a project on the EU cluster whose ID is \"vmwklxcpbi6zq\" and we created a branch called \"add-theme\". It's environment name will be similar to add-theme-def123.  The generated apex domain of this environment will be add-theme-def123-vmwklxcpbi6zq.eu.platform.sh. If we have a http://*.{default}/ route defined, the generated route will be http://*.add-theme-def123-vmwklxcpbi6zq.eu.platform.sh/. This means you could put any subdomain before the left-most . to reach your application. HTTP request to both http://foo.add-theme-def123-vmwklxcpbi6zq.eu.platform.sh/ and http://bar.add-theme-def123-vmwklxcpbi6zq.eu.platform.sh/ URLs will be routed to your application properly. However, request to http://*.add-theme-def123-vmwklxcpbi6zq.eu.platform.sh/ will not be routed since it is not a legitimate domain name.\nBe aware, however, that we do not support Let's Encrypt wildcard certificates (they would need DNS validation).  That means if you want to use a wildcard route and protect it with HTTPS you will need to provide a custom TLS certificate.\n\nnote\nIn projects created before November 2017 the . in subdomains was replaced with a triple dash (---).  It was switched to preserve . to simplify SSL handling and improve support for longer domains.  If your project was created before November 2017 then it will still use --- to the left of the environment name.  If you wish to switch to dotted-domains please file a support ticket and we can do that for you.  Be aware that doing so may change the domain name that your production domain name should CNAME to.\n\nWebSocket routes\nTo use WebSocket on a route, cache must be disabled because WebSocket is incompatible with buffering, which is a requirement of caching on our router.  Here is an example to define a route that serves WebSocket:\n\"https://{default}/ws\":\n    type: upstream\n    upstream: \"app:http\"\n    cache:\n        enabled: false\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Configuration", "title": "YAML", "url": "/configuration/yaml.html", "documentId": "639e7f237f11e8475a69e44e3f4a9b34acd250b0", "text": "\n                        \n                            \n                                \n                                \n                                YAML Configuration\nYAML (\"YAML Ain't Markup Language\") is a human-readable data file format, well suited to human-edited configuration files. Nearly all aspects of your project's build and deploy pipeline are controlled via YAML files.\nYAML is a whitespace-sensitive format that is especially good at key/value type configuration, such as that used by Platform.sh. There are many good YAML tutorials online and the format is reasonably self-documenting.  We especially recommend:\n\nGravCMS's YAML tutorial\nLearn YAML in Y Minutes\n\nThe following is only a cursory look at YAML itself.  The tutorials above will provide a more in-depth introduction.\nBasic YAML\nA YAML file is a text file that ends in .yaml.  (Some systems use an alternative .yml extension, but Platform.sh uses the four-letter extension.)  It consists primarily of key value pairs, and supports nesting.  For example:\nname: 'app'\ntype: 'php:7.1'\nbuild:\n    flavor: 'composer'\n\ndisk: 1024\n\nThis example defines a key name with value app, a key type with value php:7.1, a key disk with a value 1024, and a key build that is itself a nested set of key/value pairs, of which there is only one: flavor, whose value is composer.  Informally, nested values are often referenced using a dotted syntax, such as build.flavor, and that format is used in this documentation in various places.\nKeys are always strings, and may be quoted or not.  Values may be strings, numbers, booleans, or further nested key/value pairs.  Alphanumeric strings may be quoted or not.  More complex strings (with punctuation, etc.) must be quoted.  Numbers should not be quoted.  The boolean values true and false should never be quoted.\nFor quoted values, both single quotes (') and double quotes (\") are valid.  Double quotes, however, will interpolate common escape characters such as \\n and so forth.  For that reason using single quotes is generally recommended unless you want escape characters to be processed rather than taken literally.\nIn general the order of keys in a YAML file does not matter.  Neither do blank lines.  Indentation may be with any number of spaces, as long as it is consistent throughout the file.  Platform.sh examples by convention use four-space indentation.\nMulti-line strings\nIn case of long, multi-line strings, the | character tells the YAML parser that the following, indented lines are all part of the same string.  That is, this:\nhooks:\n    build: |\n        set -e\n        cp a.txt b.txt\n\ncreates a nested property hooks.build, which has the value set -e\\ncp a.txt b.txt.  (That is, a string with a line break in it.)  That is useful primarily for hooks, which allow the user to enter small shell scripts within the YAML file.\nIncludes\nYAML allows for special \"tags\" on values that change their meaning.  These tags may be customized for individual applications so may vary from one system to another.  The main Platform.sh \"local tag\" is !include, which allows for external files to be logically embedded within the YAML file.  The referenced file is always relative to the YAML file's directory.\nstring\nThe string type allows an external file to be inlined in the YAML file as though it had been entered as a multi-line string.  For example, given this file on disk named build.sh:\nset -e\ncp a.txt b.txt\n\nThen the following two YAML fragments are exactly equivalent:\nhooks:\n    build: |\n        set -e\n        cp a.txt b.txt\n\nhooks:\n    build: !include\n      type: string\n      path: build.sh\n\nThat is primarily useful for breaking longer build scripts or inlined configuration files out to a separate file for easier maintenance.\nbinary\nThe binary type allows an external binary file to be inlined in the YAML file.  The file will be base64 encoded.  For example:\nproperties:\n    favicon: !include\n        type: binary\n        path: favicon.ico\n\nwill reference the favicon.ico file, which will be provided to Platform.sh's management system.\nyaml\nFinally, the yaml type allows an external YAML file to be inlined into the file as though it had been typed in directly.  That can help simplify more complex files, such a .platform.app.yaml file with many highly-customized web.locations blocks.\nThe yaml type is the default, meaning it may reference a file inline without specifying a type.\nFor example, given this file on disk named main.yaml:\nroot: 'web'\nexpires: 5m\npassthru: '/index.php'\nallow: false\nrules:\n    '\\.(jpe?g|png|gif|svgz?|css|js|map|ico|bmp|eot|woff2?|otf|ttf)$':\n        allow: true\n    '^/robots\\.txt$':\n        allow: true\n    '^/sitemap\\.xml$':\n        allow: true\n\nThen the following three location definitions are exactly equivalent:\nweb:\n    locations:\n        '/': !include \"main.yaml\"\n\nweb:\n    locations:\n        '/': !include \n            type: yaml\n            path: 'main.yaml'\n\nweb:\n    locations:\n        '/': \n            root: 'web'\n            expires: 5m\n            passthru: '/index.php'\n            allow: false\n            rules:\n                '\\.(jpe?g|png|gif|svgz?|css|js|map|ico|bmp|eot|woff2?|otf|ttf)$':\n                    allow: true\n                '^/robots\\.txt$':\n                    allow: true\n                '^/sitemap\\.xml$':\n                    allow: true\n\n!archive\nAnother custom tag available is !archive, which specifies a value is a reference to a directory on disk, relative to the location of the YAML file.  Essentially it defines the value of key as \"this entire directory\".  Consider this services.yaml fragment:\nmysearch:\n    type: solr:8.0\n    disk: 1024\n    configuration:\n        conf_dir: !archive \"solr/conf\"\n\nIn this case, the mysearch.configuration.conf_dir value is not the string \"solr/conf\", but the contents of the solr/conf directory (relative to the services.yaml file).  On Platform.sh, that is used primarily for service definitions in services.yaml to provide a directory of configuration files for the service (such as Solr in this case).  Platform.sh will use that directive to copy the entire specified directory into our management system so that it can be deployed with the specified service.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Set your domain", "url": "/gettingstarted/going-live/set-domain.html", "documentId": "dc94db25c1364c207b60c598fdc8434e15501108", "text": "\n                        \n                            \n                                \n                                \n                                Going live\nConfigure Domain\nYou will need to configure your registered domain on your Platform.sh project before going live, and you can do that either through the management console or by using the CLI.\nThrough the management console\n\n  \n\n\nNow that you have changed your project to a production plan, you can click the same \"Go live\" button at the top of the project page. Alternatively, you can click \"Settings\" at the top of the page, and then visit the \"Domains\" section on the left.\nClick the \"Add+\" button in the top right hand corner of the page, enter your registered domain and select if you want it to be the default domain for the project. You can add multiple domains to a project, but only one can be set as the default.\nWhen you're finished, click \"Add domain\", and the project will once again redeploy to apply your changes.\nUsing the CLI\nYou can also add a domain to your project using the Platform.CLI. From a terminal window, type the command\nplatform domain:add example.com --project &lt;project ID&gt;\n\nThe CLI will validate your registered domain and provision Let's Encrypt certificates for it.\n\n  Back\n  I have configured my registered domain on my project\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Routes", "title": "HTTPS", "url": "/configuration/routes/https.html", "documentId": "4774b993a74c40e9c1d18c95f1400ffe13ba9b80", "text": "\n                        \n                            \n                                \n                                \n                                HTTPS\nLet's Encrypt\nAll environments on Platform.sh support both HTTP and HTTPS automatically.  Production SSL certificates are provided by Let's Encrypt.  You may alternatively provide your own SSL certificate from a 3rd party issuer of your choice at no charge from us.\n\nnote\nLet's Encrypt certificate renewals are attempted each time your environment is deployed. If your project does not receive regular code commits, you will need to manually issue a re-deployment to ensure the certificate remains valid. We suggest that you do so when your project doesn't receive any updates for over 1 month. This can be done by pushing a code change via git or issuing the following command from your local environment:\nplatform redeploy\n\nAlternatively, see the section below on automatically redeploying the site in order to renew the certificate.\n\nPlatform.sh recommends using HTTPS requests for all sites exclusively.  Doing so provides better security, access to certain features that web browsers only permit over HTTPS, and access to HTTP/2 connections on all sites which can greatly improve performance.\nHow HTTPS redirection is handled depends on the routes you have defined.  Platform.sh recommends specifying all HTTPS routes in your routes.yaml file.  That will result in all pages being served over SSL, and any requests for an HTTP URL will automatically be redirected to HTTPS.\n\"https://{default}/\":\n    type: upstream\n    upstream: \"app:http\"\n\n\"https://www.{default}/\":\n    type: redirect\n    to: \"https://{default}/\"\n\nSpecifying only HTTP routes will result in duplicate HTTPS routes being created automatically, allowing the site to be served from both HTTP and HTTPS without redirects.\nAlthough Platform.sh does not recommend it, you can also redirect HTTPS requests to HTTP explicitly to serve the site over HTTP only.  The use cases for this configuration are few.\n\"http://{default}/\":\n    type: upstream\n    upstream: \"app:http\"\n\n\"http://www.{default}/\":\n    type: redirect\n    to: \"http://{default}/\"\n\n\"https://{default}/\":\n    type: redirect\n    to: \"http://{default}/\"\n\n\"https://www.{default}/\":\n    type: redirect\n    to: \"http://{default}/\"\n\nOf course, more complex routing logic is possible if the situation calls for it. However, we recommend defining HTTPS routes exclusively.\nTLS configuration\nOptionally, it's possible to further refine how secure TLS connections are handled on your cluster via the tls route property.\nhttps://{default}/:\n    type: upstream\n    upstream: app:http\n    tls:\n        # ...\n\nmin_version\n\nNote\nThis directive was put into place when Platform.sh supported older versions of TLS for customers.\nCurrently only TLS v1.2 is supported. Support for TLS v1.3 will be added in the near future.\n\nSetting a minimum version of TLS will cause the server to automatically reject any connections using an older version of TLS.  Rejecting older versions with known security vulnerabilities is necessary for some security compliance processes.\ntls:\n    min_version: TLSv1.2\n\nThe above configuration will result in requests using older TLS versions to be rejected.  Legal values are TLSv1.2.\nNote that if multiple routes for the same domain have different min_versions specified, the highest specified will be used for the whole domain.\nstrict_transport_security\nHTTP Strict Transport Security (HSTS) is a mechanism for telling browsers to use HTTPS exclusively with a particular website.  You can toggle it on for your site at the router level without having to touch your application, and configure it's behavior from routes.yaml.\ntls:\n    strict_transport_security:\n        enabled: true\n        include_subdomains: true\n        preload: true\n\nThere are three sub-properties for the strict_transport_security property:\n\nenabled: Can be true, false, or null.  Defaults to null.  If false, the other properties wil be ignored.\ninclude_subdomains: Can be true or false.  Defaults to false. If true, browsers will be instructed to apply HSTS restrictions to all subdomains as well.\npreload: Can be true or false.  Defaults to false.  If true, Google and others may add your site to a lookup reference of sites that should only ever be connected to over HTTPS.  Many although not all browsers will consult this list before connecting to a site over HTTP and switch to HTTPS if instructed.  Although not part of the HSTS specification it is supported by most browsers.\n\nIf enabled, the Strict-Transport-Security header will always be sent with a lifetime of 1 year.  The Mozilla Developer Network has more detailed information on HSTS.\nNote: If multiple routes for the same domain specify different HSTS settings, the entire domain will still use a shared configuration.  Specifically, if any route on the domain has strict_transport_security.enabled set to false, HSTS will be disabled for the whole domain.  Otherwise, it will be enabled for the whole domain if at least one such route has enabled set to true.  As this logic may be tricky to configure correctly we strongly recommend picking a single configuration for the whole domain and adding it on only a single route.\nClient authenticated TLS\nIn some non-browser applications (such as mobile applications, IoT devices, or other restricted-client-list use cases), it is beneficial to restrict access to selected devices using TLS.  This process is known as client-authenticated TLS, and functions effectively as a more secure alternative to HTTP Basic Auth.\nBy default, any valid SSL cert issued by one of the common certificate issuing authorities will be accepted.  Alternatively, you can restrict access to SSL certs issued by just those certificate authorities you specify, including a custom authority.  (The latter is generally only applicable if you are building a mass-market IoT device or similar.)  To do so, set client_authentication required and then provide a list of the certificates of the certificate authorities you wish to allow.\ntls:\n    client_authentication: \"require\"\n    client_certificate_authorities:\n        - !include\n            type: string\n            path: root-ca1.crt\n        - !include\n            type: string\n            path: root-ca2.crt\n\nIn this case, the certificate files are resolved relative to the .platform directory.  Alternatively, the certificates can be specified inline in the file:\ntls:\n    client_authentication: \"require\"\n    client_certificate_authorities:\n        - |\n            -----BEGIN CERTIFICATE-----\n            ### Several lines of random characters here ###\n            -----END CERTIFICATE-----\n        - |\n            -----BEGIN CERTIFICATE-----\n            ### Several lines of different random characters here ###\n            -----END CERTIFICATE-----\n\nAutomated SSL certificate renewal using Cron\nIf the Let's Encrypt certificate is due to expire in less than one month then it will be renewed automatically during a deployment.  That makes it feasible to set up regular auto-renewal of the Let's Encrypt certificate.  The caveat is that, like any deploy, there is a very brief downtime (a few seconds, usually) so it's best to do during off-hours.\nYou will first need to install the CLI in your application container.  See the section on API tokens for instructions on how to do so.\n\nnote\nAutomated SSL certificate renewal using cron requires you to get an API token and install the CLI in your application container.\n\nOnce the CLI is installed in your application container and an API token configured you can add a cron task to run twice a month to trigger a redeploy. For example:\ncrons:\n    renewcert:\n        # Force a redeploy at 10 am (UTC) on the 1st and 15th of every month.\n        spec: '0 10 1,15 * *'\n        cmd: |\n            if [ \"$PLATFORM_BRANCH\" = master ]; then\n                platform redeploy --yes --no-wait\n            fi\n\nThe above cron task will run on the 1st and 15th of the month at 10 am (UTC), and, if the current environment is the master branch, it will run platform redeploy on the current project and environment.  The --yes flag will skip any user-interaction.  The --no-wait flag will cause the command to complete immediately rather than waiting for the redeploy to complete.  We recommend adjusting the cron schedule to whenever is off-peak time for your site, and to random days within the month.\n\nwarning\nIt is very important to include the --no-wait flag.  If you do not, the cron process will block waiting on the deployment to finish, but the deployment will be blocked by the running cron task.  That will take your site offline until you log in and manually terminate the running cron task.  You want the --no-wait flag.  We're not joking.\n\nThe certificate will not renew unless it has less than one month remaining; trying twice a month is sufficient to ensure a certificate is never less than 2 weeks from expiring.  As the redeploy does cause a momentary pause in service we recommend running during non-peak hours for your site.\nLet's Encrypt limits and branch names\nYou may encounter Let's Encrypt certificates failing to provision after the build hook has completed:\nProvisioning certificates\n  Validating 2 new domains\n  E: Error provisioning the new certificate, will retry in the background.\n  (Next refresh will be at 2020-02-13 14:29:22.860563+00:00.)\n  Environment certificates\n\nW: Missing certificate for domain www.&lt;PLATFORM_ENVIRONMENT&gt;-&lt;PLATFORM_PROJECT&gt;.&lt;REGION&gt;.platformsh.site\nW: Missing certificate for domain &lt;PLATFORM_ENVIRONMENT&gt;-&lt;PLATFORM_PROJECT&gt;.&lt;REGION&gt;.platformsh.site\n\nOne reason that this can happen has to do with the limits of Let's Encrypt itself, which caps off at 64 characters for URLS. If your TLS certificates are not being provisioned, it's possible that the names of your branches are too long, and the environment's generated URL goes over that limit.\nAt this time, generated URLs have the following pattern:\n&lt;PLATFORM_ENVIRONMENT&gt;-&lt;PLATFORM_PROJECT&gt;.&lt;REGION&gt;.platformsh.site\n\nPLATFORM_ENVIRONMENT = PLATFORM_BRANCH + 7 character hash\nPLATFORM_PROJECT = 13 characters\nREGION = 2-4 characters, depending on the region\nplatformsh.site = 15 characters\nextra characters (. &amp; -) = 4 characters\n\nThis breakdown leaves you with 21-23 characters to work with naming your branches (PLATFORM_BRANCH) without going over the 64 character limit, dependent on the region. Since this pattern for generated URLs will remain similar, but could change slightly over time, it's our recommendation to use branch names with a maximum length between 15 and 20 characters.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Troubleshooting", "url": "/gettingstarted/going-live/troubleshooting.html", "documentId": "39552cf77072bdd32544c6a04dc71ca0359105a7", "text": "\n                        \n                            \n                                \n                                \n                                Going live\nDone!\nNow your application has been taken live!\nAs mentioned before, it may take a little time for the DNS to fully propagate, depending on the registrar. Otherwise, your domain should now point to the master production environment of your project.\nAdditional information\nIf through the following steps your project did not successfully configure to your domain, you can consult the Troubleshooting guide here:\n\nGoing Live: Troubleshooting\n\nConsider using the Fastly CDN for increased performance and more control over caching:\n\nGoing Live: Fastly\n\nConsider using Cloudflare's TLS/SSL service to secure your site via HTTPS when using a CDN:\n\nGoing Live: Cloudflare\n\n\n  Back\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Configure DNS", "url": "/gettingstarted/going-live/configure-dns.html", "documentId": "ad42861a069c1e92221045a62cbb08ed45bbd14c", "text": "\n                        \n                            \n                                \n                                \n                                Going live\nConfigure your DNS provider\nThe next step is to configure your DNS provider to point to the domain of your master environment on Platform.sh.\n&lt;/asciinema-player&gt;\nYou can access the CNAME target from your terminal by using the CLI and the command\nplatform environment:info edge_hostname\n\nAdd a CNAME record from your desired domain to the value of the edge_hostname. Depending on your registrar, this value may be called an \"Alias\" or something similar.\nIf your application is going to serve multiple domains, you will need to add a CNAME record for each of them.\nYou can find out more information about using an apex domain and CNAME records in the Going Live documentation.\nDepending on your registrar and the TTL you set for the domain, it may take up to 72 hours for the DNS change to fully propagate across the Internet.\n\n  Back\n  I have configured my DNS provider\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Upgrade plan", "url": "/gettingstarted/going-live/upgrade-plan.html", "documentId": "b917f5b84ecb57b9f6b47960fa0369aacaba1bb8", "text": "\n                        \n                            \n                                \n                                \n                                Going live\nUpgrade Plan\n\"Development\" plan projects cannot be assigned a domain name, so you will not be able to go live until you upgrade to at least a Standard plan. This can be done using the management console.\n\n  \n\n\nDevelopment plans come with four environments: three development and one \"future\" production environment, which is the master branch.\nFor example, \"Small\" plan sizes provide a production environment, but restrict your application to the use of a single service (i.e. a database).\nOn your project, click the \"Go live\" button in the top right hand corner of your project preview image. This will allow you to edit the project's plan, and it can also be reached from your \"Account\" page by clicking \"Edit\" from the vertical dot dropdown for your project.\nSelect the plan size that is appropriate for the needs of your application. This is also the page where you can increase the number of development environments, and the amount of storage. Make your changes and then click \"Update plan\" at the bottom of the page. Your application will redeploy.\n\n  Back\n  I have upgraded my plan size\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "First steps", "url": "/gettingstarted/going-live/first-steps.html", "documentId": "3a10d741efca1a84e2f543989f3598ee46dde40c", "text": "\n                        \n                            \n                                \n                                \n                                Going live\nPreliminary Steps\nBefore you take your site live, there are a few steps that will help you prepare the project.\n\nRegister your domain and choose a suitable DNS provider\n If you plan on serving exclusively from a subdomain such as the historically common www. subdomain, you will be able use any DNS provider that supports CNAME records. If you wish to use the apex domain, eg. https://site.com, with no www. subdomain, choose one of the specialized DNS providers that allow you to use ALIAS or ANAME records. Make sure to do this before moving on to the next steps, as the CLI will reject attempts to add domains that do not allow CNAMEs.\n\nTest your routes\n Test your application and make sure that all of your routes are functioning as you intended. Consult the routes documentation as well to verify that your routes.yaml has been properly written.\n If any access restrictions have been enabled during development, be sure to remove them as well.\n\n(Optional) Obtain 3rd party SSL if needed\n Let's Encrypt SSL certificates are automatically issued for Platform.sh projects at no charge to you. If you instead would like to use a 3rd party SSL certificate, make sure that you have purchased it and that it is active prior to going live.\n Additionally, if your application uses wildcard routes, it will require custom certificates for them, as Let's Encrypt does not support wildcard certificates.\n\n\nAfter you have gone through the following checklist your application is ready to be taken live!\n\n  Back\n  I'm ready to go live\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Connect to services", "url": "/gettingstarted/own-code/connect-services.html", "documentId": "93ab822f2b0ce8be648d02788a82d566e48ccd72", "text": "\n                        \n                            \n                                \n                                \n                                Import your own code\nConnect to Services\n\nNote: If your application does not require access to services and you left .platform/services.yaml blank, feel free to proceed to the next step.\n\nAt this point you have configured your application's services as well as its build and deploy process in your .platform/services.yaml and .platform.app.yaml files. As an example, in your .platform.app.yaml file you may have defined a relationship called database:\nrelationships:\n    database: \"mysqldb:mysql\"\n\nwhich was configured in .platform/services.yaml with\nmysqldb:\n    type: mysql:10.2\n    disk: 1024\n\nIn order to connect to this service and use it in your application, Platform.sh exposes its credentials in the application container within a base64-encoded JSON PLATFORM_RELATIONSHIPS environment variable.\nTo access this variable you can install a Platform.sh configuration reader library\nPHPPythonNode.jsJava (Maven)Java (Gradle)Gocomposer install platformsh/config-readerpip install platformshconfignpm install platformsh-config --save&lt;dependency&gt;\n    &lt;groupId&gt;sh.platform&lt;/groupId&gt;\n    &lt;artifactId&gt;config&lt;/artifactId&gt;\n    &lt;version&gt;2.2.0&lt;/version&gt;\n&lt;/dependency&gt;compile group: 'sh.platform', name: 'config', version: 2.2.0'go mod edit -require=github.com/platformsh/config-reader-go/v2\nand access the credentials of database\nPHPPythonNode.jsJavaGouse Platformsh\\ConfigReader\\Config;\n\n$config = new Config();\n$credentials = $config-&gt;credentials('database');from platformshconfig import Config\n\nconfig = Config()\ncredentials = config.credentials('database')const config = require(\"platformsh-config\").config();\nconst credentials = config.credentials('database');import Config;\n\nConfig config = new Config();\nCredential cred = config.getCredential('database')import psh \"github.com/platformsh/config-reader-go/v2\"\n\nconfig, err := psh.NewRuntimeConfig()\n// Handle err\n\ncredentials, err := config.Credentials(\"database\")\n// Handle err\nor read and decode the environment variable directly.\nPHPPythonNode.js$relationships = json_decode(base64_decode(getenv('PLATFORM_RELATIONSHIPS')), TRUE);\n$credentials = $relationships['database'];import os, json, base64\n\nrelationships = json.loads(base64.b64decode(os.environ[\"PLATFORM_RELATIONSHIPS\"]))\ncredentials = relationships['database']relationships = JSON.parse(new Buffer(process.env['PLATFORM_RELATIONSHIPS'], 'base64').toString());\ncredentials = relationships['database'];\nIn either case, credentials can now be used to connect to database:\n{\n  \"username\": \"user\",\n  \"scheme\": \"mysql\",\n  \"service\": \"mysql\",\n  \"fragment\": null,\n  \"ip\": \"169.254.197.253\",\n  \"hostname\": \"czwb2d7zzunu67lh77infwkm6i.mysql.service._.eu-3.platformsh.site\",\n  \"public\": false,\n  \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\",\n  \"host\": \"mysql.internal\",\n  \"rel\": \"mysql\",\n  \"query\": {\n    \"is_master\": true\n  },\n  \"path\": \"main\",\n  \"password\": \"\",\n  \"type\": \"mysql:10.2\",\n  \"port\": 3306\n}\n\nYou can find out more information about Platform.sh Config Reader libraries on GitHub:\n\nPHP Config Reader\nPython Config Reader\nNode.js Config Reader\nJava Config Reader\nGo Config Reader\n\nYou can also find examples of how to connect to each of Platform.sh managed services in multiple languages in the Services Documentation.\nProject configured, services connected - time to commit the changes and push your repository onto your project.\n\n  Back\n  I have connected my services to my application\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Going live", "url": "/gettingstarted/going-live.html", "documentId": "07385bd23c156e46785e70a13a485742fd9e43c9", "text": "\n                        \n                            \n                                \n                                \n                                Going live\nYou've set up a project on Platform.sh by either pushing your code directly or by setting up an integration to an external repository. Now it's time to take your site live.\nThis guide will take you through the process configuring your project for production, setting up a domain, and configuring DNS so that your users can reach the application the way you want them to.\n\n  Take your site live!\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Configure routes", "url": "/gettingstarted/own-code/routes-configuration.html", "documentId": "2ea9e0cafee8d82fb99b4cd2415838d2f81a1ff8", "text": "\n                        \n                            \n                                \n                                \n                                Import your own code\nConfigure routes\nThe final configuration file you will need to modify in your repository is the .platform/routes.yaml file, which describes how an incoming HTTP request is going to be processed by Platform.sh.\n\u251c\u2500\u2500 .platform\n\u2502   \u251c\u2500\u2500 routes.yaml\n\u2502   \u2514\u2500\u2500 services.yaml\n\u251c\u2500\u2500 .platform.app.yaml\n\u2514\u2500\u2500 &lt; application code &gt;\n\nA minimal configuration .platform/routes.yaml for all languages will look very similar:\n# The routes of the project.\n#\n# Each route describes how an incoming URL is going\n# to be processed by Platform.sh.\n\n\"https://{default}/\":\n    type: upstream\n    upstream: \"app:http\"\n\n\"https://www.{default}/\":\n    type: redirect\n    to: \"https://{default}/\"\n\nConfiguring the routes can be done using either an absolute URL or a URL template as shown in the examples above that have the form http://www.{default}, where {default} will be substituted by either your configured domain or those automatically generated by Platform.sh.\nIf you set up a domain of example.com, the route configuration http://www.{default} will be resolved to http://www.example.com/. Your production (master) environment's routes will be configured according to these rules, but so will each development environment that you activate.\nEach route can then be configured with the following properties:\n\ntype:\nupstream: serves the application. Takes the form upstream: &lt;application name&gt;:http, using the application name set in your .platform.app.yaml.\nredirect: configures redirects from http://{default} to your application.\n\n\ncache: controls caching behavior of the route.\nredirects: controls redirect rules associated with the route.\n\n\nEach language and framework may have additional attributes that you will need to include in .platform/routes.yaml depending on the needs of your application. To find out what else you may need to include to configure your routes, consult\n\nThe Routes documentation for Platform.sh\n The documentation goes into far more extensive detail of which attributes can also be included for route configuration, and should be used as your primary reference.   \nLanguage-specific templates for Platform.sh Projects\n Compare the .platform/routes.yaml file from the simple template above to other templates when writing your own.\n\n\nIn the next step, you will be able to see how Platform.sh leverages environment variables to make connecting your application to its services simple.   \n\n  Back\n  I have configured my routes\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Configure services", "url": "/gettingstarted/own-code/service-configuration.html", "documentId": "3a56af54f40b92e1aec6b77aad7348219f377439", "text": "\n                        \n                            \n                                \n                                \n                                Import your own code\nConfigure services\nIn the previous step, you created a collection of empty configuration files that have given your project the following structure:\n\u251c\u2500\u2500 .platform\n\u2502   \u251c\u2500\u2500 routes.yaml\n\u2502   \u2514\u2500\u2500 services.yaml\n\u251c\u2500\u2500 .platform.app.yaml\n\u2514\u2500\u2500 &lt; application code &gt;\n\nNow you will need to include information that will tell Platform.sh how you want your application to connect to its services. An example .platform/services.yaml will look something like this:\n.platform/services.yaml\n# This file defines services you want available to your application.\n\nelasticsearch:\n    type: elasticsearch:7.2\n    disk: 256\n    size: S\ninfluxdb:\n    type: influxdb:1.7\n    disk: 256\n    size: S\nkafka:\n    type: kafka:2.2\n    disk: 512\n    size: S\nmemcached:\n    type: memcached:1.4\n    size: S\nmongodb:\n    type: mongodb:3.6\n    disk: 1024\n    size: S\nmysql:\n    type: mariadb:10.4\n    disk: 256\n    size: S\npostgresql:\n    type: postgresql:11\n    disk: 256\n    size: S\nrabbitmq:\n    type: rabbitmq:3.7\n    disk: 256\n    size: S\nredis:\n    type: redis:5.0\n    size: S\nsolr:\n    type: solr:8.0\n    disk: 256\n    size: S\n    configuration:\n        cores:\n            maincore:\n                conf_dir: !archive \"solr-config\"\n        endpoints:\n            solr:\n                core: maincore\n\nIf your application does not use any services at this point then you can leave it blank, but it must exist in your repository to run on Platform.sh. If your application does use a database or other services, you can configure them with the following attributes:\n\nname: Provide a name for the service, so long as it is alphanumeric. If your application requires multiple services of the same type, make sure to give them different names so that your data from one service is never overwritten by another.\n\ntype: This specifies the service type and its version using the format\ntype:version\n\nConsult the table below that lists all Platform.sh maintained services, along with their type and supported versions. The links will take you to each service's dedicated page in the documentation.\n\n\n\n\n\nService\ntype\nSupported version\n\n\n\n\nHeadless Chrome\nchrome-headless\n73\n\n\nElasticsearch\nelasticsearch\n6.5, 7.2\n\n\nInfluxDB\ninfluxdb\n1.2, 1.3, 1.7\n\n\nKafka\nkafka\n2.1, 2.2, 2.3, 2.4\n\n\nMariaDB\nmariadb\n10.0, 10.1, 10.2, 10.3, 10.4\n\n\nMemcached\nmemcached\n1.4, 1.5, 1.6\n\n\nMongoDB\nmongodb\n3.0, 3.2, 3.4, 3.6\n\n\nNetwork Storage\nnetwork-storage\n1.0\n\n\nOracle MySQL\noracle-mysql\n5.7, 8.0\n\n\nPostgreSQL\npostgresql\n9.6, 10, 11, 12\n\n\nRabbitMQ\nrabbitmq\n3.5, 3.6, 3.7, 3.8\n\n\nRedis\nredis\n3.2, 4.0, 5.0\n\n\nSolr\nsolr\n3.6, 4.1, 6.3, 6.6, 7.6, 7.7, 8.0, 8.4\n\n\nVarnish\nvarnish\n5.6, 6.0\n\n\n\n\ndisk: The disk attribute configures the amount of persistent disk that will be allocated between all of your services. Projects by default are allocated 5 GB (5120 MB), and that space can be distributed across all of your services. Note that not all services require disk space. If you specify a disk attribute for a service that doesn't use it, like Redis, you will receive an error when trying to push your changes.\n\n\nEach language and framework may have additional attributes that you will need to include in .platform/services.yaml depending on the needs of your application. To find out what else you may need to include to configure your services, consult\n\nThe Services documentation for Platform.sh\n The documentation goes into far more extensive detail of which attributes can also be included for service configuration, and should be used as your primary reference.  \n\nLanguage-specific templates for Platform.sh Projects\n Compare the .platform/services.yaml file from the simple template above to other templates when writing your own.\n\n\n\nPlatform.sh provides managed services, and each service comes with considerable default configuration that you will not have to include yourself in .services.yaml.\nNext, you will next need to tell Platform.sh how to build and deploy your application using the .platform.app.yaml file.\n\n  Back\n  I have configured my services\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Configuring projects", "url": "/gettingstarted/own-code/project-configuration.html", "documentId": "5730e9e7d62f1653a8f2ede9f2fe823d4fc02c1e", "text": "\n                        \n                            \n                                \n                                \n                                Import your own code\nConfiguring projects\nIn the previous step, you created a new project on Platform.sh using the CLI. Now, there are a few configuration steps left that will help Platform.sh know what to do with your application during builds and deployments.\n\nConsult a template alongside this guide\n As you go through this guide, example files will be provided that will give you a good impression of how to configure applications on Platform.sh in the programming language they use. However, since they are simple examples and your own application may require more detailed configuration than those examples address, it is recommended that you take a look at our maintained templates for additional guidance.\n Select a language and choose one or more templates that most closely resemble your application and keep the template in another tab as you continue through this guide. Using these two resources together is the fastest way to correctly configure your project for Platform.sh.\n\nPHP Templates\nPython Templates\nNode.js Templates\nJava Templates\nRuby Templates\nGo Templates\n\n\nCreate empty configuration files\n You will notice that each of the templates above contain the following structure around their application code:\n \u251c\u2500\u2500 .platform\n \u2502   \u251c\u2500\u2500 routes.yaml\n \u2502   \u2514\u2500\u2500 services.yaml\n \u251c\u2500\u2500 .platform.app.yaml\n \u2514\u2500\u2500 &lt; application code &gt;\n\n In order to successfully deploy to Platform.sh you must add three YAML files:\n\nA .platform/routes.yaml file, which configures the routes used in your environments. That is, it describes how an incoming HTTP request is going to be processed by Platform.sh.\nA .platform/services.yaml file, which configures the services that will be used by the application. Connecting to Platform.sh's maintained services only requires properly writing this file. While this file must be present, if your application does not require services it can remain empty.\nAt least one .plaform.app.yaml file, which configures the application itself. It provides control over the way the application will be built and deployed on Platform.sh.\nWhen you set Platform.sh as a remote for your repository in the previous step, the CLI automatically created the hidden configuration directory .platform for you. The next steps will explore in more detail what each configuration files must include, but for now create empty files in their place.\ntouch .platform/routes.yaml\ntouch .platform/services.yaml\ntouch .platform.app.yaml\n\n\n\n\n(Optional) Follow the Project Setup Wizard instructions in the management console\n All of the steps in this guide are also available in the Project Setup Wizard in your management console. Once you have created your project, the Wizard will appear at the top of your project page with detailed steps to help you properly configure your applications on Platform.sh.\n \n\n\nWith the empty configuration files in place, you will need to specify your service configuration in .platform/services.yaml.\n\n  Back\n  I have initialized my code with empty configuration files\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Configure application", "url": "/gettingstarted/own-code/app-configuration.html", "documentId": "efcf7b38afd58dc0e363da3b3a9f19910ba09e61", "text": "\n                        \n                            \n                                \n                                \n                                Import your own code\nConfigure application\nYou will next need to include information that defines how you want your application to behave each time it is built and deployed on Platform.sh in a .platform.app.yaml file.\n\u251c\u2500\u2500 .platform\n\u2502   \u251c\u2500\u2500 routes.yaml\n\u2502   \u2514\u2500\u2500 services.yaml\n\u251c\u2500\u2500 .platform.app.yaml\n\u2514\u2500\u2500 &lt; application code &gt;\n\nAn example .platform.app.yaml looks like this:\nPHPGoPythonNode.js# This file describes an application. You can have multiple applications\n# in the same project.\n\n# The name of this app. Must be unique within a project.\nname: app\n\n# The type of the application to build.\ntype: php:7.3\nbuild:\n    flavor: composer\n\n# The hooks that will be performed when the package is deployed.\nhooks:\n    build: |\n        set -e\n\n    deploy: |\n        set -e\n\n# The relationships of the application with services or other applications.\n# The left-hand side is the name of the relationship as it will be exposed\n# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand\n# side is in the form `&lt;service name&gt;:&lt;endpoint name&gt;`.\n#relationships:\n#    database: \"db:mysql\"\n\n# The size of the persistent disk of the application (in MB).\ndisk: 2048\n\n# The mounts that will be performed when the package is deployed.\nmounts:\n    # Because this directory is in the webroot, files here will be web-accessible.\n    'web/uploads':\n        source: local\n        source_path: uploads\n    # Files in this directory will not be web-accessible.\n    'private':\n        source: local\n        source_path: private\n\n\n# The configuration of app when it is exposed to the web.\nweb:\n    locations:\n        \"/\":\n            # The public directory of the app, relative to its root.\n            root: \"web\"\n            # The front-controller script to send non-static requests to.\n            passthru: \"/index.php\"\n# This file describes an application. You can have multiple applications\n# in the same project.\n#\n# See https://docs.platform.sh/user_guide/reference/platform-app-yaml.html\n\n# The name of this app. Must be unique within a project.\nname: app\n\n# The runtime the application uses.\ntype: golang:1.14\n\n# The hooks executed at various points in the lifecycle of the application.\nhooks:\n    build: |\n        # Modify this line if you want to build differently or use an alternate name for your executable.\n        go build -o bin/app\n\n# The relationships of the application with services or other applications.\n#\n# The left-hand side is the name of the relationship as it will be exposed\n# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand\n# side is in the form `&lt;service name&gt;:&lt;endpoint name&gt;`.\nrelationships:\n    database: \"db:mysql\"\n\n# The configuration of app when it is exposed to the web.\nweb:\n    upstream:\n        socket_family: tcp\n        protocol: http\n\n    commands:\n        # If you change the build output in the build hook above, update this line as well.\n        start: ./bin/app\n\n    locations:\n        /:\n            # Route all requests to the Go app, unconditionally.\n            # If you want some files served directly by the web server without hitting Go, see\n            # https://docs.platform.sh/configuration/app/web.html\n            allow: false\n            passthru: true\n\n# The size of the persistent disk of the application (in MB).\ndisk: 1024\n# This file describes an application. You can have multiple applications\n# in the same project.\n#\n# See https://docs.platform.sh/user_guide/reference/platform-app-yaml.html\n\n# The name of this app. Must be unique within a project.\nname: app\n\n# The runtime the application uses.\ntype: \"python:3.7\"\n\n# The build-time dependencies of the app.\ndependencies:\n    python3:\n       pipenv: \"2018.10.13\"\n\n# The hooks executed at various points in the lifecycle of the application.\nhooks:\n    build: |\n      pipenv install --system --deploy\n\n# The size of the persistent disk of the application (in MB).\ndisk: 1024\n# The relationships of the application with services or other applications.\n#\n# The left-hand side is the name of the relationship as it will be exposed\n# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand\n# side is in the form `&lt;service name&gt;:&lt;endpoint name&gt;`.\nrelationships:\n    database: \"db:mysql\"\n    rediscache: \"cache:redis\"\n\n# The configuration of app when it is exposed to the web.\nweb:\n    commands:\n        start: python server.py\n# This file describes an application. You can have multiple applications\n# in the same project.\n#\n# See https://docs.platform.sh/user_guide/reference/platform-app-yaml.html\n\n# The name of this app. Must be unique within a project.\nname: app\n\n# The runtime the application uses.\ntype: nodejs:10\n\n# The relationships of the application with services or other applications.\n#\n# The left-hand side is the name of the relationship as it will be exposed\n# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand\n# side is in the form `&lt;service name&gt;:&lt;endpoint name&gt;`.\nrelationships:\n  database: \"db:mysql\"\n\n# The configuration of app when it is exposed to the web.\nweb:\n  commands:\n   start: \"node index.js\"\n\n# The size of the persistent disk of the application (in MB).\ndisk: 512\n\nmounts:\n    'run':\n        source: local\n        source_path: run\n\nThe .platform.appl.yaml file is extremely flexible, and can contain many lines with very fine-grained control over your application. At the very least, Platform.sh requires three principle attributes in this file to control your builds:\n\nname: The name of your application container does not have to be the same as your project name, and in most single application cases you can simply name it app. You should notice in the next step, when you configure how requests are handled in .platform/routes.yaml that name is reused there, and it is important that they are the same.\n\nNote  If you are trying to to deploy microservices, the only constraint is that each of these application names must be unique.\n\n\ntype: The type attribute in .platform.app.yaml sets the container base image for the application, and sets the primary language. In general, type should have the form\ntype: &lt;runtime&gt;:&lt;version&gt;\n\nSet version to one supported by Platform.sh, which you can find below as well as in the documentation for each language:\n\n\n\n\n\nLanguage\nruntime\nSupported version\n\n\n\n\nC#/.Net Core\ndotnet\n2.0, 2.1, 2.2, 3.1\n\n\nGo\ngolang\n1.11, 1.12, 1.13, 1.14\n\n\nJava\njava\n11, 12, 8, 13\n\n\nLisp\nlisp\n1.5\n\n\nNode.js\nnodejs\n6, 8, 10, 12\n\n\nPHP\nphp\n7.2, 7.3, 7.4\n\n\nPython\npython\n2.7, 3.5, 3.6, 3.7, 3.8\n\n\nRuby\nruby\n2.3, 2.4, 2.5, 2.6, 2.7\n\n\n\n\ndisk: The disk attribute defines that amount of persistent storage you need to have available for your application, and requires a minimum value of 256 MB.\n\nThere are a few additional keys in .platform.app.yaml you will likely need to use to fully configure your application, but are not required:\n\nrelationships: Relationships define how services are mapped within your application. Without this block, an application cannot by default communicate with a service container. Provide a unique name for each relationship and associate it with a service. For example, if in the previous step you defined a MariaDB container in your .platform/services.yaml with\n  db:\n    type: mariadb:10.4\n    disk: 256\n  You must define a relationship (i.e. database) in .platform.app.yaml to connect to it:\n  relationships:\n    database: \"db:mysql\"\n\nBuild and deploy tasks: There are a number of ways in which your Git repository is turned into a running application. In general, the build process will run the the build flavor, install dependencies, and then execute the build hook you provide. When the build process is completed, the deploy process will run the deploy hook.\n\nbuild: The build key defines what happens during the build process using the flavor property. This is a common inclusion for PHP and Node.js applications, so check the the documentation to see if your configuration requires this key.\ndependencies: This key makes it possible to install system-level dependencies as part of the build process.\nhooks: Hooks define custom scripts that you want to run at different points during the deployment process.\nbuild: The build hook is run after the build flavor if that is present. The file system is fully writable, but no services and only a subset of variables are available at this point. The full list of build time and runtime variables is available on the variables section of the public documentation.\ndeploy: The deploy hook is run after the application container has been started, but before it has started accepting requests. Services are now available, but the file system will be read-only from this point forward.\npost-deploy: The post-deploy hook functions exactly the same as the deploy hook, but after the container is accepting connections.\n\n\n\n\nweb: The web key configures the web server through a single web instance container running a single Nginx server process, behind which runs your application.\n\ncommands: Defines the command to actually launch the application. The start key launches your application. In all languages except for PHP, web.commands.start should be treated as required. For PHP, you will instead need to define a script name in passthru, described below in locations.\nlocations: Allows you to control how the application container responds to incoming requests at a very fine-grained level. The simplest possible locations configuration is one that simply passes all requests on to your application unconditionally:\nweb:\n  locations:\n    '/':\n        passthru: true\n\nThe above configuration forwards all requests to /* to the process started by web.commands.start. In the case of PHP containers, passthru must specify what PHP file to forward the request to, as well as the docroot under which the file lives. For example,\nweb:\n  locations:\n      '/':\n          root: 'web'\n          passthru: '/app.php'\n\n\n\n\nmounts: Configuring mounts are not required, unless part of your application requires write-access. By default, Platform.sh provided a read-only filesystem for your projects so that you can be confident in the health and security of your application once it has deployed.\nIf your application requires writable storage to function properly (i.e., saving files; mounts should not contain code) it can be defined like so:\nmounts:\n  'web/uploads':\n      source: local\n      source_path: uploads\n\nIn this case, the application will be able to write to a mount that is visible in the /app/web/uploads directory of the application container, and which has a local source at /mnt/uploads. Consult the mounts documentation for a more thorough discussion of how these attributes should be written.\n\n\n\nEach language and framework may have additional attributes that you will need to include in .platform.app.yaml depending on the needs of your application. To find out what else you may need to include to configure your application, consult\n\nThe Application documentation for Platform.sh:\n The documentation goes into far more extensive detail of which attributes can also be included for application configuration, and should be used as your primary reference.\n\nLanguage-specific templates for Platform.sh Projects:\n Compare the .platform.app.yaml file from the simple template above to other templates when writing your own.\n\n\n\nNow that you have configured your application, you will next need to handle HTTP requests to your application using the .platform/routes.yaml file.\n\n  Back\n  I have configured my application\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Create a new project", "url": "/gettingstarted/own-code/create-project.html", "documentId": "918f6345136f5ae6729d47846c88e29de3b8604a", "text": "\n                        \n                            \n                                \n                                \n                                Import your own code\nCreate a new project\nWith the Platform.sh CLI now installed and configured to communicate with your account, you can create a new project from the command line and connect it to your application.\n&lt;/asciinema-player&gt;\n\nCreate an empty project\n Type the command platform create in your terminal.\n The CLI will then ask you to set up some initial project configurations:\n\nProject title: You need a unique name for each project, so title this one My CLI Project.\n\nRegion: In general you will choose the region that is closest to where most of your site's traffic is coming from. Here, go ahead and begin typing us-2.platform.sh and the CLI will auto-complete the rest for you.\n\nPlan: Select the development plan for your trial project.\n\nEnvironments: The master branch will become the Master environment, the live production environment for your application. Additionally, other branches may be activated as fully running environments for developing new features. More on that later. This value selects the maximum number of development environments the project will allow. You can change this value later at any time.\n For now, press Enter to select the default number of environments.\n\nStorage: You can modify the amount of storage your application can use from the CLI and from the management console, as well as upgrade that storage later once your project starts growing.\n For now, press Enter to select the default amount of storage.\nWhen the CLI has finished creating a project, it will output your project ID. This is the primary identifier for making changes to your projects, and you will need to use it to set Platform.sh as the remote for your repository in the next step.\nYou can also retrieve the project ID with the command platform project:list, which lists all of your projects and their IDs in a table.\n\n\n\nSet Platform.sh as remote for your application\n Next you will need to connect to the remote project in order to push your code to Platform.sh.\n If you have not already initialized your project directory as a Git repository, you will first need to do so\n git init\n Then you can set Platform.sh as a remote with the command\n platform project:set-remote &lt;project ID&gt;\n\n\nThat's it! You have now created an empty project and connected your repository to that project using the CLI.\nMove on now to the next step to start configuring your repository to deploy on Platform.sh.\n\n  Back\n  I created a project with the CLI\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Install the CLI", "url": "/gettingstarted/own-code/cli-install.html", "documentId": "7e092116e3868151e1a75e9fe1396e56f200d6df", "text": "\n                        \n                            \n                                \n                                \n                                Import your own code\nInstall the CLI\nIn the previous steps you checked that the requirements on your computer were met and configured an SSH key on your Platform.sh account. Now all we have to do is install the CLI and you can access your projects from the command line.\n&lt;/asciinema-player&gt;\n\nInstall the CLI\n In your terminal run the following command depending on your operating system:\n\nInstalling on OSX or Linux\n curl -sS https://platform.sh/cli/installer | php\n\n\nInstalling on Windows\n curl https://platform.sh/cli/installer -o cli-installer.php\n php cli-installer.php\n\n\n\n\nAuthenticate and Verify\nOnce the installation has completed, you can run the CLI in your terminal with the command\nplatform\n\nTake a moment to view some of the available commands with the command\nplatform list\n\n\n\nNow that you have installed the CLI and it is communicating with Platform.sh, you can configure and push your project to Platform.sh.\n\n  Back\n  I have installed the CLI\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Open a free trial account", "url": "/gettingstarted/own-code/free-trial.html", "documentId": "de74f1dbf719933f55c05bb33154cc523ed9fd87", "text": "\n                        \n                            \n                                \n                                \n                                Import your own code\nOpen a free trial account\nThe best way to understand a tool is to use it, that's why Platform.sh offers a free one month trial.\nVisit the Platform.sh accounts page and fill out your information to set up your trial account.\n\nAlternatively, you can sign up using an existing GitHub, Bitbucket, or Google account. If you choose this option, you will be able to set a password for your Platform.sh account later.\n\n  Back\n  I have set up my free trial account\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Requirements for the CLI", "url": "/gettingstarted/own-code/cli-requirements.html", "documentId": "e30ede04346231550dc13196d7fd9e796774d4ce", "text": "\n                        \n                            \n                                \n                                \n                                Import your own code\nRequirements for the CLI\nNow that you have created your free trial account, you are able to push your application to Platform.sh once you have installed the CLI, but there are a few requirements that must be met first.\nGit\nGit is the open source version control system that is utilized by Platform.sh. Any change you make to your Platform.sh project will need to be committed via Git. You can see all the Git commit messages of an environment in the Environment Activity feed of the management console for each project you create.\nBefore getting started, make sure you have Git installed on your computer.\nSSH key pair\nOnce your account has been set up and the CLI is installed, Platform.sh needs one additional piece of information about your computer so that you can access your projects from the command line.\nIf you are unfamiliar with how to generate an RSA public and private key, there are excellent instructions in the documentation about how to do so.\nAdd your SSH key to your account\nNow that you have the requirements out of the way, place your SSH key onto Platform.sh so that you can communicate with your projects from your computer using the management console. Log in to your account\n\n  \n\n\n\nAccess SSH key settings in the management console\n From the management console, move to the top right hand corner of the screen and click the dropdown menu to the left of the settings gear box icon. In the menu, click on Account. The next page will normally list all of your projects, which at this point will be empty if you're just starting out.\n Click on the Account Settings link at the top of the page, then click the SSH keys tab to the left of your account information.\n\nAdd your SSH key to your account\n At this point you won't see anything listed in the body of the page, because you don't have SSH configured with Platform.sh yet. Click the + Add public key button in the top right hand corner of the screen.\n This will open up another window with two fields. Name the key with something memorable, like home-computer, and in the field below that, paste the content of the public key you created in the previous step.\n When you have finished, click Save to save the key.\n\n\nThat's it! Now that you have met the requirements and configured an SSH key, Platform.sh can authenticate your computer and you can interact with your project from the command line.\nNext, you will need to install the Platform.sh CLI so that you can import your code to a project.\n\n  Back\n  I have configured my SSH public key in the management console\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Import your own code", "url": "/gettingstarted/own-code.html", "documentId": "14ec8c9530a06b1cf5c691c5ad81f5518afeada6", "text": "\n                        \n                            \n                                \n                                \n                                Import your own code\nWelcome to Platform.sh! Importing your own code to Platform.sh is as easy as installing the CLI and configuring your application with a few YAML files.\nWhen code examples are provided in the guide, click the language of your application.  If you consult those examples and a few templates as you go along, your code will be up and running on Platform.sh in no time.\n\n  Get started!\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Requirements for the CLI", "url": "/gettingstarted/template/cli-requirements.html", "documentId": "d783ccc01c71a0726aec50576b218019e03bc849", "text": "\n                        \n                            \n                                \n                                \n                                Start with a template\nNext Steps: Requirements for the CLI\nWith the management console you can start new projects from templates just as you did in the previous steps, but deploying your own applications requires you to also use the Platform.sh CLI.\nBefore you install it there are a few requirements that must be met first.\nGit\nGit is the open source version control system used by Platform.sh. Any change you make to your Platform.sh project will need to be committed via Git. You can see all the Git commit messages of an environment in the Environment Activity feed of the management console for each project you create.\nBefore getting started, make sure you have Git installed on your computer.\nSSH key pair\nOnce your account has been set up and the CLI is installed, Platform.sh needs one additional piece of information about your computer so that you can access your projects from the command line.\nIf you are unfamiliar with how to generate an SSH public and private key, there are instructions in the documentation about how to do so.\nAdd your SSH public key to your account\nAdd your SSH public key to your Platform.sh account so that you can communicate with your projects using the CLI.\n\n  \n\n\n\nAccess SSH key settings in the management console\n From the management console, move to the top right hand corner of the screen and click the dropdown menu to the left of the settings gear box icon. In the menu, click on Account. This next page lists all of your active projects, which now includes My First Project.\n Click on the Account Settings link at the top of the page, then click the SSH keys tab to the left of your account information.\n\nAdd your SSH public key to your account\n Click the + Add public key button in the top right hand corner of the screen. This will open up another window with two fields. Name the key with something memorable, like home-computer, and in the field below that, paste the content of your public key.\n When you have finished, click Save to save the key.\n\n\nThat's it! Now that you have met the requirements and configured an SSH key, all that's left is to install the Platform.sh CLI so you can interact with your projects from the command line.\n\n  Back\n  I have added my SSH public key to my account\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Install the CLI", "url": "/gettingstarted/template/cli-install.html", "documentId": "edcb144026176dafe0bdc6b08304fe620afa0dbe", "text": "\n                        \n                            \n                                \n                                \n                                Start with a template\nNext Steps: Installing the Platform.sh CLI\nWith all of the requirements met, install the CLI to start developing with Platform.sh.\n&lt;/asciinema-player&gt;\n\nInstall the CLI\n In your terminal run the following command:\n\nInstalling on OSX or Linux\n curl -sS https://platform.sh/cli/installer | php\n\n\nInstalling on Windows\n curl https://platform.sh/cli/installer -o cli-installer.php\n php cli-installer.php\n\n\n\n\nAuthenticate and Verify\nOnce the installation has completed, you can run the CLI in your terminal with the command\nplatform\n\n\nNote: If you opened your free trial account using another login (i.e. GitHub), you will not be able to authenticate with this command until you setup your account password with Platform.sh in the console.\n\nYou should now be able to see a list of your Platform.sh projects, including the template you made in this guide. You can copy its project ID hash, and then download a local copy of the repository with the command\nplatform get &lt;project ID&gt;\n\nWith a local copy, you can create branches, commit to them, and push your changes to Platform.sh right away!\ngit push platform master\n\nTake a minute to explore some of the commands available with the CLI by using the command platform list.\n\n\nThat's it! Now that you have the management console set up and the CLI installed on your computer, you're well on your way to exploring all of the ways that Platform.sh can improve your development workflow.\n\n  Back\n  I have installed the CLI\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Next Steps", "url": "/gettingstarted/template/next-steps.html", "documentId": "f7ad46f53f824eff39c71e9d1075af74cfb17fd8", "text": "\n                        \n                            \n                                \n                                \n                                Next steps\nIn this guide you created a project using the management console and installed the Platform.sh CLI. Now you can explore some of the next steps for working with Platform.sh.\nImport your own code\nTemplates are great, but configuring your own application to run on Platform.sh is the goal.\n\n  Import your own codeUse the CLI and a few configuration files to deploy your code on Platform.sh.\n\n\nDeveloping on Platform.sh\nOnce an application has been migrated to Platform.sh, there's plenty more features that will help improve your development life cycle.\n\n  Local DevelopmentRemotely connect to services and build your application locally during development.\n\n  Development environmentsActivate development branches and test new features before merging into production.\n\n\nAdditional Resources\n\n  Next stepsSet up domains to take your application live, configure external integrations to GitHub, and more!\n\n\n\n  Platform.sh CommunityCheck out how-tos, tutorials, and get help for your questions about Platform.sh.\n\n\n\n  Platform.sh BlogRead news and how-to posts all about working with Platform.sh.\n\n\n\n  Back\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Build, Deploy, Done!", "url": "/gettingstarted/template/check-status.html", "documentId": "184b5562496c102d69f7b72b25b6b3aea7342b57", "text": "\n                        \n                            \n                                \n                                \n                                Start with a template\nBuild, Deploy, Done!\nOnce you have configured the template application in the previous step, Platform.sh will build your project for you. If you created a blank project, be sure to set up your SSH keys before trying to upload your files.\n\n  \n\n\n\nExplore the management console\nWhen the build screen has cleared, Platform.sh will return you to the management console. Since you now have a project on your account, a version of this page will be what you see each time you visit the console.\nYou will start on the main page for your new project, My First Project. From here, you can control the settings of this project and monitor its status.\nIn the Environments box, click on Master.\n\nCheck the build status\nTake a minute to notice some the information available on this page.\n\nOverview\n In this box the Master environment, which is a live environment built from the master branch of your application code, will have a status of Building.\n Once that status has updated to Active, the build is complete and the application has deployed.\n\nEnvironment Activity\n In this block, you can see what you have done so far has two initial entries: My First Project was created, and the template profile you chose was initialized on the environment.\n\n\n\nDone!\nThat's it! Once the build status has changed to Active, your application has been deployed on Platform.sh.\nYou can view the template by clicking on the link that is now visible for the Master environment under the Overview box. It will open another tab in your browser to your new live site!\n\n\nIn these few steps you created a free trial account, configured a template application on a project and deployed it using the management console entirely from your browser.\nUsing the Platform.sh CLI, however, you get even more control over your project configurations, including the ability to migrate your own applications to Platform.sh. Move onto the next step to install it.\n\n  Back\n  I have built and deployed a template application\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Create a new project", "url": "/gettingstarted/template/create-project.html", "documentId": "5962dccd0882dc96524dc2d07b02a878a6d15f89", "text": "\n                        \n                            \n                                \n                                \n                                Start with a template\nCreate a new project from a template\nIn the previous step, you set up a free trial account with Platform.sh. Now you have access to the Platform.sh management console.\nFrom here you can create projects, adjust account settings, and a lot more that you will explore throughout these Getting Started guides.\n\n  \n\n\nSince you do not yet have any projects on Platform.sh, the first thing you will see when you sign in is a workflow for creating a new project.\n\nProject type\nYou will first be given the option of creating a New Project that is empty (one that you can migrate your own code base to), or to Use a Template. Select the Use a Template option, and then click Next.\n\nDetails\nGive your project a name like My First Project.\nIn the next field, you have the option to configure the project's region, which corresponds with the data center where your project will live.  Select the region that most closely matches where most of your traffic will come from and click Next.\n\nTemplate\nNow you will be able to see the large collection of Platform.sh's supported templates. There are several types of templates available, including simple language-specific examples, ready-to-use frameworks you can build upon, and setapplications you can start using immediately after installation.\nIf you are more comfortable with a particular language, click the dropdown labeled All languages. Select a language and the template list will update.\nSelect a template and click Next.\n\nNote\nYou can find the source code for all Platform.sh templates in the GitHub Templates Organization\n\n\nPlan &amp; Pricing\nUnder your free trial, your project will be created under a \"Development\" plan size. The management console will tell you how many users, development environments, and the price of that plan after your trial has completed. After you have read through the features, click Continue.\n\n\nWith these few selections Platform.sh will create the project and build the template in less than two minutes.\n\n  Back\n  I have created a new project with a template\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Open a free trial account", "url": "/gettingstarted/template/free-trial.html", "documentId": "3714c45d5098d9d96ed1c3b2177a2ef54810a8d1", "text": "\n                        \n                            \n                                \n                                \n                                Start with a template\nOpen a free trial account\nThe best way to understand a tool is to use it. That's why Platform.sh offers a free one month trial.\nVisit the Platform.sh accounts page and fill out your information to set up your trial account.\n\nAlternatively, you can sign up using an existing GitHub, Bitbucket, or Google account. If you choose this option, you will be able to set a password for your Platform.sh account later.\n\n  Back\n  I have set up my free trial account\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Getting started", "title": "Start with a template", "url": "/gettingstarted/template.html", "documentId": "54b0955e3a3a9b0d1f9e95dcc64ef23b733a8b47", "text": "\n                        \n                            \n                                \n                                \n                                Start with a template\nWelcome to Platform.sh! Getting started is as easy as opening a free trial account and initializing a template project.\nThere are no requirements on your part at this point. This guide will take you from zero to hero - from first glance to a deployed application entirely from your browser.\n\n  Get started!\n\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Overview", "title": "Getting help", "url": "/overview/getting-help.html", "documentId": "3329b2f79cd63c6966ee11511d7a32bd3641a54f", "text": "\n                        \n                            \n                                \n                                \n                                Getting Help\nIf you're facing any issue with Platform.sh, you can submit a support ticket from the Platform.sh management console.\n\nYou will be redirected to a list of your support tickets, and you can click to open a 'New ticket' at the top of the page. \n\nFile your issues in the fields for the new ticket and Submit.\n\nYou can also open a support ticket directly from your user account.\nYou are more than welcome to hop-on to our public Slack chat channel.\nAnd you are always invited to drop us a line at our contact form.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Pricing", "title": "Sponsored Sites", "url": "/overview/pricing/sponsored.html", "documentId": "64843b4c7696c3616c8b80e5ddfd891212674477", "text": "\n                        \n                            \n                                \n                                \n                                Sponsored Community Sites\nPlatform.sh provides sponsored hosting for Free Software projects and tech community events and organizations as part of our effort to support the Free Software community.\nEligibility\n\nA non-trivial software project released under a Free and Open Source license (*GPL, BSD, MIT, Apache, etc.)\nA community-oriented conference or similar event run either by the community or a related non-profit corporation.  For-profit events and organizations are not eligible.\nAn offline IT community organization, such as a local user group for a Free and Open Source project.\nAn online non-profit community project that supports one of the above.\n\nThe Platform.sh Developer Relations team manages the program and collectively has final say on what projects are eligible for sponsorship.\nPlatform.sh will periodically reevaluate sponsored sites to ensure they remain eligible, and may terminate the free service at its own will. Platform.sh will provide a month notice to allow users to migrate in these cases.\nOffering\nAn eligible project is entitled to one (1) Standard instance, with the standard 3 environments and 5 GB of storage.  Any number of users may be added as needed for the project at no additional cost.  The site is to be used exclusively for content supporting and promoting the project.  (Brochureware site, documentation, user forums, etc. are all acceptable.)\nThere is no limit on the number of domains or application containers (subject to resource limits of the Standard Plan) that a project may use, although the project is expected to register and manage its own domain(s). Platform.sh will provision SSL certificates for free (as may be limited by the Let\u2019s Encrypt API) but the hosted site may provide their own.\nA project is under no obligation to use the application container of their project.  (Eg, a PHP project is free to use Jekyll to produce a static site using Platform.sh, a Python project is free to host a PHPBB site, etc.)\nExpectations\nIn return, eligible projects are expected to include an HTML snippet conspicuously on each page of the site.  That may be in a sidebar or footer, with text no smaller than the standard body text size for the site. The image may be scaled via CSS as appropriate as long as the text is fully readable.  The badge markup will be provided by Platform.sh when setting up the site and will include a link to Platform.sh and an analytics tracking value so we know where links are coming from.  It does not include any cookies.\nPlatform.sh may from time to time update the widget\u2019s contents and the sponsored site will update the contents in a timely fashion.\nThe sponsored site may ask, and Platform.sh shall accept variations of the widget as long as these fall within the branding guidelines of Platform.sh.\nPlatform.sh may announce on its own media presence both online and offline that it hosts the said project, and kindly ask the sponsored site communicate at least once over its usual social media presence (Twitter, Mailing list, etc) the sponsored hosting.\nIf the eligible project is a registered non-profit organization in the US or Germany, it is also expected to provide paperwork to register an in-kind donation to the organization of the equivalent price of the service ($50 USD/month or the EUR equivalent), for tax purposes.\nSponsored projects will receive the same level of support as any other self-service hosted customer site.\nContact\nIf you are an eligible Free Software project or community group, contact our Developer Relations team and let us know that you are interested.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Overview", "title": "Pricing", "url": "/overview/pricing.html", "documentId": "e211b28284d6413b3d3ff7f075d5a68774741c48", "text": "\n                        \n                            \n                                \n                                \n                                Platform.sh pricing\nYou can see our full pricing information at: https://platform.sh/pricing/\nAll Platform.sh plans include:\n\nfour Environments (3 for staging/development, 1 for the live site).\none Developer license\n5GB of Storage per environment\nmultiple Backend services (MySQL, PostgreSQL, Elasticsearch, Redis, Solr..)\nsupport\n\nYou can switch between plans (downgrade or upgrade) freely, but note that reducing storage is currently not supported for technical reasons. If you need to reduce storage, please create a support ticket. You will always be billed the prorated rate of your plan over the period it was used.\nYou may cancel your plan at any time and you will only be billed for the actual period used.\nFor Elite, Enterprise and Agency Plans you can pay by purchase order. For all other plans you need to add a credit card to your account.\nWe offer a free trial period so you can test the service and see how great it is. If ever you need more time to evaluate Platform.sh, please contact our sales representatives. They can issue you an extra voucher to prolong your test.\n\nPrices below are listed in US Dollars.  You will be billed in US Dollars, Euros, or British Pounds depending on where your billing address is. For a list of current prices please refer to https://platform.sh/pricing\nEuro Prices are presented excluding VAT.  In your bill, as appropriate we will include the correct VAT rate.\n\nExtras\nAll extra consumption is prorated to the time it was actually used.\n\nFor example, if you added an extra developer for 10 days you would be billed around $3 extra at the end of the month (based on the then-current price of an extra developer seat).\n\nExtra developers\nAdding a developer to your project will add a monthly per project per user fee unless you have an agency or an enterprise account. \nExtra environments\nYou can add extra staging/development environments to any plan by multiples of 3.\n\nFor example, if you want to have 12 staging environments you would pay additional $63 per month on top of your basic plan price.\n\nExtra storage\nYou can add additional storage at $2.50 per 5GB  per staging/development environment.\n\nFor example, if you have the default plan (with 3 staging environments) and you add 10GB (for a total of 15GB per environment), you would pay an extra $15 a month.\nIf you added 3 extra environments (for a total of 6 staging environments) and you added 10GB (for a total of 15GB per environment), you would pay an extra $30 a month.\n\nDevelopment\nThe basic plan (Development) starts at $10 per month, and includes 4 environments: 3 staging/development and 1 future production).\n\nYou can not map a custom domain name to a development plan\n\nDevelopment environments have less resources than production environments.\nProduction\nThe live environment (master) of a production plan has more resources than the development environments of the project. https://platform.sh/pricing lists the resources available per plan (these are always only the production environment resources) the development environment have their own resources, and are not counted towards the limit.\nYou can map domain names to your master environment. SSL support is always included. \nMultiple Applications in a single project\nAll Platform.sh plans support multiple applications in a single cluster, but they share the global resources of the cluster.\nThe resources of a Standard plan are not sufficient to run more than one application in the same cluster if there is also a MySQL database as a Service. Useful multi-apps start at Medium.\nA Medium plan, for example, can support 3 Apps with a MySQL instance and a Redis instance.\nIf you wonder if a specific setup would fit in a plan, don't hesitate to contact our support.\nDedicated Instances\nFor a price lower than traditional managed hosting, you get included development and staging environments, as well as triple redundancy on every element of the stack with:\n\n99.99% Uptime Guaranteed\n24/7 White Glove On-boarding and Support\n\nPlease contact our sales department to discuss how we can help you.\nAgencies\nWe propose three tiers for agencies with many perks.\n\nFree user licenses\nA free site for your own agency\nUp to 10% customer lifetime referral fees and 15% discounts\nAccess to an agency speci\ufb01c \"Small\" price plan\nFree Medium or Large plan website for your own agency site\nFree Small plan for every Enterprise project sold\n\u2026and more!\n\nLearn more and join today...\nGerman Cloud Pricing\nThe prices for Germany are currently set at 10% above the EU and US plan prices. Thus, a \"Production Standard\" environment on the Sovereign German Cloud will be $55 instead of $50.\nOur estimation page (which you can reach by clicking on your account dashboard on the edit link for a project) does reflect these new options.\nIf you have any questions don't hesitate to contact our sales department.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1},
{"site": "docs", "section": "Overview", "title": "Build & Deploy", "url": "/overview/build-deploy.html", "documentId": "dd6ef45d9c9d6a48198a9d5b1745fd4ff6352661", "text": "\n                        \n                            \n                                \n                                \n                                Building and deploying applications\nEvery time you push to a live branch (a git branch with an active environment attached to it) or activate an environment for a branch, there are two main processes that happen: Build and Deploy.  \n\nThe build process looks through the configuration files in your repository and assembles the necessary containers.  \nThe deploy process makes those containers live, replacing the previous versions, with virtually no interruption in service.\n\n\nAlways Be Compiling\nInterpreted languages like PHP or Node.js may not seem like they have to be compiled, but with modern package management tools like Composer or npm, as well as the growing use of CSS preprocessors such as Sass, most modern web applications need a \"build\" step between their source code and their production execution code.  At Platform.sh, we aim to make that easy.  The build step includes the entire application container\u2014from language version to build tools to your code\u2014rebuilt every time.\nThat build process is under your control and runs on every Git push.  Every Git push is a validation not only of your code, but of your build process.  Whether that's installing packages using Composer or Bundler, compiling TypeScript or Sass, or building your application code in Go or Java, your build process is vetted every time you push.\nWhenever possible, you should avoid committing build assets to your repository that can be regenerated at build time.  Depending on your application, that may include 3rd party libraries and frameworks, generated and optimized CSS and JS files, generated source code, etc.\nThe following two constraints make sure you have fast, repeatable builds:\n\nThe build step should be environment-independent. This is paramount to ensure development environments are truly perfect copies of production. This means you can not connect to services (like the database) during the build.\nThe final built application must be read-only. If your application requires writing to the filesystem, you can specify the directories that require Read/Write access. These should not be directories that have code, because that would be a security risk.\n\nBuilding the application\nAfter you push your code, the first build step is to validate your configuration files (i.e. .platform.app.yaml, .platform/services.yaml, and .platform/routes.yaml). The Git server will issue an error if validation fails, and nothing will happen on the server.\n\nWhile most projects have a single .platform.app.yaml file, Platform.sh supports multiple applications in a single project.  It will scan the repository for .platform.app.yaml files in subdirectories and each directory containing one will be built as an independent application. A built application will not contain any directories above the one in which it is found. The system is smart enough not to rebuild applications that have already been built, so if you have multiple applications, only changed applications will be rebuilt and redeployed.\n\nThe live environment is composed of multiple containers\u2014both for your application(s) and for the services it depends on. It also has a virtual network connecting them, as well as a router to route incoming requests to the appropriate application.\nBased on your application type the system will select one of our pre-built container images and run the following:\n\nFirst, any dependencies specified in the .platform.app.yaml file are installed.  Those include tools like Sass, Gulp, Drupal Console, or any others that you may need.  \n\nThen, depending on the \u201cbuild flavor\u201d specified in the configuration file, we run a series of standard commands. The default for PHP containers, for example, is simply to run composer install.\n\nFinally, we run the \u201cbuild hook\u201d from the configuration file.  The build hook comprises one or more shell commands that you write to finish creating your production code base.  That could be compiling Sass files, running a Gulp or Grunt script, rearranging files on disk, compiling an application in a compiled language, or whatever else you want.  Note that, at this point, all you are able to access is the file system; there are no services or other databases available.\n\n\nOnce all of that is completed, we freeze the file system and produce a read-only container image.  That image is the final build artifact: a reliable, repeatable snapshot of your application, built the way you want, with the environment you want.\nBecause  container configuration (both for your application and its underlying services) is exclusively based on your configuration files, and your configuration files are managed through Git, we know that a given container has a 1:1 relationship with a Git commit.  That means builds are always repeatable.  It also means that if we detect that there are no changes that would affect a given container, we can skip the build step entirely and reuse the existing container image, saving a great deal of time.\nIn practice, the entire build process usually takes less than a minute.\nDeploying the application\nDeploying the application also has several steps, although they're much quicker.\nFirst, we pause all incoming requests and hold them so that there's no interruption of service.  Then we disconnect the current containers from their file system mounts, if any, and connect the file systems to the new containers instead.  If it's a new branch and there is no existing file system, we clone it from the parent branch.\nWe then open networking connections between the various containers, but only those that were specified in the configuration files (using the relationships key).  No configuration, no connection. That helps with security, as only those connections that are actually needed even exist.  The connection information for each service is available in an application as environment variables.\nNow that we have working, running containers there are two more steps to run.  First, if there is a \u201cstart\u201d command specified in your .platform.app.yaml file to start the application, we run that. (With PHP, this is optional.)\nThen we run your deploy hook.  Just like the build hook, the deploy hook is any number of shell commands you need to prepare your application.  Usually this includes clearing caches, running database migrations, and so on.  You have complete access to all services here as your application is up and running, but the file system where your code lives will now be read-only.\nFinally, once the deploy hooks are done, we open the floodgates and let incoming requests through to your newly deployed application.  You're done!\nIn practice, the deploy process takes only a few seconds, plus whatever time is required for your deploy hook, if any.\n\n                                \n                                \n                            \n                        \n                    ", "rank": 1}
][
{"site": "docs"}
][
{"site": "docs"}
]